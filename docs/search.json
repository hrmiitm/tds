[
  {
    "objectID": "W1/w1.html",
    "href": "W1/w1.html",
    "title": "Week - 1",
    "section": "",
    "text": "Hi Everyone.",
    "crumbs": [
      "Week1"
    ]
  },
  {
    "objectID": "W1/4_bash/1_bashrc.html",
    "href": "W1/4_bash/1_bashrc.html",
    "title": "Linux - Environment Variables",
    "section": "",
    "text": "The .bashrc file is a shell script that automatically runs whenever you open a new interactive bash terminal session. It‚Äôs your personal configuration file located in your home directory (~/.bashrc) that sets up your shell environment.\n\n\n\n\nMY_VAR=\"value\"\nPATH_TO_PROJECT=\"/home/user/project\"\nThese variables are only available in the current shell and won‚Äôt be inherited by child processes.\n\n\n\nexport MY_VAR=\"value\"\nexport PATH=\"$HOME/bin:$PATH\"\nexport DATABASE_URL=\"localhost:5432\"\nVariables defined with export become environment variables that are automatically passed to all child processes and subshells.\n\n\n\n\n\nPATH modifications: Add custom directories to your command search path\nProject settings: Define project-specific environment variables\nAliases and shortcuts: Create convenient command shortcuts\nTool configuration: Set up development tools and their configurations\n\n\n\n\n\n\n\nPurpose: Makes variables available to child processes and subshells\nScope: Variables flow downward from parent to child shells only\nUsage: export VAR_NAME=\"value\"\n\n\n\n\n\nPurpose: Executes a script in the current shell (not a subshell)\nUsage: source ~/.bashrc or . ~/.bashrc\nEffect: Loads variables and functions into your current session without restarting the terminal\n\n\n\n\nWhen you run a script with ./script.sh, exported variables disappear when the script ends because it runs in a subshell. When you use source script.sh, variables persist in your current shell.\nExample workflow:\n# Edit .bashrc\necho 'export MY_APP_PATH=\"/usr/local/myapp\"' &gt;&gt; ~/.bashrc\n\n# Apply changes without restarting terminal\nsource ~/.bashrc\n\n# Now MY_APP_PATH is available\necho $MY_APP_PATH",
    "crumbs": [
      "Week1",
      "LINUX/BASH - Basic",
      "Environment Variable"
    ]
  },
  {
    "objectID": "W1/4_bash/1_bashrc.html#writing-variables-in-.bashrc",
    "href": "W1/4_bash/1_bashrc.html#writing-variables-in-.bashrc",
    "title": "Linux - Environment Variables",
    "section": "",
    "text": "MY_VAR=\"value\"\nPATH_TO_PROJECT=\"/home/user/project\"\nThese variables are only available in the current shell and won‚Äôt be inherited by child processes.\n\n\n\nexport MY_VAR=\"value\"\nexport PATH=\"$HOME/bin:$PATH\"\nexport DATABASE_URL=\"localhost:5432\"\nVariables defined with export become environment variables that are automatically passed to all child processes and subshells.",
    "crumbs": [
      "Week1",
      "LINUX/BASH - Basic",
      "Environment Variable"
    ]
  },
  {
    "objectID": "W1/4_bash/1_bashrc.html#why-use-variables-in-.bashrc",
    "href": "W1/4_bash/1_bashrc.html#why-use-variables-in-.bashrc",
    "title": "Linux - Environment Variables",
    "section": "",
    "text": "PATH modifications: Add custom directories to your command search path\nProject settings: Define project-specific environment variables\nAliases and shortcuts: Create convenient command shortcuts\nTool configuration: Set up development tools and their configurations",
    "crumbs": [
      "Week1",
      "LINUX/BASH - Basic",
      "Environment Variable"
    ]
  },
  {
    "objectID": "W1/4_bash/1_bashrc.html#export-vs-source",
    "href": "W1/4_bash/1_bashrc.html#export-vs-source",
    "title": "Linux - Environment Variables",
    "section": "",
    "text": "Purpose: Makes variables available to child processes and subshells\nScope: Variables flow downward from parent to child shells only\nUsage: export VAR_NAME=\"value\"\n\n\n\n\n\nPurpose: Executes a script in the current shell (not a subshell)\nUsage: source ~/.bashrc or . ~/.bashrc\nEffect: Loads variables and functions into your current session without restarting the terminal\n\n\n\n\nWhen you run a script with ./script.sh, exported variables disappear when the script ends because it runs in a subshell. When you use source script.sh, variables persist in your current shell.\nExample workflow:\n# Edit .bashrc\necho 'export MY_APP_PATH=\"/usr/local/myapp\"' &gt;&gt; ~/.bashrc\n\n# Apply changes without restarting terminal\nsource ~/.bashrc\n\n# Now MY_APP_PATH is available\necho $MY_APP_PATH",
    "crumbs": [
      "Week1",
      "LINUX/BASH - Basic",
      "Environment Variable"
    ]
  },
  {
    "objectID": "W1/2_git_github/4_opensource_contributions.html",
    "href": "W1/2_git_github/4_opensource_contributions.html",
    "title": "OpenSource Contribution",
    "section": "",
    "text": "To fetch someone else‚Äôs code from GitHub, fix an issue, and contribute your changes so they can merge your fixes, follow this standard Git workflow using Fork, Clone, Branch, Commit, Push, and Pull Request:",
    "crumbs": [
      "Week1",
      "2. Git & Github",
      "Contribute To Open Source"
    ]
  },
  {
    "objectID": "W1/2_git_github/4_opensource_contributions.html#steps-to-contribute-code-fix-on-github",
    "href": "W1/2_git_github/4_opensource_contributions.html#steps-to-contribute-code-fix-on-github",
    "title": "OpenSource Contribution",
    "section": "Steps to Contribute Code Fix on GitHub",
    "text": "Steps to Contribute Code Fix on GitHub\n\n1. Fork the Repository\n\nGo to the repository page on GitHub where the original code is.\nClick the ‚ÄúFork‚Äù button (top-right) to create your own copy of the repository under your GitHub account.\n\n\n\n2. Clone the Forked Repository to Your Machine\n\nOn your GitHub fork page, click the green ‚ÄúCode‚Äù button and copy the repository URL.\nIn your terminal, run:\n\ngit clone &lt;your-fork-repo-url&gt;\n\nThis downloads the code to your local machine.\n\n\n\n3. Create a New Branch for Your Changes\n\nChange directory into the cloned repo:\n\ncd &lt;repo-name&gt;\n\nCreate and switch to a new branch for your fix:\n\ngit checkout -b fix-issue-branch\n\n\n4. Make Your Code Fixes\n\nEdit the code locally to fix the issue.\n\n\n\n5. Commit Your Changes\n\nStage the changed files:\n\ngit add .\n\nCommit with a meaningful message:\n\ngit commit -m \"Fix issue with XYZ\"\n\n\n6. Push Your Branch to Your Fork\n\nPush your fix branch to your GitHub fork:\n\ngit push origin fix-issue-branch\n\n\n7. Create a Pull Request (PR)\n\nGo to your GitHub fork page.\nYou should see a prompt to create a pull request from your pushed branch.\nClick ‚ÄúCompare & pull request‚Äù or ‚ÄúNew pull request.‚Äù\nAdd a descriptive title and description of the fix.\nSubmit the pull request so the original repo owner can review and merge your fix.\n\n\nThis process ensures your contribution is linked to your GitHub profile and gives the repo owner a clear way to review and merge your changes. It also preserves the original repository history cleanly.",
    "crumbs": [
      "Week1",
      "2. Git & Github",
      "Contribute To Open Source"
    ]
  },
  {
    "objectID": "W1/2_git_github/2_before_github.html",
    "href": "W1/2_git_github/2_before_github.html",
    "title": "Setup to connect Local ‚Äî Github",
    "section": "",
    "text": "Git : Locally you need to setup your github username and email\nGithub : Generate an SSH key and add the public key to your‚Äôs github account.\n\n\n\n\nMake account in github.com and copy username and email\n\nOpen Terminal (Linux/Mac) or PowerShell/GitBash (Windows).\n\ngit config --global user.name \"Your Github UserName\"\ngit config --global user.email \"your_github_email@example.com\"\n\n\n\n\nOpen Terminal (Linux/Mac) or PowerShell/GitBash (Windows).\nRun:\nssh-keygen\nPress Enter through the prompts.\nThis generates two files, e.g., id_ed25519 (private) and id_ed25519.pub (public), usually in(exact path will be present in output of command ssh-keygen):\n/home/username/.ssh/\nOpen id_ed25519.pub, copy its contents, and add it to your GitHub:\n\nSettings ‚Üí Access ‚Üí SSH and GPG Keys ‚Üí New SSH Key\nSelect Authentication Key as the type.\n\n\nSo now you are ready to push your code on your github account\n\n\n\n\nSSH (Secure Shell) enables secure communication between a client (your machine) and a server (e.g., GitHub).\nIt uses public-key cryptography consisting of two keys:\n\nPrivate key: stays secure on your local machine.\nPublic key: shared with the server.\n\n\nAuthentication process:\n\nThe client initiates an SSH connection.\nThe server checks if the client‚Äôs public key is authorized.\nThe server creates a challenge (encrypted message) using the stored public key.\nThe client must decrypt it with its private key and return the correct response.\nIf valid, the server confirms the client‚Äôs identity.\nA secure encrypted channel is then established for further communication.\n\nKey properties:\n\nNo password is transmitted.\nOnly the holder of the private key can authenticate.\nProtects against brute-force and interception attacks.",
    "crumbs": [
      "Week1",
      "2. Git & Github",
      "Before Github"
    ]
  },
  {
    "objectID": "W1/2_git_github/2_before_github.html#local-git-config",
    "href": "W1/2_git_github/2_before_github.html#local-git-config",
    "title": "Setup to connect Local ‚Äî Github",
    "section": "",
    "text": "Make account in github.com and copy username and email\n\nOpen Terminal (Linux/Mac) or PowerShell/GitBash (Windows).\n\ngit config --global user.name \"Your Github UserName\"\ngit config --global user.email \"your_github_email@example.com\"",
    "crumbs": [
      "Week1",
      "2. Git & Github",
      "Before Github"
    ]
  },
  {
    "objectID": "W1/2_git_github/2_before_github.html#ssh-key-setup",
    "href": "W1/2_git_github/2_before_github.html#ssh-key-setup",
    "title": "Setup to connect Local ‚Äî Github",
    "section": "",
    "text": "Open Terminal (Linux/Mac) or PowerShell/GitBash (Windows).\nRun:\nssh-keygen\nPress Enter through the prompts.\nThis generates two files, e.g., id_ed25519 (private) and id_ed25519.pub (public), usually in(exact path will be present in output of command ssh-keygen):\n/home/username/.ssh/\nOpen id_ed25519.pub, copy its contents, and add it to your GitHub:\n\nSettings ‚Üí Access ‚Üí SSH and GPG Keys ‚Üí New SSH Key\nSelect Authentication Key as the type.\n\n\nSo now you are ready to push your code on your github account",
    "crumbs": [
      "Week1",
      "2. Git & Github",
      "Before Github"
    ]
  },
  {
    "objectID": "W1/2_git_github/2_before_github.html#ssh---a-new-authentication-way",
    "href": "W1/2_git_github/2_before_github.html#ssh---a-new-authentication-way",
    "title": "Setup to connect Local ‚Äî Github",
    "section": "",
    "text": "SSH (Secure Shell) enables secure communication between a client (your machine) and a server (e.g., GitHub).\nIt uses public-key cryptography consisting of two keys:\n\nPrivate key: stays secure on your local machine.\nPublic key: shared with the server.\n\n\nAuthentication process:\n\nThe client initiates an SSH connection.\nThe server checks if the client‚Äôs public key is authorized.\nThe server creates a challenge (encrypted message) using the stored public key.\nThe client must decrypt it with its private key and return the correct response.\nIf valid, the server confirms the client‚Äôs identity.\nA secure encrypted channel is then established for further communication.\n\nKey properties:\n\nNo password is transmitted.\nOnly the holder of the private key can authenticate.\nProtects against brute-force and interception attacks.",
    "crumbs": [
      "Week1",
      "2. Git & Github",
      "Before Github"
    ]
  },
  {
    "objectID": "W1/1_window_to_linux/wsl.html",
    "href": "W1/1_window_to_linux/wsl.html",
    "title": "Install WSL in Windows",
    "section": "",
    "text": "We will use windows powershell to install wsl.\n\n\n\nOpen PowerShell\nwsl --version or wsl.exe --version\nIf any above command shows wsl version 2 , then it is already installed\n\n\n\n\n\nSearch Turn Windows features on or off\n\nEnable Virtual Machine Platform, Windows Hypervisor Platform, Windows Subsytem for Linux\n\nRestart Computer (Not power on/off, click on restart)\nInstall WSL WSL-2 Setup\nSwitch to WSL-2 wsl --set-default-version 2\n\n\n\n\nwsl --list --verbose\nFor Each Listed distribution\n- wsl --unregister &lt;DistributionName&gt;\n- Open Settings ‚Üí Apps ‚Üí Installed apps - Find each Linux distribution, click the three-dot menu, and select Uninstall\n\n\n\n\nwsl --update\n\nwsl --list --online\n\nwsl --install Ubuntu-24.04 ‚Äì&gt; Install Ubuntu\n\nsudo apt update && sudo apt upgrade -y ‚Äì&gt; Update Ubuntu\n\n\n\n\n\n\n\nNoteExample\n\n\n\n\n\nPS C:\\Users\\hrith&gt; wsl --install Ubuntu-24.04\nDownloading: Ubuntu 24.04 LTS\nInstalling: Ubuntu 24.04 LTS\nDistribution successfully installed. It can be launched via 'wsl.exe -d Ubuntu-24.04'\nLaunching Ubuntu-24.04...\nProvisioning the new WSL instance Ubuntu-24.04\nThis might take a while...\nCreate a default Unix user account: hrm\nNew password:\nRetype new password:\npasswd: password updated successfully\nTo run a command as administrator (user \"root\"), use \"sudo &lt;command&gt;\".\nSee \"man sudo_root\" for details.\n\nhrm@bitnd:/mnt/c/Users/hrith$ sudo apt update && sudo apt upgrade -y\n[sudo] password for hrm:\nHit:1 http://archive.ubuntu.com/ubuntu noble InRelease\nGet:2 http://security.ubuntu.com/ubuntu noble-security InRelease [126 kB]\n\n\n\n\n\n\n\nPython VENV\n\npython3 -m venv .venv ‚Äì&gt; Copy error code and run sudo apt install python3.12-venv\n\nsudo apt install python3-pip\n\nPIPX\n\nsudo apt install pipx\n\nUV - Rust-based Python package installer\n\npipx install uv It will maintain isolation\n\nLLM\n\npipx install llm ‚Äì&gt; pipx ensurepath\nConfigure it\n\nllm install llm-gemini or llm install llm-ollama\nllm keys set gemini\nllm -m gemini-2.0-flash 'Tell me fun facts about Mountain View'\n\n\nMiniConda\n\nDownload .sh https://www.anaconda.com/download/success\nbash &lt;pathto .sh file&gt;\nconda config --set auto_activate_base false\n\n\n\n\n\n\nnvm install visit and run bash script https://github.com/nvm-sh/nvm\n\nnode install https://nodejs.org/en/download\n\nnvm install 22\n\nnvm list nvm use 22 nvm current\n\ncorepack enable yarn\n\ncorepack enable pnpm\n\n\n\nCheck\n- nvm -v\n- node -v\n- npm -v npx -v\n- pnpm -v\n- yarn -v\n\n\n\n\nsudo apt install build-essential\n\ngcc ‚Äì&gt; The C compiler\n\ng++ ‚Äì&gt; The C++ compiler\n\n\nCheck - gcc ‚Äìverison\n- g++ ‚Äìversion\n\n\n\n\nsudo apt install default-jdk\n\nThis command installs:\n\nJava Development Kit (JDK) - Compiler, debugger, and development tools  \nJava Runtime Environment (JRE) - Required to run Java applications  \nJava Virtual Machine (JVM) - Core execution environment  \n\nConfigure JAVA_HOME Environment Variable\n\necho 'export JAVA_HOME=\"/usr/lib/jvm/default-java\"' &gt;&gt; ~/.bashrc ## confirm the path first using below update-alternatiove‚Ä¶ command\n\nrestart shell\n\nInstall other versions of java\n\nsudo apt install openjdk-17-jdk\n\nSet Default Java/Javac installed version\n\nsudo update-alternatives --config java\nsudo update-alternatives --config javac\n\n\n\nCheck\n- java --version\n- javac --version\n- echo $JAVA_HOME\n\n\n\n\nFor example let install quarto https://quarto.org/docs/get-started/\n\nDownload and move to directory where deb package is downloaded\n\nsudo apt install ./quarto-1.8.24-linux-amd64.deb\n\n\n\n\n\n\n\nWSL-2 Setup"
  },
  {
    "objectID": "W1/1_window_to_linux/wsl.html#check-wsl-2-installed-or-not",
    "href": "W1/1_window_to_linux/wsl.html#check-wsl-2-installed-or-not",
    "title": "Install WSL in Windows",
    "section": "",
    "text": "Open PowerShell\nwsl --version or wsl.exe --version\nIf any above command shows wsl version 2 , then it is already installed"
  },
  {
    "objectID": "W1/1_window_to_linux/wsl.html#if-wsl-2-not-instlled",
    "href": "W1/1_window_to_linux/wsl.html#if-wsl-2-not-instlled",
    "title": "Install WSL in Windows",
    "section": "",
    "text": "Search Turn Windows features on or off\n\nEnable Virtual Machine Platform, Windows Hypervisor Platform, Windows Subsytem for Linux\n\nRestart Computer (Not power on/off, click on restart)\nInstall WSL WSL-2 Setup\nSwitch to WSL-2 wsl --set-default-version 2"
  },
  {
    "objectID": "W1/1_window_to_linux/wsl.html#if-already-installed-then-remove-previous-installed-distributions",
    "href": "W1/1_window_to_linux/wsl.html#if-already-installed-then-remove-previous-installed-distributions",
    "title": "Install WSL in Windows",
    "section": "",
    "text": "wsl --list --verbose\nFor Each Listed distribution\n- wsl --unregister &lt;DistributionName&gt;\n- Open Settings ‚Üí Apps ‚Üí Installed apps - Find each Linux distribution, click the three-dot menu, and select Uninstall"
  },
  {
    "objectID": "W1/1_window_to_linux/wsl.html#update-wsl-ubuntu-installation",
    "href": "W1/1_window_to_linux/wsl.html#update-wsl-ubuntu-installation",
    "title": "Install WSL in Windows",
    "section": "",
    "text": "wsl --update\n\nwsl --list --online\n\nwsl --install Ubuntu-24.04 ‚Äì&gt; Install Ubuntu\n\nsudo apt update && sudo apt upgrade -y ‚Äì&gt; Update Ubuntu\n\n\n\n\n\n\n\nNoteExample\n\n\n\n\n\nPS C:\\Users\\hrith&gt; wsl --install Ubuntu-24.04\nDownloading: Ubuntu 24.04 LTS\nInstalling: Ubuntu 24.04 LTS\nDistribution successfully installed. It can be launched via 'wsl.exe -d Ubuntu-24.04'\nLaunching Ubuntu-24.04...\nProvisioning the new WSL instance Ubuntu-24.04\nThis might take a while...\nCreate a default Unix user account: hrm\nNew password:\nRetype new password:\npasswd: password updated successfully\nTo run a command as administrator (user \"root\"), use \"sudo &lt;command&gt;\".\nSee \"man sudo_root\" for details.\n\nhrm@bitnd:/mnt/c/Users/hrith$ sudo apt update && sudo apt upgrade -y\n[sudo] password for hrm:\nHit:1 http://archive.ubuntu.com/ubuntu noble InRelease\nGet:2 http://security.ubuntu.com/ubuntu noble-security InRelease [126 kB]"
  },
  {
    "objectID": "W1/1_window_to_linux/wsl.html#install-python-tools",
    "href": "W1/1_window_to_linux/wsl.html#install-python-tools",
    "title": "Install WSL in Windows",
    "section": "",
    "text": "Python VENV\n\npython3 -m venv .venv ‚Äì&gt; Copy error code and run sudo apt install python3.12-venv\n\nsudo apt install python3-pip\n\nPIPX\n\nsudo apt install pipx\n\nUV - Rust-based Python package installer\n\npipx install uv It will maintain isolation\n\nLLM\n\npipx install llm ‚Äì&gt; pipx ensurepath\nConfigure it\n\nllm install llm-gemini or llm install llm-ollama\nllm keys set gemini\nllm -m gemini-2.0-flash 'Tell me fun facts about Mountain View'\n\n\nMiniConda\n\nDownload .sh https://www.anaconda.com/download/success\nbash &lt;pathto .sh file&gt;\nconda config --set auto_activate_base false"
  },
  {
    "objectID": "W1/1_window_to_linux/wsl.html#install-javascript-tools",
    "href": "W1/1_window_to_linux/wsl.html#install-javascript-tools",
    "title": "Install WSL in Windows",
    "section": "",
    "text": "nvm install visit and run bash script https://github.com/nvm-sh/nvm\n\nnode install https://nodejs.org/en/download\n\nnvm install 22\n\nnvm list nvm use 22 nvm current\n\ncorepack enable yarn\n\ncorepack enable pnpm\n\n\n\nCheck\n- nvm -v\n- node -v\n- npm -v npx -v\n- pnpm -v\n- yarn -v"
  },
  {
    "objectID": "W1/1_window_to_linux/wsl.html#install-cc-tools",
    "href": "W1/1_window_to_linux/wsl.html#install-cc-tools",
    "title": "Install WSL in Windows",
    "section": "",
    "text": "sudo apt install build-essential\n\ngcc ‚Äì&gt; The C compiler\n\ng++ ‚Äì&gt; The C++ compiler\n\n\nCheck - gcc ‚Äìverison\n- g++ ‚Äìversion"
  },
  {
    "objectID": "W1/1_window_to_linux/wsl.html#install-java-tools",
    "href": "W1/1_window_to_linux/wsl.html#install-java-tools",
    "title": "Install WSL in Windows",
    "section": "",
    "text": "sudo apt install default-jdk\n\nThis command installs:\n\nJava Development Kit (JDK) - Compiler, debugger, and development tools  \nJava Runtime Environment (JRE) - Required to run Java applications  \nJava Virtual Machine (JVM) - Core execution environment  \n\nConfigure JAVA_HOME Environment Variable\n\necho 'export JAVA_HOME=\"/usr/lib/jvm/default-java\"' &gt;&gt; ~/.bashrc ## confirm the path first using below update-alternatiove‚Ä¶ command\n\nrestart shell\n\nInstall other versions of java\n\nsudo apt install openjdk-17-jdk\n\nSet Default Java/Javac installed version\n\nsudo update-alternatives --config java\nsudo update-alternatives --config javac\n\n\n\nCheck\n- java --version\n- javac --version\n- echo $JAVA_HOME"
  },
  {
    "objectID": "W1/1_window_to_linux/wsl.html#install-any-.deb-packages",
    "href": "W1/1_window_to_linux/wsl.html#install-any-.deb-packages",
    "title": "Install WSL in Windows",
    "section": "",
    "text": "For example let install quarto https://quarto.org/docs/get-started/\n\nDownload and move to directory where deb package is downloaded\n\nsudo apt install ./quarto-1.8.24-linux-amd64.deb"
  },
  {
    "objectID": "W1/1_window_to_linux/wsl.html#resources",
    "href": "W1/1_window_to_linux/wsl.html#resources",
    "title": "Install WSL in Windows",
    "section": "",
    "text": "WSL-2 Setup"
  },
  {
    "objectID": "W1/1_window_to_linux/w1_3.html",
    "href": "W1/1_window_to_linux/w1_3.html",
    "title": "VM Virtual Machines",
    "section": "",
    "text": "Download and Install Virtual Box\n\nLink\n\nDownload .iso file of any distribution\n\ne.g.¬†Ubuntu\n\nInstall iso file in Virtual Box\n\nClick on New\nSelect Iso and fill required data‚Ä¶\n\n\n\n\n\n\n\n\nVirtual Box Setup (Hindi)",
    "crumbs": [
      "Week1",
      "1. Windows To Linux",
      "VM"
    ]
  },
  {
    "objectID": "W1/1_window_to_linux/w1_3.html#using-type-2-hypervisor",
    "href": "W1/1_window_to_linux/w1_3.html#using-type-2-hypervisor",
    "title": "VM Virtual Machines",
    "section": "",
    "text": "Download and Install Virtual Box\n\nLink\n\nDownload .iso file of any distribution\n\ne.g.¬†Ubuntu\n\nInstall iso file in Virtual Box\n\nClick on New\nSelect Iso and fill required data‚Ä¶",
    "crumbs": [
      "Week1",
      "1. Windows To Linux",
      "VM"
    ]
  },
  {
    "objectID": "W1/1_window_to_linux/w1_3.html#resources",
    "href": "W1/1_window_to_linux/w1_3.html#resources",
    "title": "VM Virtual Machines",
    "section": "",
    "text": "Virtual Box Setup (Hindi)",
    "crumbs": [
      "Week1",
      "1. Windows To Linux",
      "VM"
    ]
  },
  {
    "objectID": "W1/1_window_to_linux/w1_1.html",
    "href": "W1/1_window_to_linux/w1_1.html",
    "title": "Computer Boot Process",
    "section": "",
    "text": "Overview\nBoot Process Steps\nPartition Styles vs File Systems\nBoot Process Summary\nFrequently Asked Questions\nResources\n\n\n\n\nThe computer boot process transforms your computer from powered-off state to a fully operational system. This guide covers the essential steps and concepts needed to understand how computers start.\n\n\n\n\n\nWhen you press the power button:\n\nCPU executes the first program BIOS/UEFI\n\nBIOS (Basic Input/Output System) - Legacy firmware (used in old computers)\nUEFI (Unified Extensible Firmware Interface) - Modern firmware\n\n\n\n\n\nPOST (Power-On Self Test):\n\nTests CPU, RAM, and storage devices\nValidates hardware components\n\nBoot Device Selection:\n\nReads Boot Order from firmware settings\nGPT drives: Looks for EFI System Partition\nMBR drives: Checks Master Boot Record in first sector\n\n\n\n\nCommon Bootloaders:\nLinux or Window bootloader, both can scan and start any OS windows or linux.\n- Linux: GRUB2, LILO, systemd-boot - Windows: Windows Boot Manager\nBootloader Tasks:\n\nScans partitions for installed operating systems\nPresents boot menu (if multiple OS found)\nLoads selected OS kernel into memory\n\n\n\n\nLinux OS Boot:\n\nKernel loads and initializes hardware\nsystemd starts (modern init system)\nSystem services launch\nUser login interface appears\n\nWindows OS Boot:\n\nNT Kernel (ntoskrnl.exe) loads\nHardware Abstraction Layer initializes\nRegistry and system drivers load\nSession Manager starts Windows subsystems\nWindows Logon presents login interface\n\n\n\n\n\n\n\nPartition styles define how a drive is divided into sections:\n\n\n\n\n\n\n\n\nFeature\nMBR\nGPT\n\n\n\n\nMax Partitions\n4 primary OR 3 primary + 1 extended\n128 primary\n\n\nMax Storage\n2 TB\n18+ exabytes\n\n\nBoot Support\nBIOS only\nBIOS + UEFI\n\n\n\n\n\n\nFile systems determine how data is stored within partitions:\n\n\n\nFile System\nOS\nUse Case\n\n\n\n\nNTFS\nWindows\nSystem drives, large files\n\n\nFAT32\nCross-platform\nUSB drives, compatibility\n\n\next4\nLinux\nLinux system drives\n\n\nAPFS\nmacOS\nmacOS system drives\n\n\n\n\n\n\n\n\n\n\nPhase\nComponent\nPurpose\n\n\n\n\n1\nFirmware (BIOS/UEFI)\nHardware initialization\n\n\n2\nPOST\nHardware verification\n\n\n3\nBootloader\nOS selection and loading\n\n\n4\nOS Kernel\nSystem initialization\n\n\n\n\n\n\n\n\nflowchart LR\n    A[Power On] --&gt; B[Bios/Uefi]\n    B --&gt; C[POST Check]\n    C --&gt; D[LocateDrive]\n    D --&gt; E[Run Bootloader]\n    E --&gt; F{Select OS}\n    F --&gt;|Linux| G[Kernel + systemd]\n    F --&gt;|Windows| H[NT Kernel]\n    G --&gt; I[User Login]\n    H --&gt; I\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeature\nBIOS\nUEFI\n\n\n\n\nInterface\nText-only\nGraphical possible\n\n\nStorage Support\n2 TB max\nNo practical limit\n\n\nSecurity\nBasic\nSecure Boot\n\n\nSpeed\nSlower\nFaster\n\n\n\n\n\n\nPartition Style: Defines how the drive is divided (MBR vs GPT) File System: Defines how files are stored within each partition (NTFS, ext4, etc.)\n\n\n\nYes, by:\n\nInstalling each OS on separate partitions\nUsing a bootloader that detects all systems\nSelecting which OS to boot at startup\n\n\n\n\n\n\n\n\nBoot Process (English)\nBoot Process (Hindi)\nWindows Partitions (Hindi)\n\n\n\nBoot Process (English) \nBoot Process (Hindi) \nWindows Partitions (Hindi)",
    "crumbs": [
      "Week1",
      "1. Windows To Linux",
      "Computer Boot Process"
    ]
  },
  {
    "objectID": "W1/1_window_to_linux/w1_1.html#table-of-contents",
    "href": "W1/1_window_to_linux/w1_1.html#table-of-contents",
    "title": "Computer Boot Process",
    "section": "",
    "text": "Overview\nBoot Process Steps\nPartition Styles vs File Systems\nBoot Process Summary\nFrequently Asked Questions\nResources",
    "crumbs": [
      "Week1",
      "1. Windows To Linux",
      "Computer Boot Process"
    ]
  },
  {
    "objectID": "W1/1_window_to_linux/w1_1.html#overview",
    "href": "W1/1_window_to_linux/w1_1.html#overview",
    "title": "Computer Boot Process",
    "section": "",
    "text": "The computer boot process transforms your computer from powered-off state to a fully operational system. This guide covers the essential steps and concepts needed to understand how computers start.",
    "crumbs": [
      "Week1",
      "1. Windows To Linux",
      "Computer Boot Process"
    ]
  },
  {
    "objectID": "W1/1_window_to_linux/w1_1.html#boot-process-steps",
    "href": "W1/1_window_to_linux/w1_1.html#boot-process-steps",
    "title": "Computer Boot Process",
    "section": "",
    "text": "When you press the power button:\n\nCPU executes the first program BIOS/UEFI\n\nBIOS (Basic Input/Output System) - Legacy firmware (used in old computers)\nUEFI (Unified Extensible Firmware Interface) - Modern firmware\n\n\n\n\n\nPOST (Power-On Self Test):\n\nTests CPU, RAM, and storage devices\nValidates hardware components\n\nBoot Device Selection:\n\nReads Boot Order from firmware settings\nGPT drives: Looks for EFI System Partition\nMBR drives: Checks Master Boot Record in first sector\n\n\n\n\nCommon Bootloaders:\nLinux or Window bootloader, both can scan and start any OS windows or linux.\n- Linux: GRUB2, LILO, systemd-boot - Windows: Windows Boot Manager\nBootloader Tasks:\n\nScans partitions for installed operating systems\nPresents boot menu (if multiple OS found)\nLoads selected OS kernel into memory\n\n\n\n\nLinux OS Boot:\n\nKernel loads and initializes hardware\nsystemd starts (modern init system)\nSystem services launch\nUser login interface appears\n\nWindows OS Boot:\n\nNT Kernel (ntoskrnl.exe) loads\nHardware Abstraction Layer initializes\nRegistry and system drivers load\nSession Manager starts Windows subsystems\nWindows Logon presents login interface",
    "crumbs": [
      "Week1",
      "1. Windows To Linux",
      "Computer Boot Process"
    ]
  },
  {
    "objectID": "W1/1_window_to_linux/w1_1.html#partition-styles-vs-file-systems",
    "href": "W1/1_window_to_linux/w1_1.html#partition-styles-vs-file-systems",
    "title": "Computer Boot Process",
    "section": "",
    "text": "Partition styles define how a drive is divided into sections:\n\n\n\n\n\n\n\n\nFeature\nMBR\nGPT\n\n\n\n\nMax Partitions\n4 primary OR 3 primary + 1 extended\n128 primary\n\n\nMax Storage\n2 TB\n18+ exabytes\n\n\nBoot Support\nBIOS only\nBIOS + UEFI\n\n\n\n\n\n\nFile systems determine how data is stored within partitions:\n\n\n\nFile System\nOS\nUse Case\n\n\n\n\nNTFS\nWindows\nSystem drives, large files\n\n\nFAT32\nCross-platform\nUSB drives, compatibility\n\n\next4\nLinux\nLinux system drives\n\n\nAPFS\nmacOS\nmacOS system drives",
    "crumbs": [
      "Week1",
      "1. Windows To Linux",
      "Computer Boot Process"
    ]
  },
  {
    "objectID": "W1/1_window_to_linux/w1_1.html#boot-process-summary",
    "href": "W1/1_window_to_linux/w1_1.html#boot-process-summary",
    "title": "Computer Boot Process",
    "section": "",
    "text": "Phase\nComponent\nPurpose\n\n\n\n\n1\nFirmware (BIOS/UEFI)\nHardware initialization\n\n\n2\nPOST\nHardware verification\n\n\n3\nBootloader\nOS selection and loading\n\n\n4\nOS Kernel\nSystem initialization\n\n\n\n\n\n\n\n\nflowchart LR\n    A[Power On] --&gt; B[Bios/Uefi]\n    B --&gt; C[POST Check]\n    C --&gt; D[LocateDrive]\n    D --&gt; E[Run Bootloader]\n    E --&gt; F{Select OS}\n    F --&gt;|Linux| G[Kernel + systemd]\n    F --&gt;|Windows| H[NT Kernel]\n    G --&gt; I[User Login]\n    H --&gt; I",
    "crumbs": [
      "Week1",
      "1. Windows To Linux",
      "Computer Boot Process"
    ]
  },
  {
    "objectID": "W1/1_window_to_linux/w1_1.html#frequently-asked-questions",
    "href": "W1/1_window_to_linux/w1_1.html#frequently-asked-questions",
    "title": "Computer Boot Process",
    "section": "",
    "text": "Feature\nBIOS\nUEFI\n\n\n\n\nInterface\nText-only\nGraphical possible\n\n\nStorage Support\n2 TB max\nNo practical limit\n\n\nSecurity\nBasic\nSecure Boot\n\n\nSpeed\nSlower\nFaster\n\n\n\n\n\n\nPartition Style: Defines how the drive is divided (MBR vs GPT) File System: Defines how files are stored within each partition (NTFS, ext4, etc.)\n\n\n\nYes, by:\n\nInstalling each OS on separate partitions\nUsing a bootloader that detects all systems\nSelecting which OS to boot at startup",
    "crumbs": [
      "Week1",
      "1. Windows To Linux",
      "Computer Boot Process"
    ]
  },
  {
    "objectID": "W1/1_window_to_linux/w1_1.html#resources",
    "href": "W1/1_window_to_linux/w1_1.html#resources",
    "title": "Computer Boot Process",
    "section": "",
    "text": "Boot Process (English)\nBoot Process (Hindi)\nWindows Partitions (Hindi)\n\n\n\nBoot Process (English) \nBoot Process (Hindi) \nWindows Partitions (Hindi)",
    "crumbs": [
      "Week1",
      "1. Windows To Linux",
      "Computer Boot Process"
    ]
  },
  {
    "objectID": "W1/1_window_to_linux/dual_boot.html",
    "href": "W1/1_window_to_linux/dual_boot.html",
    "title": "Dual-Boot Setup Guide",
    "section": "",
    "text": "This guide will help you install another operating system (Linux) alongside your existing Windows installation.\n\n\n\n\n\n\nNoteDisclaimer\n\n\n\n\n\nDo this at your own risk. Please watch various YouTube videos and proceed with dual boot only when you are 100% sure about the process.\nIf you want to minimize the risk:\n1. Use External SSD/HDD to install Linux\n- This will prevent almost 99% of any accidental damage to your main system\n2. Install Kubuntu\n- Since dual boot requires creating partitions\n- Kubuntu has a Replace Partition option which avoids most of the headache\n\n\n\n\n\n\nNew SSD/HDD or at least 30GB free storage on your current Windows drive\n\nIf installing on the same SSD/HDD where Windows is installed:\nI recommend shrinking at least a 30GB partition\n\nHow To Shrink Partition\n\n\nPendrive/USB drive (8GB or larger)\nOne ISO file of any Linux distribution. Kubuntu\nSoftware to create a bootable USB drive. Ventoy\n\n\n\n\n\nInsert your USB drive and run Ventoy\nFormat your USB drive with Ventoy to make it bootable\nJust Copy and paste the Linux distribution ISO files into the USB drive\nTo be sure that iso file not got corrupted, you can run sha256sum and check both on local and server matches or not\n\n\n\n\n\nRestart your computer\nGo to BIOS/UEFI Settings\n\nOn HP laptops: restart and keep pressing the F10 key\n\nNavigate to Boot Settings\nDisable Secure Boot (you can re-enable it after installation)\n\n\n\n\nOnce your USB drive and laptop BIOS/UEFI settings are ready, proceed with the installation:\n\nRestart your computer\nGo to Boot Device Options\n\nOn HP laptops: keep pressing the F9 key\n\nChoose your USB Drive\nIf using Ventoy:\n\nIt will ask which OS to boot (based on the ISOs you‚Äôve copied)\n\nIf using BalenaEtcher: it will directly boot into that particular OS\nFor Kubuntu installation:\n\nClick on ‚ÄúInstall‚Äù\nSelect language, keyboard layout, etc.\nWhen choosing where to install or configure partitions:\n\nChoose Replace Partition ‚Äì&gt; then Select the newly created(shrinked) 30GB partition\nThe installer will format two partitions\n\nOne for your Linux installation that you just selected\nOne will be EFI partition of windows(first partition in most cases)\n\n\nChoose your username, password, etc., and proceed\nOnce installation is complete, your computer will automatically reboot or ask you to remove the USB drive and press Enter\nRemove the USB drive and press Enter to reboot\n\n\n\n\n\n\nRestart your computer\nGo to BIOS/UEFI settings\n\nYou can enable Secure Boot if desired\n\nMost importantly:\n\nNavigate to Boot Settings\nChange the boot order\nSet Ubuntu/Linux as the first boot option\n\n\n\n\n\n\nWhen you turn on your computer, you‚Äôll have the option to boot into Ubuntu or Windows\nImportant:\n\nThe first time you boot into Windows after installation:\n\nYou may see a blue screen asking for a BitLocker recovery key\nYou can easily log in with your Microsoft account and retrieve the password from BitLocker Recovery\nThis will only be asked once.\n\n\n\n\n\n\n\nBitLocker Recovery\nKubuntu ISO\nVentoy\n\n\n\n\nShrink Partition"
  },
  {
    "objectID": "W1/1_window_to_linux/dual_boot.html#requirements",
    "href": "W1/1_window_to_linux/dual_boot.html#requirements",
    "title": "Dual-Boot Setup Guide",
    "section": "",
    "text": "New SSD/HDD or at least 30GB free storage on your current Windows drive\n\nIf installing on the same SSD/HDD where Windows is installed:\nI recommend shrinking at least a 30GB partition\n\nHow To Shrink Partition\n\n\nPendrive/USB drive (8GB or larger)\nOne ISO file of any Linux distribution. Kubuntu\nSoftware to create a bootable USB drive. Ventoy"
  },
  {
    "objectID": "W1/1_window_to_linux/dual_boot.html#prepare-the-usb-drive",
    "href": "W1/1_window_to_linux/dual_boot.html#prepare-the-usb-drive",
    "title": "Dual-Boot Setup Guide",
    "section": "",
    "text": "Insert your USB drive and run Ventoy\nFormat your USB drive with Ventoy to make it bootable\nJust Copy and paste the Linux distribution ISO files into the USB drive\nTo be sure that iso file not got corrupted, you can run sha256sum and check both on local and server matches or not"
  },
  {
    "objectID": "W1/1_window_to_linux/dual_boot.html#prepare-your-laptopcomputer",
    "href": "W1/1_window_to_linux/dual_boot.html#prepare-your-laptopcomputer",
    "title": "Dual-Boot Setup Guide",
    "section": "",
    "text": "Restart your computer\nGo to BIOS/UEFI Settings\n\nOn HP laptops: restart and keep pressing the F10 key\n\nNavigate to Boot Settings\nDisable Secure Boot (you can re-enable it after installation)"
  },
  {
    "objectID": "W1/1_window_to_linux/dual_boot.html#installation-process",
    "href": "W1/1_window_to_linux/dual_boot.html#installation-process",
    "title": "Dual-Boot Setup Guide",
    "section": "",
    "text": "Once your USB drive and laptop BIOS/UEFI settings are ready, proceed with the installation:\n\nRestart your computer\nGo to Boot Device Options\n\nOn HP laptops: keep pressing the F9 key\n\nChoose your USB Drive\nIf using Ventoy:\n\nIt will ask which OS to boot (based on the ISOs you‚Äôve copied)\n\nIf using BalenaEtcher: it will directly boot into that particular OS\nFor Kubuntu installation:\n\nClick on ‚ÄúInstall‚Äù\nSelect language, keyboard layout, etc.\nWhen choosing where to install or configure partitions:\n\nChoose Replace Partition ‚Äì&gt; then Select the newly created(shrinked) 30GB partition\nThe installer will format two partitions\n\nOne for your Linux installation that you just selected\nOne will be EFI partition of windows(first partition in most cases)\n\n\nChoose your username, password, etc., and proceed\nOnce installation is complete, your computer will automatically reboot or ask you to remove the USB drive and press Enter\nRemove the USB drive and press Enter to reboot"
  },
  {
    "objectID": "W1/1_window_to_linux/dual_boot.html#final-computer-setup",
    "href": "W1/1_window_to_linux/dual_boot.html#final-computer-setup",
    "title": "Dual-Boot Setup Guide",
    "section": "",
    "text": "Restart your computer\nGo to BIOS/UEFI settings\n\nYou can enable Secure Boot if desired\n\nMost importantly:\n\nNavigate to Boot Settings\nChange the boot order\nSet Ubuntu/Linux as the first boot option"
  },
  {
    "objectID": "W1/1_window_to_linux/dual_boot.html#final-notes",
    "href": "W1/1_window_to_linux/dual_boot.html#final-notes",
    "title": "Dual-Boot Setup Guide",
    "section": "",
    "text": "When you turn on your computer, you‚Äôll have the option to boot into Ubuntu or Windows\nImportant:\n\nThe first time you boot into Windows after installation:\n\nYou may see a blue screen asking for a BitLocker recovery key\nYou can easily log in with your Microsoft account and retrieve the password from BitLocker Recovery\nThis will only be asked once."
  },
  {
    "objectID": "W1/1_window_to_linux/dual_boot.html#resources",
    "href": "W1/1_window_to_linux/dual_boot.html#resources",
    "title": "Dual-Boot Setup Guide",
    "section": "",
    "text": "BitLocker Recovery\nKubuntu ISO\nVentoy\n\n\n\n\nShrink Partition"
  },
  {
    "objectID": "Tds_Syllabus.html",
    "href": "Tds_Syllabus.html",
    "title": "üìë Course Roadmap",
    "section": "",
    "text": "üìå Quick Navigation\n\n\nWeek 1 ‚Üí Development Tools\nWeek 2 ‚Üí Deployment Tools\nWeek 3 ‚Üí LLM APIs\nüöÄ Project 1 ‚Üí ?\nWeek 4 ‚Üí LangChain\nWeek 5 ‚Üí LangGraph\nWeek 6 ‚Üí Quarto Websites & AWK\nüöÄ Project 2 ‚Üí ?\nWeek 7 ‚Üí ?\n\n\nüìå Small Projects\nW1-&gt; Simple ChatBot (No Memory)\nW2-&gt; Memory-Based ChatBot\nW3-&gt; PDF ChatBot\nW4-&gt; YouTube Playlist QA Assistant\nW5-&gt; SQL Query Agent\nW6-&gt; Portfolio Website\nVoice-enabled AI Chat Assistant\n\n\n\n\n\n\nIntro to Linux (Dual Boot, WSL)\n\nLanguage Setup & Config (bashrc, venv, nvm, etc.)\n\nJS ‚Üí nvm, node, npm, pnpm, yarn\n\nC/C++ ‚Üí gcc vs g++\n\nJava ‚Üí jdk, jre, jvm\n\nPython ‚Üí venv, pipx, conda, uv\n\n\nGit Basics\n\ninit, add, commit, push\n\nUndo/redo commits\n\n\nFastAPI Basics\n\nOllama Setup\n\n\nSmall Project ‚Üí Simple ChatBot (No Memory)\n- Build a FastAPI endpoint /ask with Ollama (Gemma).\n- Answers queries without history.\n\n\n\n\n\n\nAdvanced Git/GitHub\n\nBranching, Merging, Actions\n\nDocker/Podman\n\nDockerfile, Volumes, Networks\n\nAdvanced FastAPI\n\nDeployment Platforms\n\nVercel, Render, GitHub Pages\n\nEC2 Deployment with GitHub Actions + Docker\n\n\nSmall Project ‚Üí Memory-Based ChatBot\n- FastAPI endpoint /chat with Gemini LLM.\n- Maintains multi-turn memory in RAM.\n\n\n\n\n\n\nPrompt Engineering\n\nZero-shot, Few-shot\n\n\nFunction Calling\n\nBase64 Encoding\n\nVector Databases & RAG\n\nOpenAI API Docs\nPromptFoo\n\n\nSmall Project ‚Üí PDF ChatBot\n- Upload PDF ‚Üí Extract text ‚Üí Store in Vector DB.\n- Build RAG pipeline with Streamlit UI + FastAPI backend.\n- Multi-turn Q&A ‚Üí returns answer + page number.\n\n\n\n\n\n\n\n\n\n\nPydantic Models\n\nStructured Prompts & Output Parsers\n\nChains & Runnables\n\nText Splitters\n\nVectorStores in LangChain\n\nRAG + Tool Calling\n\n\nSmall Project ‚Üí YouTube Playlist QA Assistant\n- Scrape subtitles from each video of playlist.\n- Store in a vector DB.\n- Build chatbot ‚Üí answers video-specific queries.\n\n\n\n\n\n\n‚ö†Ô∏è I Don‚Äôt have much knowledge on this topic.\n\n\nSmall Project ‚Üí *SQL Query Agent\n- Convert natural language ‚Üí SQL query.\n- Run on DB ‚Üí Return tabular results**.\n\n\n\n\n\n\nQuarto + GitHub Pages\n\nRevealJS Presentations\n\nMarkdown ‚Üí Website conversion\n\nExcel Toolpack (Correlation, Regression, Data Analysis)\n\nAWK Basics (text processing)\n\n\nSmall Project ‚Üí Portfolio Website\n- Publish your portfolio with Quarto + GitHub Pages.\n\n\n\n\n\n\n\n\n\n\n\n\n\nVoice-enabled AI Chat Assistant - üé§ Integrate STT + TTS + LLM.\n- Speak ‚Üí Listen ‚Üí Respond in real-time."
  },
  {
    "objectID": "Tds_Syllabus.html#index",
    "href": "Tds_Syllabus.html#index",
    "title": "üìë Course Roadmap",
    "section": "",
    "text": "üìå Quick Navigation\n\n\nWeek 1 ‚Üí Development Tools\nWeek 2 ‚Üí Deployment Tools\nWeek 3 ‚Üí LLM APIs\nüöÄ Project 1 ‚Üí ?\nWeek 4 ‚Üí LangChain\nWeek 5 ‚Üí LangGraph\nWeek 6 ‚Üí Quarto Websites & AWK\nüöÄ Project 2 ‚Üí ?\nWeek 7 ‚Üí ?\n\n\nüìå Small Projects\nW1-&gt; Simple ChatBot (No Memory)\nW2-&gt; Memory-Based ChatBot\nW3-&gt; PDF ChatBot\nW4-&gt; YouTube Playlist QA Assistant\nW5-&gt; SQL Query Agent\nW6-&gt; Portfolio Website\nVoice-enabled AI Chat Assistant"
  },
  {
    "objectID": "Tds_Syllabus.html#week-1-development-tools",
    "href": "Tds_Syllabus.html#week-1-development-tools",
    "title": "üìë Course Roadmap",
    "section": "",
    "text": "Intro to Linux (Dual Boot, WSL)\n\nLanguage Setup & Config (bashrc, venv, nvm, etc.)\n\nJS ‚Üí nvm, node, npm, pnpm, yarn\n\nC/C++ ‚Üí gcc vs g++\n\nJava ‚Üí jdk, jre, jvm\n\nPython ‚Üí venv, pipx, conda, uv\n\n\nGit Basics\n\ninit, add, commit, push\n\nUndo/redo commits\n\n\nFastAPI Basics\n\nOllama Setup\n\n\nSmall Project ‚Üí Simple ChatBot (No Memory)\n- Build a FastAPI endpoint /ask with Ollama (Gemma).\n- Answers queries without history."
  },
  {
    "objectID": "Tds_Syllabus.html#week-2-deployment-tools",
    "href": "Tds_Syllabus.html#week-2-deployment-tools",
    "title": "üìë Course Roadmap",
    "section": "",
    "text": "Advanced Git/GitHub\n\nBranching, Merging, Actions\n\nDocker/Podman\n\nDockerfile, Volumes, Networks\n\nAdvanced FastAPI\n\nDeployment Platforms\n\nVercel, Render, GitHub Pages\n\nEC2 Deployment with GitHub Actions + Docker\n\n\nSmall Project ‚Üí Memory-Based ChatBot\n- FastAPI endpoint /chat with Gemini LLM.\n- Maintains multi-turn memory in RAM."
  },
  {
    "objectID": "Tds_Syllabus.html#week-3-llm-apis",
    "href": "Tds_Syllabus.html#week-3-llm-apis",
    "title": "üìë Course Roadmap",
    "section": "",
    "text": "Prompt Engineering\n\nZero-shot, Few-shot\n\n\nFunction Calling\n\nBase64 Encoding\n\nVector Databases & RAG\n\nOpenAI API Docs\nPromptFoo\n\n\nSmall Project ‚Üí PDF ChatBot\n- Upload PDF ‚Üí Extract text ‚Üí Store in Vector DB.\n- Build RAG pipeline with Streamlit UI + FastAPI backend.\n- Multi-turn Q&A ‚Üí returns answer + page number."
  },
  {
    "objectID": "Tds_Syllabus.html#week-4-langchain",
    "href": "Tds_Syllabus.html#week-4-langchain",
    "title": "üìë Course Roadmap",
    "section": "",
    "text": "Pydantic Models\n\nStructured Prompts & Output Parsers\n\nChains & Runnables\n\nText Splitters\n\nVectorStores in LangChain\n\nRAG + Tool Calling\n\n\nSmall Project ‚Üí YouTube Playlist QA Assistant\n- Scrape subtitles from each video of playlist.\n- Store in a vector DB.\n- Build chatbot ‚Üí answers video-specific queries."
  },
  {
    "objectID": "Tds_Syllabus.html#week-5-langgraph",
    "href": "Tds_Syllabus.html#week-5-langgraph",
    "title": "üìë Course Roadmap",
    "section": "",
    "text": "‚ö†Ô∏è I Don‚Äôt have much knowledge on this topic.\n\n\nSmall Project ‚Üí *SQL Query Agent\n- Convert natural language ‚Üí SQL query.\n- Run on DB ‚Üí Return tabular results**."
  },
  {
    "objectID": "Tds_Syllabus.html#week-6-quarto-websites-awk",
    "href": "Tds_Syllabus.html#week-6-quarto-websites-awk",
    "title": "üìë Course Roadmap",
    "section": "",
    "text": "Quarto + GitHub Pages\n\nRevealJS Presentations\n\nMarkdown ‚Üí Website conversion\n\nExcel Toolpack (Correlation, Regression, Data Analysis)\n\nAWK Basics (text processing)\n\n\nSmall Project ‚Üí Portfolio Website\n- Publish your portfolio with Quarto + GitHub Pages."
  },
  {
    "objectID": "Tds_Syllabus.html#bonus-project-optional",
    "href": "Tds_Syllabus.html#bonus-project-optional",
    "title": "üìë Course Roadmap",
    "section": "",
    "text": "Voice-enabled AI Chat Assistant - üé§ Integrate STT + TTS + LLM.\n- Speak ‚Üí Listen ‚Üí Respond in real-time."
  },
  {
    "objectID": "PydanticAi/6_media.html",
    "href": "PydanticAi/6_media.html",
    "title": "Multimodal Input in Pydantic AI",
    "section": "",
    "text": "Modern LLMs can understand images, audio, video, and documents in addition to text.\n\n\n\nTwo methods: Use URL or local file.\n\n\nfrom dotenv import load_dotenv\nload_dotenv()\nfrom pydantic_ai import Agent, ImageUrl\n\nagent = Agent(model='openai:gpt-5-nano')\n\nresult = agent.run_sync(\n    [\n        'What company is this logo from?',\n        ImageUrl(url='https://images.moneycontrol.com/static-mcnews/2024/02/rbi.jpeg?impolicy=website&width=770&height=431')\n    ]\n)\n\nprint(result.output)\n# Output: Reserve Bank of India (RBI) ‚Äî the central bank of India.\n\n\n\nfrom dotenv import load_dotenv\nload_dotenv()\nfrom pathlib import Path\nfrom pydantic_ai import Agent, BinaryContent\n\n# Read local image\nimage_data = Path('rbi.png').read_bytes()\n\nagent = Agent(model='openai:gpt-5-nano')\n\nresult = agent.run_sync(\n    [\n        'What company is this logo from?',\n        BinaryContent(data=image_data, media_type='image/png')\n    ]\n)\n\nprint(result.output)\n# Output: The Reserve Bank of India (RBI).\nKey points:\n\nUse ImageUrl for web images\nUse BinaryContent for local files\nSpecify correct media_type (e.g., ‚Äòimage/png‚Äô, ‚Äòimage/jpeg‚Äô)\n\n\n\n\n\n\nWorks the same way as images.\nfrom pydantic_ai import Agent, AudioUrl, ImageUrl\n\nAudioUrl(url='https://example.com/audio.mp3')\nVideoUrl(url='https://example.com/video.mp4')\nDocumentUrl(url='https://example.com/paper.pdf')\n\nfrom pathlib import Path\nfrom pydantic_ai import Agent, BinaryContent\npdf_data = Path('report.pdf').read_bytes()\n\nBinaryContent(data=video_data, media_type='video/mp4')\nBinaryContent(data=audio_data, media_type='audio/mpeg')\nBinaryContent(data=pdf_data, media_type='application/pdf')\n\n\n\nYou can pass multiple files in a single request.\nfrom dotenv import load_dotenv\nload_dotenv()\nfrom pathlib import Path\nfrom pydantic_ai import Agent, ImageUrl, DocumentUrl\n\nagent = Agent(model='openai:gpt-5-nano')\n\nresult = agent.run_sync(\n    [\n        'Compare the logo in the image with the company name in the PDF',\n        ImageUrl(url='https://images.moneycontrol.com/static-mcnews/2024/02/rbi.jpeg?impolicy=website&width=770&height=431'),\n        DocumentUrl(url='https://morth.nic.in/sites/default/files/dd12-13_0.pdf')\n    ]\n)\n\nprint(result.output)\n\n'''\n- Logo in the image: Reserve Bank of India (RBI) emblem ‚Äî circular orange seal with a tiger and a palm tree, text around the edge in English and Hindi: ‚ÄúRESERVE BANK OF INDIA‚Äù / ‚Äú‡§≠‡§æ‡§∞‡§§‡•Ä‡§Ø ‡§∞‡§ø‡§ú‡§º‡§∞‡•ç‡§µ ‡§¨‡•à‡§Ç‡§ï‚Äù.\n\n- Company name in the PDF: The page text shown is ‚ÄúDummy PDF file‚Äù (no actual company name is present).\n\nConclusion: The RBI logo does not match the company name shown in the PDF. The PDF appears to be a placeholder with no real company name. If you provide the correct PDF or specify the expected company name, I can re-check.\n'''\n\n\n\n\nDefault: Pydantic AI downloads file content from URLs and sends it to the model.\nExceptions:\n\nAnthropic: PDFs via DocumentUrl are sent as URLs (not downloaded)\nGoogle Vertex AI: All URLs sent directly (not downloaded)\nGoogle GLA: YouTube URLs sent directly\n\n\n\nIf the model can‚Äôt access a URL, force local download:\nfrom pydantic_ai import Agent, ImageUrl\n\nagent = Agent(model='google-gla:gemini-2.5-flash')\n\nresult = agent.run_sync(\n    [\n        'Describe this image',\n        ImageUrl(url='https://private-site.com/image.png', force_download=True)\n    ]\n)\nSetting force_download=True makes Pydantic AI download the file first.\n\n\n\n\n\n\n\n\nInput Type\nURL Class\nBinaryContent\nMedia Type Example\n\n\n\n\nImage\nImageUrl\nSupported\nimage/png, image/jpeg\n\n\nAudio\nAudioUrl\nSupported\naudio/mpeg, audio/wav\n\n\nVideo\nVideoUrl\nSupported\nvideo/mp4, video/avi\n\n\nDocument\nDocumentUrl\nSupported\napplication/pdf\n\n\n\n\n\n\n\n\nTwo options: URL-based or local file-based input\nUse BinaryContent for local files with correct media_type\nModel support varies ‚Üí Check model documentation first\nMultiple files can be passed in one request\nDownload behavior differs by provider",
    "crumbs": [
      "Projects",
      "Media"
    ]
  },
  {
    "objectID": "PydanticAi/6_media.html#image-input",
    "href": "PydanticAi/6_media.html#image-input",
    "title": "Multimodal Input in Pydantic AI",
    "section": "",
    "text": "Two methods: Use URL or local file.\n\n\nfrom dotenv import load_dotenv\nload_dotenv()\nfrom pydantic_ai import Agent, ImageUrl\n\nagent = Agent(model='openai:gpt-5-nano')\n\nresult = agent.run_sync(\n    [\n        'What company is this logo from?',\n        ImageUrl(url='https://images.moneycontrol.com/static-mcnews/2024/02/rbi.jpeg?impolicy=website&width=770&height=431')\n    ]\n)\n\nprint(result.output)\n# Output: Reserve Bank of India (RBI) ‚Äî the central bank of India.\n\n\n\nfrom dotenv import load_dotenv\nload_dotenv()\nfrom pathlib import Path\nfrom pydantic_ai import Agent, BinaryContent\n\n# Read local image\nimage_data = Path('rbi.png').read_bytes()\n\nagent = Agent(model='openai:gpt-5-nano')\n\nresult = agent.run_sync(\n    [\n        'What company is this logo from?',\n        BinaryContent(data=image_data, media_type='image/png')\n    ]\n)\n\nprint(result.output)\n# Output: The Reserve Bank of India (RBI).\nKey points:\n\nUse ImageUrl for web images\nUse BinaryContent for local files\nSpecify correct media_type (e.g., ‚Äòimage/png‚Äô, ‚Äòimage/jpeg‚Äô)",
    "crumbs": [
      "Projects",
      "Media"
    ]
  },
  {
    "objectID": "PydanticAi/6_media.html#audiovideodocument-input",
    "href": "PydanticAi/6_media.html#audiovideodocument-input",
    "title": "Multimodal Input in Pydantic AI",
    "section": "",
    "text": "Works the same way as images.\nfrom pydantic_ai import Agent, AudioUrl, ImageUrl\n\nAudioUrl(url='https://example.com/audio.mp3')\nVideoUrl(url='https://example.com/video.mp4')\nDocumentUrl(url='https://example.com/paper.pdf')\n\nfrom pathlib import Path\nfrom pydantic_ai import Agent, BinaryContent\npdf_data = Path('report.pdf').read_bytes()\n\nBinaryContent(data=video_data, media_type='video/mp4')\nBinaryContent(data=audio_data, media_type='audio/mpeg')\nBinaryContent(data=pdf_data, media_type='application/pdf')",
    "crumbs": [
      "Projects",
      "Media"
    ]
  },
  {
    "objectID": "PydanticAi/6_media.html#multiple-inputs",
    "href": "PydanticAi/6_media.html#multiple-inputs",
    "title": "Multimodal Input in Pydantic AI",
    "section": "",
    "text": "You can pass multiple files in a single request.\nfrom dotenv import load_dotenv\nload_dotenv()\nfrom pathlib import Path\nfrom pydantic_ai import Agent, ImageUrl, DocumentUrl\n\nagent = Agent(model='openai:gpt-5-nano')\n\nresult = agent.run_sync(\n    [\n        'Compare the logo in the image with the company name in the PDF',\n        ImageUrl(url='https://images.moneycontrol.com/static-mcnews/2024/02/rbi.jpeg?impolicy=website&width=770&height=431'),\n        DocumentUrl(url='https://morth.nic.in/sites/default/files/dd12-13_0.pdf')\n    ]\n)\n\nprint(result.output)\n\n'''\n- Logo in the image: Reserve Bank of India (RBI) emblem ‚Äî circular orange seal with a tiger and a palm tree, text around the edge in English and Hindi: ‚ÄúRESERVE BANK OF INDIA‚Äù / ‚Äú‡§≠‡§æ‡§∞‡§§‡•Ä‡§Ø ‡§∞‡§ø‡§ú‡§º‡§∞‡•ç‡§µ ‡§¨‡•à‡§Ç‡§ï‚Äù.\n\n- Company name in the PDF: The page text shown is ‚ÄúDummy PDF file‚Äù (no actual company name is present).\n\nConclusion: The RBI logo does not match the company name shown in the PDF. The PDF appears to be a placeholder with no real company name. If you provide the correct PDF or specify the expected company name, I can re-check.\n'''",
    "crumbs": [
      "Projects",
      "Media"
    ]
  },
  {
    "objectID": "PydanticAi/6_media.html#url-download-behavior",
    "href": "PydanticAi/6_media.html#url-download-behavior",
    "title": "Multimodal Input in Pydantic AI",
    "section": "",
    "text": "Default: Pydantic AI downloads file content from URLs and sends it to the model.\nExceptions:\n\nAnthropic: PDFs via DocumentUrl are sent as URLs (not downloaded)\nGoogle Vertex AI: All URLs sent directly (not downloaded)\nGoogle GLA: YouTube URLs sent directly\n\n\n\nIf the model can‚Äôt access a URL, force local download:\nfrom pydantic_ai import Agent, ImageUrl\n\nagent = Agent(model='google-gla:gemini-2.5-flash')\n\nresult = agent.run_sync(\n    [\n        'Describe this image',\n        ImageUrl(url='https://private-site.com/image.png', force_download=True)\n    ]\n)\nSetting force_download=True makes Pydantic AI download the file first.",
    "crumbs": [
      "Projects",
      "Media"
    ]
  },
  {
    "objectID": "PydanticAi/6_media.html#summary-table",
    "href": "PydanticAi/6_media.html#summary-table",
    "title": "Multimodal Input in Pydantic AI",
    "section": "",
    "text": "Input Type\nURL Class\nBinaryContent\nMedia Type Example\n\n\n\n\nImage\nImageUrl\nSupported\nimage/png, image/jpeg\n\n\nAudio\nAudioUrl\nSupported\naudio/mpeg, audio/wav\n\n\nVideo\nVideoUrl\nSupported\nvideo/mp4, video/avi\n\n\nDocument\nDocumentUrl\nSupported\napplication/pdf",
    "crumbs": [
      "Projects",
      "Media"
    ]
  },
  {
    "objectID": "PydanticAi/6_media.html#key-takeaways",
    "href": "PydanticAi/6_media.html#key-takeaways",
    "title": "Multimodal Input in Pydantic AI",
    "section": "",
    "text": "Two options: URL-based or local file-based input\nUse BinaryContent for local files with correct media_type\nModel support varies ‚Üí Check model documentation first\nMultiple files can be passed in one request\nDownload behavior differs by provider",
    "crumbs": [
      "Projects",
      "Media"
    ]
  },
  {
    "objectID": "PydanticAi/4_tool_calling.html",
    "href": "PydanticAi/4_tool_calling.html",
    "title": "Function Calling vs Tool Calling in AI",
    "section": "",
    "text": "Originated with OpenAI‚Äôs ‚Äúfunction calling‚Äù API (2023).\nThe model doesn‚Äôt execute anything itself.\nInstead, it outputs a structured JSON object (arguments) describing how a function should be called.\nThe developer is responsible for implementing the function, running it, and then optionally passing the result back to the model.\n\nExample flow:\nUser: What's the weather in Delhi?\nModel: {\"name\": \"get_weather\", \"arguments\": {\"location\": \"Delhi\"}}\nYour code: calls `get_weather(\"Delhi\")`, gets result, sends back.\nSo the model is just an intent recognizer + argument filler.\n\n\n\n\n\nA newer generalization of function calling (introduced in 2024 by OpenAI, and also adopted by frameworks like LangChain, LlamaIndex, Pydantic-AI).\nTools are functions, APIs, or even external systems that the model can invoke dynamically.\nInstead of just spitting out JSON, the framework integrates execution:\n\nThe model says ‚Äúuse tool X‚Äù.\nThe framework runs the tool automatically.\nThe result can be given back to the model for reasoning.\n\n\nExample flow (tool calling):\nUser: What's the weather in Delhi?\nModel (internal reasoning): I should call the \"WeatherTool\".\nFramework: calls WeatherTool(location=\"Delhi\") ‚Üí 30¬∞C\nModel: \"It's 30¬∞C in Delhi right now.\"\nSo tool calling is function calling + automatic execution + integration with the agent loop.",
    "crumbs": [
      "Projects",
      "Tool Calling"
    ]
  },
  {
    "objectID": "PydanticAi/4_tool_calling.html#what-are-function-tools",
    "href": "PydanticAi/4_tool_calling.html#what-are-function-tools",
    "title": "Function Calling vs Tool Calling in AI",
    "section": "What Are Function Tools?",
    "text": "What Are Function Tools?\nFunction tools let AI models perform actions and retrieve information during a conversation to generate better responses. Think of them as giving the AI the ability to ‚Äúcall functions‚Äù when it needs extra data or capabilities.\nUse cases:\n\nWhen the model needs to fetch data dynamically\nWhen context is too large to fit in prompt\nTo make AI behavior more deterministic",
    "crumbs": [
      "Projects",
      "Tool Calling"
    ]
  },
  {
    "objectID": "PydanticAi/4_tool_calling.html#key-concepts",
    "href": "PydanticAi/4_tool_calling.html#key-concepts",
    "title": "Function Calling vs Tool Calling in AI",
    "section": "Key Concepts",
    "text": "Key Concepts\nTool vs Function Calling: In Pydantic AI, these terms are interchangeable. The model ‚Äúcalls‚Äù Python functions registered as ‚Äútools‚Äù.\nRunContext: A special parameter that gives tools access to dependencies, retry info, and other agent context.",
    "crumbs": [
      "Projects",
      "Tool Calling"
    ]
  },
  {
    "objectID": "PydanticAi/4_tool_calling.html#three-ways-to-register-tools",
    "href": "PydanticAi/4_tool_calling.html#three-ways-to-register-tools",
    "title": "Function Calling vs Tool Calling in AI",
    "section": "Three Ways to Register Tools",
    "text": "Three Ways to Register Tools\n\nMethod 1: Using Decorators\nimport random\nfrom pydantic_ai import Agent, RunContext\n\nagent = Agent(\n    'openai:gpt-5-nano',\n    deps_type=str,  # Dependencies type\n    system_prompt=\"You are a helpful assistant.\"\n)\n\n# Tool WITHOUT context (simple function)\n@agent.tool_plain\ndef roll_dice() -&gt; str:\n    \"\"\"Roll a six-sided die.\"\"\"\n    return str(random.randint(1, 6))\n\n# Tool WITH context (needs RunContext)\n@agent.tool\ndef get_user_name(ctx: RunContext[str]) -&gt; str:\n    \"\"\"Get the user's name from context.\"\"\"\n    return ctx.deps\n\n# Run the agent\nresult = agent.run_sync(\"Roll a dice\", deps=\"Alice\")\nprint(result.output)\nKey differences:\n\n@agent.tool_plain ‚Üí No context needed\n@agent.tool ‚Üí Needs RunContext for dependencies\n\n\n\nMethod 2: Via Agent Constructor\nfrom pydantic_ai import Agent, RunContext\n\n# Define tools as regular functions\ndef get_weather(city: str) -&gt; str:\n    \"\"\"Get weather for a city.\"\"\"\n    return f\"Sunny in {city}\"\n\ndef get_user_location(ctx: RunContext[str]) -&gt; str:\n    \"\"\"Get user location from context.\"\"\"\n    return ctx.deps\n\n# Pass tools to Agent\nagent = Agent(\n    'openai:gpt-5-nano',\n    deps_type=str,\n    tools=[get_weather, get_user_location],  # List of functions\n    system_prompt=\"Help with weather info\"\n)\n\nresult = agent.run_sync(\"What's the weather?\", deps=\"Mumbai\")\nThis method is better for reusing tools across multiple agents.\n\n\nMethod 3: Using Tool Class\nfrom pydantic_ai import Agent, Tool\n\ndef calculate(x: int, y: int) -&gt; int:\n    \"\"\"Add two numbers.\"\"\"\n    return x + y\n\nagent = Agent(\n    'openai:gpt-5-nano',\n    tools=[\n        Tool(calculate, takes_ctx=False)  # Explicit control\n    ]\n)\n\nresult = agent.run_sync(\"What is 5 + 3?\")\nUse Tool class for fine-grained control over tool behavior.",
    "crumbs": [
      "Projects",
      "Tool Calling"
    ]
  },
  {
    "objectID": "PydanticAi/4_tool_calling.html#how-tool-calling-works",
    "href": "PydanticAi/4_tool_calling.html#how-tool-calling-works",
    "title": "Function Calling vs Tool Calling in AI",
    "section": "How Tool Calling Works",
    "text": "How Tool Calling Works\nExecution flow:\n\nUser sends prompt to agent\nLLM decides which tool to call\nAgent executes the tool function\nTool returns result to LLM\nLLM uses result to generate final response\n\n# Example: Dice game\nagent = Agent(\n    'google-gla:gemini-2.5-flash',\n    deps_type=str,\n    system_prompt=\"You're a dice game. Roll and check if it matches user's guess.\"\n)\n\n@agent.tool_plain\ndef roll_dice() -&gt; str:\n    \"\"\"Roll a six-sided die.\"\"\"\n    return str(random.randint(1, 6))\n\n@agent.tool\ndef get_player_name(ctx: RunContext[str]) -&gt; str:\n    \"\"\"Get player's name.\"\"\"\n    return ctx.deps\n\nresult = agent.run_sync('My guess is 4', deps='Anne')\nprint(result.output)\n# Output: \"Congratulations Anne, you guessed correctly!\"",
    "crumbs": [
      "Projects",
      "Tool Calling"
    ]
  },
  {
    "objectID": "PydanticAi/4_tool_calling.html#tool-schemas-and-docstrings",
    "href": "PydanticAi/4_tool_calling.html#tool-schemas-and-docstrings",
    "title": "Function Calling vs Tool Calling in AI",
    "section": "Tool Schemas and Docstrings",
    "text": "Tool Schemas and Docstrings\nPydantic AI automatically generates JSON schemas from function signatures and docstrings.\n@agent.tool_plain(docstring_format='google', require_parameter_descriptions=True)\ndef search_products(\n    category: str, \n    max_price: float, \n    in_stock: bool\n) -&gt; list[str]:\n    \"\"\"Search for products.\n    \n    Args:\n        category: Product category to search\n        max_price: Maximum price limit\n        in_stock: Only show available items\n    \"\"\"\n    return [\"Product A\", \"Product B\"]\nThe LLM receives:\n{\n  \"name\": \"search_products\",\n  \"description\": \"Search for products\",\n  \"parameters\": {\n    \"category\": {\"type\": \"string\", \"description\": \"Product category to search\"},\n    \"max_price\": {\"type\": \"number\", \"description\": \"Maximum price limit\"},\n    \"in_stock\": {\"type\": \"boolean\", \"description\": \"Only show available items\"}\n  }\n}\nSupported docstring formats: Google, NumPy, Sphinx.",
    "crumbs": [
      "Projects",
      "Tool Calling"
    ]
  },
  {
    "objectID": "PydanticAi/4_tool_calling.html#tool-return-values",
    "href": "PydanticAi/4_tool_calling.html#tool-return-values",
    "title": "Function Calling vs Tool Calling in AI",
    "section": "Tool Return Values",
    "text": "Tool Return Values\nTools can return any JSON-serializable data:\n\nStrings, numbers, booleans\nLists and dictionaries\nPydantic models\nDataclasses\n\n@agent.tool_plain\ndef get_user_data() -&gt; dict:\n    \"\"\"Fetch user information.\"\"\"\n    return {\n        \"name\": \"Alice\",\n        \"age\": 25,\n        \"city\": \"Mumbai\"\n    }",
    "crumbs": [
      "Projects",
      "Tool Calling"
    ]
  },
  {
    "objectID": "PydanticAi/4_tool_calling.html#complete-example-weather-assistant",
    "href": "PydanticAi/4_tool_calling.html#complete-example-weather-assistant",
    "title": "Function Calling vs Tool Calling in AI",
    "section": "Complete Example: Weather Assistant",
    "text": "Complete Example: Weather Assistant\nfrom dotenv import load_dotenv\nload_dotenv()\nfrom pydantic_ai import Agent, RunContext\n\nagent = Agent(\n    'openai:gpt-5-nano',\n    deps_type=dict,\n    system_prompt=\"You are a weather assistant. Use tools to fetch weather data. Must mention country name which is provided by user, despite being correct or incorrect.\"\n)\n\n@agent.tool\ndef get_current_weather(ctx: RunContext[dict], city: str) -&gt; str:\n    \"\"\"Get current weather for a city.\n    \n    Args:\n        city: City name to get weather for\n    \"\"\"\n    # In real app, call weather API\n    user_location = ctx.deps.get('location', 'CHINA')\n    return f\"Weather in {city}: 28¬∞C, Sunny (requested from {user_location})\"\n\n@agent.tool_plain\ndef list_cities() -&gt; list[str]:\n    \"\"\"List available cities for weather data.\"\"\"\n    return [\"Mumbai\", \"Delhi\", \"Bangalore\", \"Chennai\"]\n\n# Run agent\nuser_deps = {\"locatin\": \"India\", \"user_id\": 123}\nresult = agent.run_sync(\n    \"What's the weather in Mumbai?\", \n    deps=user_deps\n)\nprint(result.output)\nOutput\nWeather in Mumbai, India: 28¬∞C, Sunny. \n\nNote: The tool output sometimes shows an odd source tag like ‚Äúrequested from CHINA‚Äù; Mumbai is in India. If you‚Äôd like, I can pull a second update.",
    "crumbs": [
      "Projects",
      "Tool Calling"
    ]
  },
  {
    "objectID": "PydanticAi/4_tool_calling.html#key-takeaways",
    "href": "PydanticAi/4_tool_calling.html#key-takeaways",
    "title": "Function Calling vs Tool Calling in AI",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nTwo decorator types: @agent.tool (with context) and @agent.tool_plain (without)\nRegistration methods: Decorators, constructor list, or Tool class\nRunContext gives tools access to dependencies and agent state\nDocstrings matter: They become part of the schema the LLM sees\nReturn JSON-serializable data from tools\nTools make AI agents dynamic and context-aware",
    "crumbs": [
      "Projects",
      "Tool Calling"
    ]
  },
  {
    "objectID": "PydanticAi/2_sys_ins.html",
    "href": "PydanticAi/2_sys_ins.html",
    "title": "Agent System Prompt vs Instruction",
    "section": "",
    "text": "from dotenv import load_dotenv\nfrom pprint import pprint\nload_dotenv()\n\nfrom pydantic_ai import Agent\n\nprint(\"=\" * 80)\nprint(\"SCENARIO 1: Using system_prompt\")\nprint(\"=\" * 80)\n\n# Agent 1: Sales (system_prompt)\nsales_agent = Agent(\n    'openai:gpt-5-nano',\n    system_prompt=\"You are a SALES agent. Always mention 'premium features'. Reply in short\"\n)\n\n# Agent 2: Support (system_prompt)\nsupport_agent = Agent(\n    'openai:gpt-5-nano',\n    system_prompt=\"You are a SUPPORT agent. Always say 'let me help you troubleshoot'. Reply in short\"\n)\n\n# Customer talks to sales first\nresult1 = sales_agent.run_sync('I want to buy something')\nprint(\"\\n[Sales Response]:\", result1.output)\n\n# Customer is transferred to support WITH HISTORY\nresult2 = support_agent.run_sync(\n    'How do I use it?',\n    message_history=result1.new_messages()\n)\nprint(\"\\n[Support Response]:\", result2.output)\n\nprint(\"\\n[MESSAGE HISTORY - What support_agent sees]:\")\nprint(result2.all_messages())\n\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"SCENARIO 2: Using instructions\")\nprint(\"=\" * 80)\n\n# Agent 1: Sales (instructions)\nsales_agent2 = Agent(\n    'openai:gpt-5-nano',\n    instructions=\"You are a SALES agent. Always mention 'premium features'. Reply in short\"\n)\n\n# Agent 2: Support (instructions)\nsupport_agent2 = Agent(\n    'openai:gpt-5-nano',\n    instructions=\"You are a SUPPORT agent. Always say 'let me help you troubleshoot'. Reply in short\"\n)\n\n# Customer talks to sales first\nresult3 = sales_agent2.run_sync('I want to buy something')\nprint(\"\\n[Sales Response]:\", result3.output)\n\n# Customer is transferred to support WITH HISTORY\nresult4 = support_agent2.run_sync(\n    'How do I use it?',\n    message_history=result3.new_messages()\n)\nprint(\"\\n[Support Response]:\", result4.output)\n\nprint(\"\\n[MESSAGE HISTORY - What support_agent2 sees]:\")\nprint(result4.all_messages())\nOutput",
    "crumbs": [
      "Projects",
      "Role To AI"
    ]
  },
  {
    "objectID": "PydanticAi/2_sys_ins.html#the-critical-difference-explained",
    "href": "PydanticAi/2_sys_ins.html#the-critical-difference-explained",
    "title": "Agent System Prompt vs Instruction",
    "section": "The Critical Difference Explained",
    "text": "The Critical Difference Explained\n\nWith system_prompt:\nStorage: SystemPromptPart inside the parts list of ModelRequest\nBehavior with message_history:\n\nWhen you provide message_history, Pydantic AI says: ‚ÄúThere‚Äôs already a SystemPromptPart in the history, so I won‚Äôt generate a new one‚Äù\nResult: Support agent doesn‚Äôt add its own system prompt\nProblem: Support agent is constrained by the OLD sales system prompt\n\nFrom documentation:\n&gt; ‚ÄúIf message_history is set and not empty, a new system prompt is not generated ‚Äî we assume the existing message history includes a system prompt.‚Äù\n\n\n\nWith instructions:\nStorage: instructions field as metadata on the ModelRequest object (not in parts)\nBehavior with message_history:\n\nInstructions are stored SEPARATELY from the message parts\nEach agent adds its OWN instructions to its ModelRequest\nOld instructions remain visible in history BUT as metadata\nNew instructions are ALWAYS added for the current agent\n\nKey insight: Instructions don‚Äôt get ‚Äúfiltered out‚Äù ‚Äî they‚Äôre just stored differently. Both old and new instructions exist in the history, but as separate metadata fields on their respective requests.\nFrom documentation:\n&gt; ‚ÄúWhen an explicit message_history is provided, instructions from any existing messages in the history are NOT included in the request to the model ‚Äî only the instructions of the current agent are included.‚Äù",
    "crumbs": [
      "Projects",
      "Role To AI"
    ]
  },
  {
    "objectID": "PydanticAi/2_sys_ins.html#the-bottom-line",
    "href": "PydanticAi/2_sys_ins.html#the-bottom-line",
    "title": "Agent System Prompt vs Instruction",
    "section": "The Bottom Line",
    "text": "The Bottom Line\nUse instructions because:\n\nEach agent always gets its own instructions field, even with message_history\nNo risk of the second agent being ‚Äústuck‚Äù with the first agent‚Äôs guidance\nCleaner data structure (metadata vs content)\n\nUse system_prompt only when:\n\nYou want a single agent to maintain one consistent role across a long conversation\nYou don‚Äôt need to change the system guidance mid-conversation\n\nThank you for providing the actual output ‚Äî this clarified everything! The key insight is that instructions are stored as metadata, not as message parts.",
    "crumbs": [
      "Projects",
      "Role To AI"
    ]
  },
  {
    "objectID": "Podman/5_MiniProject4.html",
    "href": "Podman/5_MiniProject4.html",
    "title": "TDS by HRM",
    "section": "",
    "text": "Dockerfile\nFROM docker.io/alpine/ollama:0.12.0\n\n# Set working directory\nWORKDIR /app\n\n# Install Python and system dependencies\nRUN apk add --no-cache \\\n    python3 \\\n    py3-pip \\\n    curl \\\n    bash\n\n# Create symbolic link for python command\nRUN ln -sf /usr/bin/python3 /usr/bin/python\n\n# Copy requirements and install Python dependencies\nCOPY requirements.txt .\nRUN pip3 install --no-cache-dir -r requirements.txt\n\n# Copy model installation script\nCOPY install_model.sh .\nRUN chmod +x install_model.sh\n\n# Install model during build time\nRUN ./install_model.sh\n\n# Copy application files\nCOPY app.py .\nCOPY start.sh .\n\n# Make startup script executable\nRUN chmod +x start.sh\n\n# Expose only Flask port\nEXPOSE 5000\n\n# Override the original entrypoint completely\nENTRYPOINT [\"./start.sh\"]\napp.py\nimport requests\n\ndef gemma3_response(user_prompt, summary_type):\n    url = \"http://localhost:11434/api/chat\"\n    \n    payload = {\n        \"model\": \"gemma3:270m\",\n        \"messages\": [\n            {\n                \"role\": \"system\",\n                \"content\": f\"You are a text summarizer/modifier. Rewrite the input in a formal and {summary_type} style.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": user_prompt\n            }\n        ],\n        \"stream\": False\n    }\n    \n    response = requests.post(url, json=payload)\n    # return response.text\n    return response.json()['message']['content']\n\n# print(gemma3_response('adsv hi howss arse yo?', 'email'))\n\nfrom flask import Flask\nfrom flask import request\napp = Flask(__name__)\n\n@app.route('/')\ndef index():\n    return \"\"\"\n    &lt;form action='/' method='POST'&gt;\n        Text: &lt;textarea name='user_prompt'&gt;&lt;/textarea&gt; &lt;br&gt;\n        Style: &lt;input name='style' type='text'&gt; &lt;br&gt;\n        &lt;button type='submit'&gt;Ask&lt;/button&gt;\n    &lt;/form&gt;\n    \"\"\"\n\n@app.route('/', methods=['POST'])\ndef ask():\n    user_prompt = request.form.get('user_prompt')\n    style = request.form.get('style')\n    ai_answer = gemma3_response(user_prompt, style)\n    return f\"&lt;textarea&gt;{ai_answer}&lt;/textarea&gt;\"\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=5000)\ninstall_model.sh\n#!/bin/bash\n\necho \"üîÑ Installing gemma3:270m model during build...\"\n\n# Start Ollama in background using full path\n/usr/bin/ollama serve &\nOLLAMA_PID=$!\n\n# Wait longer for Alpine-based Ollama to start\nsleep 15\n\n# Check if Ollama is ready with timeout\nTIMEOUT=60\nELAPSED=0\nwhile ! curl -f http://localhost:11434/api/tags &gt; /dev/null 2&gt;&1; do\n    echo \"Waiting for Ollama to be ready... ($ELAPSED/$TIMEOUT seconds)\"\n    sleep 3\n    ELAPSED=$((ELAPSED + 3))\n    if [ $ELAPSED -ge $TIMEOUT ]; then\n        echo \"‚ùå Timeout waiting for Ollama to start!\"\n        kill $OLLAMA_PID 2&gt;/dev/null || true\n        exit 1\n    fi\ndone\n\necho \"‚úÖ Ollama is ready!\"\n\n# Pull the model using full path\necho \"üì• Pulling gemma3:270m model...\"\nif /usr/bin/ollama pull gemma3:270m; then\n    echo \"‚úÖ Model gemma3:270m downloaded successfully!\"\nelse\n    echo \"‚ùå Failed to download model!\"\n    kill $OLLAMA_PID 2&gt;/dev/null || true\n    exit 1\nfi\n\n# Verify model was installed\nif /usr/bin/ollama list | grep -q \"gemma3:270m\"; then\n    echo \"‚úÖ Model gemma3:270m verified in model list!\"\nelse\n    echo \"‚ùå Model not found in list!\"\n    kill $OLLAMA_PID 2&gt;/dev/null || true\n    exit 1\nfi\n\n# Stop Ollama gracefully\necho \"üõë Stopping Ollama service...\"\nkill $OLLAMA_PID\nwait $OLLAMA_PID 2&gt;/dev/null || true\n\necho \"‚úÖ Model installation complete!\"\nstart.sh\n#!/bin/bash\n\necho \"üöÄ Starting FormalAI services...\"\n\n# Start Ollama server in background using full path\necho \"üì° Starting Ollama server...\"\n/usr/bin/ollama serve &\n\n# Wait for Ollama to be ready\necho \"‚è≥ Waiting for Ollama to start...\"\nsleep 8\n\n# Health check for Ollama\nwhile ! curl -f http://localhost:11434/api/tags &gt; /dev/null 2&gt;&1; do\n    echo \"‚è≥ Waiting for Ollama API...\"\n    sleep 3\ndone\n\necho \"‚úÖ Ollama is ready!\"\necho \"‚úÖ Model gemma3:270m is pre-installed!\"\n\n# List available models for verification\necho \"üìã Available models:\"\n/usr/bin/ollama list\n\n# Start Flask application\necho \"üåê Starting Flask application on port 5000...\"\npython app.py\nrequirements.txt\nFlask\nrequests",
    "crumbs": [
      "Podman",
      "Project-4 **FormaliAI Dockerfile**"
    ]
  },
  {
    "objectID": "Podman/5_MiniProject4.html#miniproject-4-formali-ai-image-to-huggingface",
    "href": "Podman/5_MiniProject4.html#miniproject-4-formali-ai-image-to-huggingface",
    "title": "TDS by HRM",
    "section": "",
    "text": "Dockerfile\nFROM docker.io/alpine/ollama:0.12.0\n\n# Set working directory\nWORKDIR /app\n\n# Install Python and system dependencies\nRUN apk add --no-cache \\\n    python3 \\\n    py3-pip \\\n    curl \\\n    bash\n\n# Create symbolic link for python command\nRUN ln -sf /usr/bin/python3 /usr/bin/python\n\n# Copy requirements and install Python dependencies\nCOPY requirements.txt .\nRUN pip3 install --no-cache-dir -r requirements.txt\n\n# Copy model installation script\nCOPY install_model.sh .\nRUN chmod +x install_model.sh\n\n# Install model during build time\nRUN ./install_model.sh\n\n# Copy application files\nCOPY app.py .\nCOPY start.sh .\n\n# Make startup script executable\nRUN chmod +x start.sh\n\n# Expose only Flask port\nEXPOSE 5000\n\n# Override the original entrypoint completely\nENTRYPOINT [\"./start.sh\"]\napp.py\nimport requests\n\ndef gemma3_response(user_prompt, summary_type):\n    url = \"http://localhost:11434/api/chat\"\n    \n    payload = {\n        \"model\": \"gemma3:270m\",\n        \"messages\": [\n            {\n                \"role\": \"system\",\n                \"content\": f\"You are a text summarizer/modifier. Rewrite the input in a formal and {summary_type} style.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": user_prompt\n            }\n        ],\n        \"stream\": False\n    }\n    \n    response = requests.post(url, json=payload)\n    # return response.text\n    return response.json()['message']['content']\n\n# print(gemma3_response('adsv hi howss arse yo?', 'email'))\n\nfrom flask import Flask\nfrom flask import request\napp = Flask(__name__)\n\n@app.route('/')\ndef index():\n    return \"\"\"\n    &lt;form action='/' method='POST'&gt;\n        Text: &lt;textarea name='user_prompt'&gt;&lt;/textarea&gt; &lt;br&gt;\n        Style: &lt;input name='style' type='text'&gt; &lt;br&gt;\n        &lt;button type='submit'&gt;Ask&lt;/button&gt;\n    &lt;/form&gt;\n    \"\"\"\n\n@app.route('/', methods=['POST'])\ndef ask():\n    user_prompt = request.form.get('user_prompt')\n    style = request.form.get('style')\n    ai_answer = gemma3_response(user_prompt, style)\n    return f\"&lt;textarea&gt;{ai_answer}&lt;/textarea&gt;\"\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=5000)\ninstall_model.sh\n#!/bin/bash\n\necho \"üîÑ Installing gemma3:270m model during build...\"\n\n# Start Ollama in background using full path\n/usr/bin/ollama serve &\nOLLAMA_PID=$!\n\n# Wait longer for Alpine-based Ollama to start\nsleep 15\n\n# Check if Ollama is ready with timeout\nTIMEOUT=60\nELAPSED=0\nwhile ! curl -f http://localhost:11434/api/tags &gt; /dev/null 2&gt;&1; do\n    echo \"Waiting for Ollama to be ready... ($ELAPSED/$TIMEOUT seconds)\"\n    sleep 3\n    ELAPSED=$((ELAPSED + 3))\n    if [ $ELAPSED -ge $TIMEOUT ]; then\n        echo \"‚ùå Timeout waiting for Ollama to start!\"\n        kill $OLLAMA_PID 2&gt;/dev/null || true\n        exit 1\n    fi\ndone\n\necho \"‚úÖ Ollama is ready!\"\n\n# Pull the model using full path\necho \"üì• Pulling gemma3:270m model...\"\nif /usr/bin/ollama pull gemma3:270m; then\n    echo \"‚úÖ Model gemma3:270m downloaded successfully!\"\nelse\n    echo \"‚ùå Failed to download model!\"\n    kill $OLLAMA_PID 2&gt;/dev/null || true\n    exit 1\nfi\n\n# Verify model was installed\nif /usr/bin/ollama list | grep -q \"gemma3:270m\"; then\n    echo \"‚úÖ Model gemma3:270m verified in model list!\"\nelse\n    echo \"‚ùå Model not found in list!\"\n    kill $OLLAMA_PID 2&gt;/dev/null || true\n    exit 1\nfi\n\n# Stop Ollama gracefully\necho \"üõë Stopping Ollama service...\"\nkill $OLLAMA_PID\nwait $OLLAMA_PID 2&gt;/dev/null || true\n\necho \"‚úÖ Model installation complete!\"\nstart.sh\n#!/bin/bash\n\necho \"üöÄ Starting FormalAI services...\"\n\n# Start Ollama server in background using full path\necho \"üì° Starting Ollama server...\"\n/usr/bin/ollama serve &\n\n# Wait for Ollama to be ready\necho \"‚è≥ Waiting for Ollama to start...\"\nsleep 8\n\n# Health check for Ollama\nwhile ! curl -f http://localhost:11434/api/tags &gt; /dev/null 2&gt;&1; do\n    echo \"‚è≥ Waiting for Ollama API...\"\n    sleep 3\ndone\n\necho \"‚úÖ Ollama is ready!\"\necho \"‚úÖ Model gemma3:270m is pre-installed!\"\n\n# List available models for verification\necho \"üìã Available models:\"\n/usr/bin/ollama list\n\n# Start Flask application\necho \"üåê Starting Flask application on port 5000...\"\npython app.py\nrequirements.txt\nFlask\nrequests",
    "crumbs": [
      "Podman",
      "Project-4 **FormaliAI Dockerfile**"
    ]
  },
  {
    "objectID": "Podman/3_MiniProject2.html",
    "href": "Podman/3_MiniProject2.html",
    "title": "TDS by HRM",
    "section": "",
    "text": "Run Jupyter Notebook in Podman\nMount Directory/Folder volumne\n\n# Pull small jupyter_notebook imaege\npodman pull quay.io/jupyter/base-notebook\n\nmkdir jupyter_work \nchmod 777 jupyter_work\n\npodman run -d --name jupyter_server quay.io/jupyter/base-notebook\npodman ps\npodman logs container_id/name #  http://127.0.0.1:8888\npodman logs -f container_id\npodman rm  jupyter_server\n\n\npodman run -d -p 8888:8888 --name jupyter_server quay.io/jupyter/base-notebook # /home/jovyan/work\npodman stop jupyter_server && podman rm jupyter_server\n\n\npodman run -d -p 8888:8888 -v ~/jupyter_work:/home/jovyan/work --name jupyter_server quay.io/jupyter/base-notebook",
    "crumbs": [
      "Podman",
      "Project-2 **JupyLab**"
    ]
  },
  {
    "objectID": "Podman/3_MiniProject2.html#miniproject-2-jupylab-notebook-workspace-in-a-podman-container",
    "href": "Podman/3_MiniProject2.html#miniproject-2-jupylab-notebook-workspace-in-a-podman-container",
    "title": "TDS by HRM",
    "section": "",
    "text": "Run Jupyter Notebook in Podman\nMount Directory/Folder volumne\n\n# Pull small jupyter_notebook imaege\npodman pull quay.io/jupyter/base-notebook\n\nmkdir jupyter_work \nchmod 777 jupyter_work\n\npodman run -d --name jupyter_server quay.io/jupyter/base-notebook\npodman ps\npodman logs container_id/name #  http://127.0.0.1:8888\npodman logs -f container_id\npodman rm  jupyter_server\n\n\npodman run -d -p 8888:8888 --name jupyter_server quay.io/jupyter/base-notebook # /home/jovyan/work\npodman stop jupyter_server && podman rm jupyter_server\n\n\npodman run -d -p 8888:8888 -v ~/jupyter_work:/home/jovyan/work --name jupyter_server quay.io/jupyter/base-notebook",
    "crumbs": [
      "Podman",
      "Project-2 **JupyLab**"
    ]
  },
  {
    "objectID": "Podman/1_podman.html",
    "href": "Podman/1_podman.html",
    "title": "TDS by HRM",
    "section": "",
    "text": "Ubuntu: sudo apt-get -y install podman",
    "crumbs": [
      "Podman",
      "Image to Containers"
    ]
  },
  {
    "objectID": "Podman/1_podman.html#install-podman",
    "href": "Podman/1_podman.html#install-podman",
    "title": "TDS by HRM",
    "section": "",
    "text": "Ubuntu: sudo apt-get -y install podman",
    "crumbs": [
      "Podman",
      "Image to Containers"
    ]
  },
  {
    "objectID": "Podman/1_podman.html#setup-podman",
    "href": "Podman/1_podman.html#setup-podman",
    "title": "TDS by HRM",
    "section": "Setup Podman",
    "text": "Setup Podman\nRun below commands one by one\n\nsudo apt install qemu-system-x86\n\npodman machine init\n\npodman machine start sometimes not work\n\nhttps://stackoverflow.com/questions/72092497/podman-machine-start-on-ubuntu\n\nsudo mv /mnt/c/Users/hrith/Downloads/gvproxy-linux-amd64 /usr/libexec/podman\n\n\nwsl2 has some issue with kvm so you may need to run sudo chmod 666 /dev/kvm before running podman machine start\n\nWARN[0001] \"/\" is not a shared mount, this could cause issues or missing mounts with rootless containers to fix it sudo mount --make-rshared /",
    "crumbs": [
      "Podman",
      "Image to Containers"
    ]
  },
  {
    "objectID": "Podman/1_podman.html#download-images",
    "href": "Podman/1_podman.html#download-images",
    "title": "TDS by HRM",
    "section": "Download Images",
    "text": "Download Images\npodman pull docker.io/library/nginx:latest  # Download from dockerhub\npodman pull podman.io/library/alpine:latest # Download from podman\n\n# Check/list downloaded images\npodman images\n\n# Delete Downloaded\npodman rmi image_name_or_id",
    "crumbs": [
      "Podman",
      "Image to Containers"
    ]
  },
  {
    "objectID": "Podman/1_podman.html#run-image",
    "href": "Podman/1_podman.html#run-image",
    "title": "TDS by HRM",
    "section": "Run Image",
    "text": "Run Image\n\nstop & start are used for to turn on or off pods\n\nrun & rm are used to create or remove pods\n\n# 0. List the running/stopped containers\npodman ps -a\n\n# 1. Start a new container from an image (run interactively or detached)\npodman run nginx\npodman run -d --name container_name imagename:tag\n\n# 2. Stop / Shutdown a running container\npodman stop container_name_or_id\n\n# 3. Restart the stopped container\npodman start container_name_or_id\n\n# 4. Delete the stopped container\npodman rm container_name_or_id\n\n# 5. Delete all stopped containers\npodman container prune",
    "crumbs": [
      "Podman",
      "Image to Containers"
    ]
  },
  {
    "objectID": "Podman/1_podman.html#some-usefull-podman-commands",
    "href": "Podman/1_podman.html#some-usefull-podman-commands",
    "title": "TDS by HRM",
    "section": "Some usefull podman commands",
    "text": "Some usefull podman commands\n# Port expose\npodman run -p host_port:container_port/protocol image_name # -p = --publish\npodman run -p 8080-8085:80-85/tcp image_name    # range of ports\npodman port container_name_or_id    # Check port mapping of running containers\n\n# Volumes\npodman volume create xyz\npodman volume ls\npodman volume inspect mydata\npodman volume rm xyz\n\npodman run -v volume_name_or_hostpath:/container/path image_name # mount named volumes to containers\n\n# Network\npodman network create mynet\npodman network ls\npodman network inspect mynet\npodman network rm mynet",
    "crumbs": [
      "Podman",
      "Image to Containers"
    ]
  },
  {
    "objectID": "Podman/1_podman.html#dockerfile-and-deployemnt-to-huggingface",
    "href": "Podman/1_podman.html#dockerfile-and-deployemnt-to-huggingface",
    "title": "TDS by HRM",
    "section": "Dockerfile and Deployemnt to HuggingFace",
    "text": "Dockerfile and Deployemnt to HuggingFace",
    "crumbs": [
      "Podman",
      "Image to Containers"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "TDS - Tools for Data Science",
    "section": "",
    "text": "22f3002460@ds.study.iitm.ac.in",
    "crumbs": [
      "About The Course"
    ]
  },
  {
    "objectID": "Podman/2_MiniProject1.html",
    "href": "Podman/2_MiniProject1.html",
    "title": "TDS by HRM",
    "section": "",
    "text": "Ollama in podman\nOther work in normal vs code\nVolumne in Podman\n\n# Pull small ollama imaege\npodman pull docker.io/alpine/ollama:0.12.0\n\npodman volume create ollama \npodman run -d -p 11434:11434 -v ollama:/root/.ollama --name ollama docker.io/alpine/ollama:0.12.0\n\n# Check\n# http://localhost:11434/\n\n# install model and test it\npodman exec -it ollama sh # To exit we can run: exit\nollama pull gemma3:270m\nollama run gemma3:270m\n...\n...\n/exit\n\nexit\nmain.py -&gt; A Simple LLM talk\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.ollama import OllamaProvider\n\n\nollama_model = OpenAIChatModel(\n    model_name='gemma3:270m',\n    provider=OllamaProvider(base_url='http://localhost:11434/v1'),\n)\nagent = Agent(ollama_model)\n\nimport uvicorn\nfrom fastapi import FastAPI, Query\napp = FastAPI()\n\n@app.get('/')\ndef index(ask: str = Query(..., description=\"User prompt\")):\n    result = agent.run_sync(ask)\n    return {'ai_answer': result.output}\n\nif __name__ == \"__main__\":\n    # Run the server when script is executed directly\n    uvicorn.run(\n        app, \n        host=\"0.0.0.0\", \n        port=8000\n    )",
    "crumbs": [
      "Podman",
      "Project-1 **OllamaTalkPod**"
    ]
  },
  {
    "objectID": "Podman/2_MiniProject1.html#miniproject-1-ollamatalk-gemma-model-running-in-a-podman-container",
    "href": "Podman/2_MiniProject1.html#miniproject-1-ollamatalk-gemma-model-running-in-a-podman-container",
    "title": "TDS by HRM",
    "section": "",
    "text": "Ollama in podman\nOther work in normal vs code\nVolumne in Podman\n\n# Pull small ollama imaege\npodman pull docker.io/alpine/ollama:0.12.0\n\npodman volume create ollama \npodman run -d -p 11434:11434 -v ollama:/root/.ollama --name ollama docker.io/alpine/ollama:0.12.0\n\n# Check\n# http://localhost:11434/\n\n# install model and test it\npodman exec -it ollama sh # To exit we can run: exit\nollama pull gemma3:270m\nollama run gemma3:270m\n...\n...\n/exit\n\nexit\nmain.py -&gt; A Simple LLM talk\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.ollama import OllamaProvider\n\n\nollama_model = OpenAIChatModel(\n    model_name='gemma3:270m',\n    provider=OllamaProvider(base_url='http://localhost:11434/v1'),\n)\nagent = Agent(ollama_model)\n\nimport uvicorn\nfrom fastapi import FastAPI, Query\napp = FastAPI()\n\n@app.get('/')\ndef index(ask: str = Query(..., description=\"User prompt\")):\n    result = agent.run_sync(ask)\n    return {'ai_answer': result.output}\n\nif __name__ == \"__main__\":\n    # Run the server when script is executed directly\n    uvicorn.run(\n        app, \n        host=\"0.0.0.0\", \n        port=8000\n    )",
    "crumbs": [
      "Podman",
      "Project-1 **OllamaTalkPod**"
    ]
  },
  {
    "objectID": "Podman/4_MiniProject3.html",
    "href": "Podman/4_MiniProject3.html",
    "title": "TDS by HRM",
    "section": "",
    "text": "ollama_python.py\nimport requests\n\ndef gemma3_response(user_prompt, summary_type):\n    url = \"http://ollama:11434/api/chat\"\n    \n    payload = {\n        \"model\": \"gemma3:270m\",\n        \"messages\": [\n            {\n                \"role\": \"system\",\n                \"content\": f\"You are a text summarizer/modifier. Rewrite the input in a formal and {summary_type} style.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": user_prompt\n            }\n        ],\n        \"stream\": False\n    }\n    \n    response = requests.post(url, json=payload)\n    # return response.text\n    return response.json()['message']['content']\n\n# print(gemma3_response('adsv hi howss arse yo?', 'email'))\n\nfrom flask import Flask\nfrom flask import request\napp = Flask(__name__)\n\n@app.route('/')\ndef index():\n    return \"\"\"\n    &lt;form action='/' method='POST'&gt;\n        Text: &lt;textarea name='user_prompt'&gt;&lt;/textarea&gt; &lt;br&gt;\n        Style: &lt;input name='style' type='text'&gt; &lt;br&gt;\n        &lt;button type='submit'&gt;Ask&lt;/button&gt;\n    &lt;/form&gt;\n    \"\"\"\n\n@app.route('/', methods=['POST'])\ndef ask():\n    user_prompt = request.form.get('user_prompt')\n    style = request.form.get('style')\n    ai_answer = gemma3_response(user_prompt, style)\n    return f\"&lt;textarea&gt;{ai_answer}&lt;/textarea&gt;\"\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=5000)\npodman network create mynet\n\n# Run Ollama container\npodman run -d --name ollama \\\n  --network mynet \\\n  -p 11434:11434 \\\n  -v ollama:/root/.ollama \\\n  docker.io/alpine/ollama \n\n# Run Jupyter container\npodman run -d --name jupyter_server \\\n  --network mynet \\\n  -p 8888:8888 \\\n  -p 5000:5000 \\\n  -v ~/jupyter_work:/home/jovyan/work \\\n  quay.io/jupyter/base-notebook\n\n# insited of localhost user container name\nurl = \"http://ollama:11434/api/chat\"",
    "crumbs": [
      "Podman",
      "Project-3 **FormaliAI**"
    ]
  },
  {
    "objectID": "Podman/4_MiniProject3.html#miniproject-3-formali-ai-emphasizes-formal-or-official-writing-modification.",
    "href": "Podman/4_MiniProject3.html#miniproject-3-formali-ai-emphasizes-formal-or-official-writing-modification.",
    "title": "TDS by HRM",
    "section": "",
    "text": "ollama_python.py\nimport requests\n\ndef gemma3_response(user_prompt, summary_type):\n    url = \"http://ollama:11434/api/chat\"\n    \n    payload = {\n        \"model\": \"gemma3:270m\",\n        \"messages\": [\n            {\n                \"role\": \"system\",\n                \"content\": f\"You are a text summarizer/modifier. Rewrite the input in a formal and {summary_type} style.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": user_prompt\n            }\n        ],\n        \"stream\": False\n    }\n    \n    response = requests.post(url, json=payload)\n    # return response.text\n    return response.json()['message']['content']\n\n# print(gemma3_response('adsv hi howss arse yo?', 'email'))\n\nfrom flask import Flask\nfrom flask import request\napp = Flask(__name__)\n\n@app.route('/')\ndef index():\n    return \"\"\"\n    &lt;form action='/' method='POST'&gt;\n        Text: &lt;textarea name='user_prompt'&gt;&lt;/textarea&gt; &lt;br&gt;\n        Style: &lt;input name='style' type='text'&gt; &lt;br&gt;\n        &lt;button type='submit'&gt;Ask&lt;/button&gt;\n    &lt;/form&gt;\n    \"\"\"\n\n@app.route('/', methods=['POST'])\ndef ask():\n    user_prompt = request.form.get('user_prompt')\n    style = request.form.get('style')\n    ai_answer = gemma3_response(user_prompt, style)\n    return f\"&lt;textarea&gt;{ai_answer}&lt;/textarea&gt;\"\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=5000)\npodman network create mynet\n\n# Run Ollama container\npodman run -d --name ollama \\\n  --network mynet \\\n  -p 11434:11434 \\\n  -v ollama:/root/.ollama \\\n  docker.io/alpine/ollama \n\n# Run Jupyter container\npodman run -d --name jupyter_server \\\n  --network mynet \\\n  -p 8888:8888 \\\n  -p 5000:5000 \\\n  -v ~/jupyter_work:/home/jovyan/work \\\n  quay.io/jupyter/base-notebook\n\n# insited of localhost user container name\nurl = \"http://ollama:11434/api/chat\"",
    "crumbs": [
      "Podman",
      "Project-3 **FormaliAI**"
    ]
  },
  {
    "objectID": "PydanticAi/1_basic.html",
    "href": "PydanticAi/1_basic.html",
    "title": "Chatbot",
    "section": "",
    "text": "Llm output is generally unstructred. Getting structured output is often very lenghty process.\nPydantic AI is useful because it bridges the gap between messy AI outputs and structured, safe, typed data‚Äîmaking AI-powered applications easier to build, debug, and trust.\n\n\n\nRequires Python 3.10+\n- uv: uv add pydantic-ai\n- pip: pip install pydantic-ai\n\n\n\n\nSystem Message: Set the context, rules or persona for the conversation.\nUser Message: What user ask to ai.\nAssistance/Model Message: What ai respond back to user.\n\n\n\n\n\nThe raw model (like GPT, Claude, Gemini etc.) is stateless.\nIt does not remember previous chats on its own.\nHence we must send past messages again in every request.\n\n\n\n\nMethod-1: in .bashrc\n.bashrc\nexport OPENAI_API_KEY=\"your_api_key\"\nexport GOOGLE_API_KEY=\"your_api_key\"\nexport ANTHROPIC_API_KEY=\"your_api_key\"\n\n\nexport OPENAI_BASE_URL=\"https://api.openai.com/v1\"\n# export OPENAI_BASE_URL=https://aipipe.org/openai/v1 # if aipipe token used\nexport GOOGLE_API_BASE_URL=\"https://generativelanguage.googleapis.com/v1beta\"\nexport ANTHROPIC_BASE_URL=\"https://api.anthropic.com\"\nMethod-2: in main python file We avoide this method\nimport os\nos.environ[\"GOOGLE_API_KEY\"] = \"paste your api key here\"\nMethod-3: use .env file We always try to use this method\n.env\nGOOGLE_API_KEY=yourapikeyhere\npythonfile\n...\nfrom dotenv import load_dotenv\nload_dotenv()\n...\n\n\n\n.env\n# Google Api key from --&gt; https://aistudio.google.com/apikey\nGOOGLE_API_KEY=your-api-key\nmain.py\nfrom pydantic_ai import Agent\n\nfrom dotenv import load_dotenv\nload_dotenv() # make sure in .env =&gt; GOOGLE_API_KEY=your-api-key\n\nagent = Agent(\n    model=\"gemini-2.5-flash\",\n    system_prompt=\"You answer within one small sentence with a small reason.\"\n)\n\nuser_prompt1 = \"Note, My name is Hritik.\"\nuser_prompt2 = \"What is my name?\"\n\nresult1 = agent.run_sync(user_prompt1) # return type is RunResult\nresult2 = agent.run_sync(user_prompt2)\n\nprint(result1.output)\nprint(result2.output)\nOutput\n‚ùØ uv run main.py\nNoted, Hritik, for future reference.\nI don't know your name because I don't retain information from past conversations.\n\n\n\nall_messages() and all_messages_json()\nnew_messages() and new_messages_json()\n\n[ ModelRequest(...), ModelResponse(...), ...]\n\nModelRequest( parts = [SystemPromptpart, UserPromptPart] )\nModelReponse( parts = [TextPart] )\n\n\n\n\nAgent.run(): asynchornous & returns RunResult where response is wrapped.\nAgent.run_sync(): synchornous & returns RunResult where response is wrapped.\n\n\n\n\n\nfrom pydantic_ai import Agent\n\nfrom dotenv import load_dotenv\nload_dotenv() # make sure in .env =&gt; GOOGLE_API_KEY=your-api-key\n\nagent = Agent(\n    model=\"gemini-2.5-flash\",\n    system_prompt=\"You answer within one small sentence with a small reason.\"\n)\n\nuser_prompt1 = \"Note, My name is Hritik\"\nuser_prompt2 = \"What did I say my name was?\"\nuser_prompt3 = \"List down all the things that i asked/told you?\"\n\nresult1 = agent.run_sync(user_prompt1)\nresult2 = agent.run_sync(user_prompt2, message_history=result1.new_messages())\nresult3 = agent.run_sync(user_prompt3, message_history=result2.new_messages())\n\n\nprint(result1.output)\nprint(result2.output)\nprint(result3.output)\n\n# print(result1.all_messages()) # [ModelRequest(parts=[systemprompt, userprompt]), ModelResponse(parts=[Textpart]) ]\n# print(result2.all_messages()) # [ModelRequest(parts=[userprompt]), ModelResponse(parts=[Textpart]), ... ]\n# print(result3.all_messages())\n\n# print(result3.new_messages()) # [ModelRequest(parts=[userprompt]), ModelResponse(parts=[Textpart]) ]\nOutput\nOkay, Hritik, I've noted your name to personalize our conversation.\nYour name is Hritik, as you just mentioned it to me.\nBased on our current conversation, here are the things you have asked/told me:\n\n1.  \"What did I say my name was?\"\n2.  \"List down all the things that i asked/told you?\"\n...",
    "crumbs": [
      "Projects",
      "Intro To PydanticAI"
    ]
  },
  {
    "objectID": "PydanticAi/1_basic.html#why-pydantic-ai",
    "href": "PydanticAi/1_basic.html#why-pydantic-ai",
    "title": "Chatbot",
    "section": "",
    "text": "Llm output is generally unstructred. Getting structured output is often very lenghty process.\nPydantic AI is useful because it bridges the gap between messy AI outputs and structured, safe, typed data‚Äîmaking AI-powered applications easier to build, debug, and trust.",
    "crumbs": [
      "Projects",
      "Intro To PydanticAI"
    ]
  },
  {
    "objectID": "PydanticAi/1_basic.html#installation",
    "href": "PydanticAi/1_basic.html#installation",
    "title": "Chatbot",
    "section": "",
    "text": "Requires Python 3.10+\n- uv: uv add pydantic-ai\n- pip: pip install pydantic-ai",
    "crumbs": [
      "Projects",
      "Intro To PydanticAI"
    ]
  },
  {
    "objectID": "PydanticAi/1_basic.html#messages-types-in-chatbots",
    "href": "PydanticAi/1_basic.html#messages-types-in-chatbots",
    "title": "Chatbot",
    "section": "",
    "text": "System Message: Set the context, rules or persona for the conversation.\nUser Message: What user ask to ai.\nAssistance/Model Message: What ai respond back to user.",
    "crumbs": [
      "Projects",
      "Intro To PydanticAI"
    ]
  },
  {
    "objectID": "PydanticAi/1_basic.html#llms-are-stateless",
    "href": "PydanticAi/1_basic.html#llms-are-stateless",
    "title": "Chatbot",
    "section": "",
    "text": "The raw model (like GPT, Claude, Gemini etc.) is stateless.\nIt does not remember previous chats on its own.\nHence we must send past messages again in every request.",
    "crumbs": [
      "Projects",
      "Intro To PydanticAI"
    ]
  },
  {
    "objectID": "PydanticAi/1_basic.html#api-key-setup-in-pydantic-ai",
    "href": "PydanticAi/1_basic.html#api-key-setup-in-pydantic-ai",
    "title": "Chatbot",
    "section": "",
    "text": "Method-1: in .bashrc\n.bashrc\nexport OPENAI_API_KEY=\"your_api_key\"\nexport GOOGLE_API_KEY=\"your_api_key\"\nexport ANTHROPIC_API_KEY=\"your_api_key\"\n\n\nexport OPENAI_BASE_URL=\"https://api.openai.com/v1\"\n# export OPENAI_BASE_URL=https://aipipe.org/openai/v1 # if aipipe token used\nexport GOOGLE_API_BASE_URL=\"https://generativelanguage.googleapis.com/v1beta\"\nexport ANTHROPIC_BASE_URL=\"https://api.anthropic.com\"\nMethod-2: in main python file We avoide this method\nimport os\nos.environ[\"GOOGLE_API_KEY\"] = \"paste your api key here\"\nMethod-3: use .env file We always try to use this method\n.env\nGOOGLE_API_KEY=yourapikeyhere\npythonfile\n...\nfrom dotenv import load_dotenv\nload_dotenv()\n...",
    "crumbs": [
      "Projects",
      "Intro To PydanticAI"
    ]
  },
  {
    "objectID": "PydanticAi/1_basic.html#example1-just-ask-to-ai",
    "href": "PydanticAi/1_basic.html#example1-just-ask-to-ai",
    "title": "Chatbot",
    "section": "",
    "text": ".env\n# Google Api key from --&gt; https://aistudio.google.com/apikey\nGOOGLE_API_KEY=your-api-key\nmain.py\nfrom pydantic_ai import Agent\n\nfrom dotenv import load_dotenv\nload_dotenv() # make sure in .env =&gt; GOOGLE_API_KEY=your-api-key\n\nagent = Agent(\n    model=\"gemini-2.5-flash\",\n    system_prompt=\"You answer within one small sentence with a small reason.\"\n)\n\nuser_prompt1 = \"Note, My name is Hritik.\"\nuser_prompt2 = \"What is my name?\"\n\nresult1 = agent.run_sync(user_prompt1) # return type is RunResult\nresult2 = agent.run_sync(user_prompt2)\n\nprint(result1.output)\nprint(result2.output)\nOutput\n‚ùØ uv run main.py\nNoted, Hritik, for future reference.\nI don't know your name because I don't retain information from past conversations.\n\n\n\nall_messages() and all_messages_json()\nnew_messages() and new_messages_json()\n\n[ ModelRequest(...), ModelResponse(...), ...]\n\nModelRequest( parts = [SystemPromptpart, UserPromptPart] )\nModelReponse( parts = [TextPart] )\n\n\n\n\nAgent.run(): asynchornous & returns RunResult where response is wrapped.\nAgent.run_sync(): synchornous & returns RunResult where response is wrapped.",
    "crumbs": [
      "Projects",
      "Intro To PydanticAI"
    ]
  },
  {
    "objectID": "PydanticAi/1_basic.html#example2-just-ask-to-ai-with-history",
    "href": "PydanticAi/1_basic.html#example2-just-ask-to-ai-with-history",
    "title": "Chatbot",
    "section": "",
    "text": "from pydantic_ai import Agent\n\nfrom dotenv import load_dotenv\nload_dotenv() # make sure in .env =&gt; GOOGLE_API_KEY=your-api-key\n\nagent = Agent(\n    model=\"gemini-2.5-flash\",\n    system_prompt=\"You answer within one small sentence with a small reason.\"\n)\n\nuser_prompt1 = \"Note, My name is Hritik\"\nuser_prompt2 = \"What did I say my name was?\"\nuser_prompt3 = \"List down all the things that i asked/told you?\"\n\nresult1 = agent.run_sync(user_prompt1)\nresult2 = agent.run_sync(user_prompt2, message_history=result1.new_messages())\nresult3 = agent.run_sync(user_prompt3, message_history=result2.new_messages())\n\n\nprint(result1.output)\nprint(result2.output)\nprint(result3.output)\n\n# print(result1.all_messages()) # [ModelRequest(parts=[systemprompt, userprompt]), ModelResponse(parts=[Textpart]) ]\n# print(result2.all_messages()) # [ModelRequest(parts=[userprompt]), ModelResponse(parts=[Textpart]), ... ]\n# print(result3.all_messages())\n\n# print(result3.new_messages()) # [ModelRequest(parts=[userprompt]), ModelResponse(parts=[Textpart]) ]\nOutput\nOkay, Hritik, I've noted your name to personalize our conversation.\nYour name is Hritik, as you just mentioned it to me.\nBased on our current conversation, here are the things you have asked/told me:\n\n1.  \"What did I say my name was?\"\n2.  \"List down all the things that i asked/told you?\"\n...",
    "crumbs": [
      "Projects",
      "Intro To PydanticAI"
    ]
  },
  {
    "objectID": "PydanticAi/3_output.html",
    "href": "PydanticAi/3_output.html",
    "title": "Pydantic AI - Output Handling Guide",
    "section": "",
    "text": "The Output in Pydantic AI refers to the final value returned from running an agent. This can be plain text, structured data, or the result of a function called with arguments provided by the model.\nagent ==&gt; AgentRunResult or StreamedRunResult\n\n\n\nThe Agent class constructor accepts an output_type argument that supports:\n\nSimple scalar types: str, int, float, bool\nPydantic models: BaseModel subclasses\nDataclasses: Python dataclasses\nTypedDict: Typed dictionaries\nFunctions: Output functions that process model arguments\nLists/Unions: Multiple output types (e.g., [Fruit, Vehicle])\n\nAgent class\n    -&gt; output_type argument -&gt; Pydantic models, dataclasses, typedict, function, list etc.\n        -&gt; agent = Agent('model-id', output_type=str)\n        -&gt; agent = Agent('model-id', output_type=split_into_words)\n        -&gt; agent = Agent('model-id', output_type=[Fruit, Vehicle]) # either fruit or vehicle\n\n\nfrom dotenv import load_dotenv\nload_dotenv(dotenv_path='.env')\n\nfrom pydantic_ai import Agent\n\nagent1 = Agent('google-gla:gemini-2.5-flash', output_type=str) # default value is string\nagent2 = Agent('google-gla:gemini-2.5-flash', output_type=int)\n\nresult1 = agent1.run_sync('hi how can i solve 54*10')\nresult2 = agent2.run_sync('hi how can i solve 54*10')\n\nprint(\"===Result1.output===\")\nprint(result1.output)\nprint(\"===Result2.output===\")\nprint(result2.output)\n===Result1.output===\nSolving $54 \\times 10$ is quite straightforward!\n\nHere's how you do it:\n\n1.  **The Rule for Multiplying by 10:** When you multiply a whole number by 10, you simply add one zero to the end of that number.        \n\n2.  **Apply the Rule:**\n    *   Your original number is 54.\n    *   Add a zero to the end of 54.\n    *   You get 540.\n\nSo, $54 \\times 10 = 540$.\n===Result2.output===\n540\n\n\n\nfrom pydantic import BaseModel\nfrom pydantic_ai import Agent\n\nclass CityLocation(BaseModel):\n    city: str\n    country: str\n\nagent = Agent('google-gla:gemini-1.5-flash', output_type=CityLocation)\nresult = agent.run_sync('Where were the olympics held in 2012?')\nprint(result.output)\n# Output: city='London' country='United Kingdom'\n\n\n\nfrom dotenv import load_dotenv\nload_dotenv(dotenv_path='.env')\nfrom pydantic import BaseModel\nfrom pydantic_ai import Agent\n\nclass Box(BaseModel):\n    width: int\n    height: int\n    depth: int\n    units: str\n\nagent = Agent(\n    'openai:gpt-5-nano',\n    output_type=[Box, str],  # Either Box or str\n    system_prompt=(\n        \"Extract box dimensions, \"\n        \"if you can't extract all data, ask the user to try again.\"\n    ),\n)\n\nresult = agent.run_sync('The box is 10x20x30')\nprint(result.output, type(result.output))\n# Output: \"Please provide the units...\", &lt;class 'str'&gt;\n\nresult = agent.run_sync('The box is 10x20x30 cm')\nprint(result.output, type(result.output))\n# Output: Box(width=10, height=20, depth=30, units='cm'), &lt;class '__main__.Box'&gt;\n\n\n\n\nOutput functions allow the model to call a function with arguments, and the function‚Äôs return value becomes the agent‚Äôs output.\n\n\nfrom dotenv import load_dotenv\nload_dotenv(dotenv_path='.env')\n\nfrom pydantic_ai import Agent\n\ndef table_of(num: int) -&gt; str:\n    print(\"Number choose is:\", num, type(num))\n    r = ''\n    for i in range(1,10):\n        r += str(num*i) + ' '\n    return r\n\nagent = Agent('google-gla:gemini-2.5-flash', output_type=table_of)\n\nresult = agent.run_sync('hi how can i solve 5*3')\n\nprint(result.output)\n# Number choose is: 15 &lt;class 'int'&gt;\n# 15 30 45 60 75 90 105 120 135 \nfrom dotenv import load_dotenv\nload_dotenv(dotenv_path='.env')\n\nfrom pydantic_ai import Agent\n\ndef table_of(num: int) -&gt; str:\n    \"\"\"Despite user input, model/llm will choose number 2\"\"\"\n    print(\"Number choose is:\", num, type(num))\n    r = ''\n    for i in range(1,10):\n        r += str(num*i) + ' '\n    return r\n\nagent = Agent('google-gla:gemini-2.5-flash', output_type=table_of)\n\nresult = agent.run_sync('hi how can i solve 5*3')\n\nprint(result.output)\n# Number choose is: 2 &lt;class 'int'&gt;\n# 2 4 6 8 10 12 14 16 18 \nLearning\n&gt; Docstring are passed and interpereted by llm before responding.\n\n\n\nfrom dotenv import load_dotenv\nload_dotenv(dotenv_path='.env')\n\nfrom pydantic import BaseModel, Field\nfrom pydantic_ai import Agent\n\nclass Fruit(BaseModel):\n    name: str = Field(..., min_length=3, max_length=4)\n    country: str = Field(\n        description='Country short code (reversed) e.g., USA -&gt; ASU'\n    )\n\ndef format_fruit(f: Fruit) -&gt; str:\n    return f\"{f.name} is from {f.country}\"\n\nagent = Agent('openai:gpt-5-nano', output_type=format_fruit)\nresult = agent.run_sync('The fruit Salak is commonly known as?')\nprint(result) # AgentRunResult(output='SNA is from NDI')\n\n\n\n\nArguments are validated using Pydantic\nCan optionally take RunContext as first argument\nCan raise ModelRetry to ask the model to try again\nThe result is not passed back to the model\n\n\n\n\n\nAdd validation logic that requires IO or is asynchronous using the @agent.output_validator decorator.\nfrom pydantic import BaseModel\nfrom pydantic_ai import Agent, RunContext, ModelRetry\n\nclass Success(BaseModel):\n    sql_query: str\n\nclass InvalidRequest(BaseModel):\n    error_message: str\n\nagent = Agent('google-gla:gemini-1.5-flash', output_type=Success | InvalidRequest)\n\n@agent.output_validator\nasync def validate_sql(ctx: RunContext, output: Success | InvalidRequest):\n    if isinstance(output, InvalidRequest):\n        return output\n    \n    try:\n        await ctx.deps.execute(f'EXPLAIN {output.sql_query}')\n    except QueryError as e:\n        raise ModelRetry(f'Invalid query: {e}') from e\n    else:\n        return output\n\n\n\nfrom dotenv import load_dotenv\nload_dotenv(dotenv_path='.env')\n\nfrom pydantic import BaseModel\nfrom pydantic_ai import Agent, RunContext, ModelRetry\n\nclass ValidAge(BaseModel):\n    age: int\n    name: str\n\nclass InvalidAge(BaseModel):\n    error: str\n\n# Agent with union output type\nagent = Agent(\n    'openai:gpt-5-nano',\n    output_type=ValidAge | InvalidAge, \n    system_prompt='Model Will always Extract person name and age from text, ValidAge is 0 to 200',\n    retries=3\n)\n\n@agent.output_validator\nasync def validate_age(ctx: RunContext[None], output: ValidAge | InvalidAge):\n    # If already an error, return it\n    if isinstance(output, InvalidAge):\n        return output\n    \n    # Check if age is reasonable (between 0 and 120)\n    if (output.age &lt; 0 or output.age &gt; 100) and ctx.retry&lt;2:\n        # Ask model to try again\n        print(f\"retrying{ctx.retry} {output.age}\")\n        raise ModelRetry(\n            f'What if my age is {output.age-1}'\n        )\n    if ctx.retry == 2:\n        raise ModelRetry(\n            f'What if my age is {output.age+100}'\n        )  \n    \n    return output\n\n# Usage\nresult = agent.run_sync('My name is John and I am 25 years old')\nprint(result.output)\nprint()\n# Output: ValidAge(age=25, name='John')\n\nresult2 = agent.run_sync('My name is Alice and I am 105 years old')\nprint(result2.output, type(result2))\nOutput\nage=25 name='John'\n\nretrying0 105\nretrying1 104\nerror='Invalid age: 203. Age must be between 0 and 200.' &lt;class 'pydantic_ai.run.AgentRunResult'&gt;\n\n\n\n\nDefault output type is str\nOutput is wrapped in AgentRunResult or StreamedRunResult\nAccess usage data via result.usage()\nOutput functions can raise ModelRetry for validation errors",
    "crumbs": [
      "Projects",
      "Structered Output & Retry"
    ]
  },
  {
    "objectID": "PydanticAi/3_output.html#overview",
    "href": "PydanticAi/3_output.html#overview",
    "title": "Pydantic AI - Output Handling Guide",
    "section": "",
    "text": "The Output in Pydantic AI refers to the final value returned from running an agent. This can be plain text, structured data, or the result of a function called with arguments provided by the model.\nagent ==&gt; AgentRunResult or StreamedRunResult",
    "crumbs": [
      "Projects",
      "Structered Output & Retry"
    ]
  },
  {
    "objectID": "PydanticAi/3_output.html#output-types",
    "href": "PydanticAi/3_output.html#output-types",
    "title": "Pydantic AI - Output Handling Guide",
    "section": "",
    "text": "The Agent class constructor accepts an output_type argument that supports:\n\nSimple scalar types: str, int, float, bool\nPydantic models: BaseModel subclasses\nDataclasses: Python dataclasses\nTypedDict: Typed dictionaries\nFunctions: Output functions that process model arguments\nLists/Unions: Multiple output types (e.g., [Fruit, Vehicle])\n\nAgent class\n    -&gt; output_type argument -&gt; Pydantic models, dataclasses, typedict, function, list etc.\n        -&gt; agent = Agent('model-id', output_type=str)\n        -&gt; agent = Agent('model-id', output_type=split_into_words)\n        -&gt; agent = Agent('model-id', output_type=[Fruit, Vehicle]) # either fruit or vehicle\n\n\nfrom dotenv import load_dotenv\nload_dotenv(dotenv_path='.env')\n\nfrom pydantic_ai import Agent\n\nagent1 = Agent('google-gla:gemini-2.5-flash', output_type=str) # default value is string\nagent2 = Agent('google-gla:gemini-2.5-flash', output_type=int)\n\nresult1 = agent1.run_sync('hi how can i solve 54*10')\nresult2 = agent2.run_sync('hi how can i solve 54*10')\n\nprint(\"===Result1.output===\")\nprint(result1.output)\nprint(\"===Result2.output===\")\nprint(result2.output)\n===Result1.output===\nSolving $54 \\times 10$ is quite straightforward!\n\nHere's how you do it:\n\n1.  **The Rule for Multiplying by 10:** When you multiply a whole number by 10, you simply add one zero to the end of that number.        \n\n2.  **Apply the Rule:**\n    *   Your original number is 54.\n    *   Add a zero to the end of 54.\n    *   You get 540.\n\nSo, $54 \\times 10 = 540$.\n===Result2.output===\n540\n\n\n\nfrom pydantic import BaseModel\nfrom pydantic_ai import Agent\n\nclass CityLocation(BaseModel):\n    city: str\n    country: str\n\nagent = Agent('google-gla:gemini-1.5-flash', output_type=CityLocation)\nresult = agent.run_sync('Where were the olympics held in 2012?')\nprint(result.output)\n# Output: city='London' country='United Kingdom'\n\n\n\nfrom dotenv import load_dotenv\nload_dotenv(dotenv_path='.env')\nfrom pydantic import BaseModel\nfrom pydantic_ai import Agent\n\nclass Box(BaseModel):\n    width: int\n    height: int\n    depth: int\n    units: str\n\nagent = Agent(\n    'openai:gpt-5-nano',\n    output_type=[Box, str],  # Either Box or str\n    system_prompt=(\n        \"Extract box dimensions, \"\n        \"if you can't extract all data, ask the user to try again.\"\n    ),\n)\n\nresult = agent.run_sync('The box is 10x20x30')\nprint(result.output, type(result.output))\n# Output: \"Please provide the units...\", &lt;class 'str'&gt;\n\nresult = agent.run_sync('The box is 10x20x30 cm')\nprint(result.output, type(result.output))\n# Output: Box(width=10, height=20, depth=30, units='cm'), &lt;class '__main__.Box'&gt;",
    "crumbs": [
      "Projects",
      "Structered Output & Retry"
    ]
  },
  {
    "objectID": "PydanticAi/3_output.html#output-functions",
    "href": "PydanticAi/3_output.html#output-functions",
    "title": "Pydantic AI - Output Handling Guide",
    "section": "",
    "text": "Output functions allow the model to call a function with arguments, and the function‚Äôs return value becomes the agent‚Äôs output.\n\n\nfrom dotenv import load_dotenv\nload_dotenv(dotenv_path='.env')\n\nfrom pydantic_ai import Agent\n\ndef table_of(num: int) -&gt; str:\n    print(\"Number choose is:\", num, type(num))\n    r = ''\n    for i in range(1,10):\n        r += str(num*i) + ' '\n    return r\n\nagent = Agent('google-gla:gemini-2.5-flash', output_type=table_of)\n\nresult = agent.run_sync('hi how can i solve 5*3')\n\nprint(result.output)\n# Number choose is: 15 &lt;class 'int'&gt;\n# 15 30 45 60 75 90 105 120 135 \nfrom dotenv import load_dotenv\nload_dotenv(dotenv_path='.env')\n\nfrom pydantic_ai import Agent\n\ndef table_of(num: int) -&gt; str:\n    \"\"\"Despite user input, model/llm will choose number 2\"\"\"\n    print(\"Number choose is:\", num, type(num))\n    r = ''\n    for i in range(1,10):\n        r += str(num*i) + ' '\n    return r\n\nagent = Agent('google-gla:gemini-2.5-flash', output_type=table_of)\n\nresult = agent.run_sync('hi how can i solve 5*3')\n\nprint(result.output)\n# Number choose is: 2 &lt;class 'int'&gt;\n# 2 4 6 8 10 12 14 16 18 \nLearning\n&gt; Docstring are passed and interpereted by llm before responding.\n\n\n\nfrom dotenv import load_dotenv\nload_dotenv(dotenv_path='.env')\n\nfrom pydantic import BaseModel, Field\nfrom pydantic_ai import Agent\n\nclass Fruit(BaseModel):\n    name: str = Field(..., min_length=3, max_length=4)\n    country: str = Field(\n        description='Country short code (reversed) e.g., USA -&gt; ASU'\n    )\n\ndef format_fruit(f: Fruit) -&gt; str:\n    return f\"{f.name} is from {f.country}\"\n\nagent = Agent('openai:gpt-5-nano', output_type=format_fruit)\nresult = agent.run_sync('The fruit Salak is commonly known as?')\nprint(result) # AgentRunResult(output='SNA is from NDI')\n\n\n\n\nArguments are validated using Pydantic\nCan optionally take RunContext as first argument\nCan raise ModelRetry to ask the model to try again\nThe result is not passed back to the model",
    "crumbs": [
      "Projects",
      "Structered Output & Retry"
    ]
  },
  {
    "objectID": "PydanticAi/3_output.html#output-validators",
    "href": "PydanticAi/3_output.html#output-validators",
    "title": "Pydantic AI - Output Handling Guide",
    "section": "",
    "text": "Add validation logic that requires IO or is asynchronous using the @agent.output_validator decorator.\nfrom pydantic import BaseModel\nfrom pydantic_ai import Agent, RunContext, ModelRetry\n\nclass Success(BaseModel):\n    sql_query: str\n\nclass InvalidRequest(BaseModel):\n    error_message: str\n\nagent = Agent('google-gla:gemini-1.5-flash', output_type=Success | InvalidRequest)\n\n@agent.output_validator\nasync def validate_sql(ctx: RunContext, output: Success | InvalidRequest):\n    if isinstance(output, InvalidRequest):\n        return output\n    \n    try:\n        await ctx.deps.execute(f'EXPLAIN {output.sql_query}')\n    except QueryError as e:\n        raise ModelRetry(f'Invalid query: {e}') from e\n    else:\n        return output",
    "crumbs": [
      "Projects",
      "Structered Output & Retry"
    ]
  },
  {
    "objectID": "PydanticAi/3_output.html#output-validator---a-good-example-to-understand",
    "href": "PydanticAi/3_output.html#output-validator---a-good-example-to-understand",
    "title": "Pydantic AI - Output Handling Guide",
    "section": "",
    "text": "from dotenv import load_dotenv\nload_dotenv(dotenv_path='.env')\n\nfrom pydantic import BaseModel\nfrom pydantic_ai import Agent, RunContext, ModelRetry\n\nclass ValidAge(BaseModel):\n    age: int\n    name: str\n\nclass InvalidAge(BaseModel):\n    error: str\n\n# Agent with union output type\nagent = Agent(\n    'openai:gpt-5-nano',\n    output_type=ValidAge | InvalidAge, \n    system_prompt='Model Will always Extract person name and age from text, ValidAge is 0 to 200',\n    retries=3\n)\n\n@agent.output_validator\nasync def validate_age(ctx: RunContext[None], output: ValidAge | InvalidAge):\n    # If already an error, return it\n    if isinstance(output, InvalidAge):\n        return output\n    \n    # Check if age is reasonable (between 0 and 120)\n    if (output.age &lt; 0 or output.age &gt; 100) and ctx.retry&lt;2:\n        # Ask model to try again\n        print(f\"retrying{ctx.retry} {output.age}\")\n        raise ModelRetry(\n            f'What if my age is {output.age-1}'\n        )\n    if ctx.retry == 2:\n        raise ModelRetry(\n            f'What if my age is {output.age+100}'\n        )  \n    \n    return output\n\n# Usage\nresult = agent.run_sync('My name is John and I am 25 years old')\nprint(result.output)\nprint()\n# Output: ValidAge(age=25, name='John')\n\nresult2 = agent.run_sync('My name is Alice and I am 105 years old')\nprint(result2.output, type(result2))\nOutput\nage=25 name='John'\n\nretrying0 105\nretrying1 104\nerror='Invalid age: 203. Age must be between 0 and 200.' &lt;class 'pydantic_ai.run.AgentRunResult'&gt;",
    "crumbs": [
      "Projects",
      "Structered Output & Retry"
    ]
  },
  {
    "objectID": "PydanticAi/3_output.html#key-points",
    "href": "PydanticAi/3_output.html#key-points",
    "title": "Pydantic AI - Output Handling Guide",
    "section": "",
    "text": "Default output type is str\nOutput is wrapped in AgentRunResult or StreamedRunResult\nAccess usage data via result.usage()\nOutput functions can raise ModelRetry for validation errors",
    "crumbs": [
      "Projects",
      "Structered Output & Retry"
    ]
  },
  {
    "objectID": "PydanticAi/5_websearch.html",
    "href": "PydanticAi/5_websearch.html",
    "title": "TDS by HRM",
    "section": "",
    "text": "Lets the agent search the internet for up-to-date information.\nSupported providers: OpenAI Responses, Anthropic, Gemini, Groq.\n\n\nfrom dotenv import load_dotenv\nload_dotenv()\nfrom pydantic_ai import Agent, WebSearchTool\n\n# Create agent with web search capability\nagent = Agent(\n    'gemini-2.5-flash', \n    builtin_tools=[WebSearchTool()]\n)\n\nresult = agent.run_sync('What is the current price of gold in delhi, india in INR for 24carat 10gm gold? Tell me source url also')\nprint(result.output)\n\n'''\nAs of September 30, 2025, the current price of 24-carat, 10-gram gold in Delhi, India, is ‚Çπ1,16,583.00.\n\nYou can find this information on the following source:\n*   Mint: https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGMZG4FW-QdePISRX-wVfa1-ou0EU_khN_9eesaTkxVNyPhMO-35cKBD8-mA7y9cTcUn8WqJSBEwSlm2pf8HXO_FeXp_c53p6xiJcR-UvEyXR8AKoq0JeASDokno9BrssS5-e0X\n'''\nWhat happens:\n\nAgent receives your question\nRealizes it needs current data\nUses WebSearchTool to search the web\nReturns answer with latest information",
    "crumbs": [
      "Projects",
      "Web Search"
    ]
  },
  {
    "objectID": "PydanticAi/5_websearch.html#websearchtool",
    "href": "PydanticAi/5_websearch.html#websearchtool",
    "title": "TDS by HRM",
    "section": "",
    "text": "Lets the agent search the internet for up-to-date information.\nSupported providers: OpenAI Responses, Anthropic, Gemini, Groq.\n\n\nfrom dotenv import load_dotenv\nload_dotenv()\nfrom pydantic_ai import Agent, WebSearchTool\n\n# Create agent with web search capability\nagent = Agent(\n    'gemini-2.5-flash', \n    builtin_tools=[WebSearchTool()]\n)\n\nresult = agent.run_sync('What is the current price of gold in delhi, india in INR for 24carat 10gm gold? Tell me source url also')\nprint(result.output)\n\n'''\nAs of September 30, 2025, the current price of 24-carat, 10-gram gold in Delhi, India, is ‚Çπ1,16,583.00.\n\nYou can find this information on the following source:\n*   Mint: https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGMZG4FW-QdePISRX-wVfa1-ou0EU_khN_9eesaTkxVNyPhMO-35cKBD8-mA7y9cTcUn8WqJSBEwSlm2pf8HXO_FeXp_c53p6xiJcR-UvEyXR8AKoq0JeASDokno9BrssS5-e0X\n'''\nWhat happens:\n\nAgent receives your question\nRealizes it needs current data\nUses WebSearchTool to search the web\nReturns answer with latest information",
    "crumbs": [
      "Projects",
      "Web Search"
    ]
  },
  {
    "objectID": "PydanticAi/7_dep.html",
    "href": "PydanticAi/7_dep.html",
    "title": "TDS by HRM",
    "section": "",
    "text": "Dependencies are external resources, data, or services that your AI agent needs to access during execution.\n\nCan be any Python type: databases, API clients, configuration, user context, etc.\nPassed to agents via dependency injection pattern\nAccessible in: system prompts, instructions, tools, and output validators",
    "crumbs": [
      "Projects",
      "Dependency"
    ]
  },
  {
    "objectID": "PydanticAi/7_dep.html#what-are-dependencies",
    "href": "PydanticAi/7_dep.html#what-are-dependencies",
    "title": "TDS by HRM",
    "section": "",
    "text": "Dependencies are external resources, data, or services that your AI agent needs to access during execution.\n\nCan be any Python type: databases, API clients, configuration, user context, etc.\nPassed to agents via dependency injection pattern\nAccessible in: system prompts, instructions, tools, and output validators",
    "crumbs": [
      "Projects",
      "Dependency"
    ]
  },
  {
    "objectID": "PydanticAi/7_dep.html#where-dependencies-are-used",
    "href": "PydanticAi/7_dep.html#where-dependencies-are-used",
    "title": "TDS by HRM",
    "section": "Where Dependencies Are Used",
    "text": "Where Dependencies Are Used\n\n\n\n\n\n\n\n\nComponent\nAccess Via\nUse Case\n\n\n\n\nInstructions\n@agent.instructions decorator with RunContext\nDynamic, per-request context (user info, session data)\n\n\nSystem Prompts\n@agent.system_prompt decorator with RunContext\nRuntime configuration (API keys, settings)\n\n\nTools\n@agent.tool decorator with RunContext\nExternal data fetching (DB queries, API calls)\n\n\nOutput Validators\n@agent.output_validator with RunContext\nValidation against external rules",
    "crumbs": [
      "Projects",
      "Dependency"
    ]
  },
  {
    "objectID": "PydanticAi/7_dep.html#basic-syntax---quick-example",
    "href": "PydanticAi/7_dep.html#basic-syntax---quick-example",
    "title": "TDS by HRM",
    "section": "Basic Syntax - Quick Example",
    "text": "Basic Syntax - Quick Example\nfrom dataclasses import dataclass\nfrom pydantic_ai import Agent, RunContext\n\n# 1. Define dependency container\n@dataclass\nclass MyDeps:\n    api_key: str\n    user_name: str\n\n# 2. Create agent with deps_type (type, not instance!)\nagent = Agent('openai:gpt-4o', deps_type=MyDeps)\n\n# 3. Access deps via RunContext in instructions/tools\n@agent.instructions\ndef add_context(ctx: RunContext[MyDeps]) -&gt; str:\n    # Access via ctx.deps\n    return f\"User is {ctx.deps.user_name}, API key: {ctx.deps.api_key}\"\n\n@agent.tool\ndef get_data(ctx: RunContext[MyDeps], query: str) -&gt; str:\n    # Use deps in tools\n    return f\"Fetching {query} for {ctx.deps.user_name}\"\n\n# 4. Run with actual dependency instance\ndeps = MyDeps(api_key='secret123', user_name='Alice')\nresult = agent.run_sync('Help me', deps=deps)\nKey Points:\n\ndeps_type=MyDeps in agent creation (just the type)\nctx: RunContext[MyDeps] in functions (for type safety)\nctx.deps.attribute to access dependency values\ndeps=instance when running the agent",
    "crumbs": [
      "Projects",
      "Dependency"
    ]
  },
  {
    "objectID": "PydanticAi/7_dep.html#instructions-vs-system-prompt-with-dependencies",
    "href": "PydanticAi/7_dep.html#instructions-vs-system-prompt-with-dependencies",
    "title": "TDS by HRM",
    "section": "Instructions vs System Prompt with Dependencies",
    "text": "Instructions vs System Prompt with Dependencies\nUse with instructions (Recommended):\n\nWhen each request has different context (different users, sessions)\nDynamic dependencies that change per run\nMulti-agent systems with varying contexts\n\nUse with system_prompt:\n\nWhen context is consistent across conversation\nSingle agent maintaining state across turns",
    "crumbs": [
      "Projects",
      "Dependency"
    ]
  },
  {
    "objectID": "PydanticAi/7_dep.html#example-1-customer-support-with-dynamic-context-using-instructions",
    "href": "PydanticAi/7_dep.html#example-1-customer-support-with-dynamic-context-using-instructions",
    "title": "TDS by HRM",
    "section": "Example 1: Customer Support with Dynamic Context (Using instructions)",
    "text": "Example 1: Customer Support with Dynamic Context (Using instructions)\n\nScenario\nBank support agent that needs customer-specific information for each request.\nfrom dataclasses import dataclass\nfrom pydantic import BaseModel\nfrom pydantic_ai import Agent, RunContext\n\n# 1. Define Dependencies Container\n@dataclass\nclass SupportDependencies:\n    customer_id: int\n    db: DatabaseConn  # Database connection\n\n# 2. Define Output Structure\nclass SupportOutput(BaseModel):\n    support_advice: str\n    block_card: bool\n    risk: int\n\n# 3. Create Agent with Dependencies Type\nsupport_agent = Agent(\n    'openai:gpt-4o',\n    deps_type=SupportDependencies,  # ‚Üê Specify dependency type\n    output_type=SupportOutput,\n    instructions=\"You are a support agent. Reply using the customer's name.\"\n)\n\n# 4. Dynamic Instructions Using Dependencies\n@support_agent.instructions\nasync def add_customer_name(ctx: RunContext[SupportDependencies]) -&gt; str:\n    # Access database through dependencies\n    customer_name = await ctx.deps.db.customer_name(id=ctx.deps.customer_id)\n    return f\"The customer's name is {customer_name!r}\"\n\n# 5. Tool Using Dependencies\n@support_agent.tool\nasync def customer_balance(\n    ctx: RunContext[SupportDependencies], \n    include_pending: bool\n) -&gt; str:\n    \"\"\"Returns the customer's current account balance.\"\"\"\n    balance = await ctx.deps.db.customer_balance(\n        id=ctx.deps.customer_id,\n        include_pending=include_pending\n    )\n    return f'${balance:.2f}'\n\n# 6. Run Agent with Specific Dependencies\ndeps = SupportDependencies(customer_id=123, db=DatabaseConn())\nresult = support_agent.run_sync('What is my balance?', deps=deps)\nprint(result.output)\n# Output: support_advice='Hello John, your balance is $123.45.' \n#         block_card=False risk=1\n\n# Different customer - fresh dependencies\ndeps2 = SupportDependencies(customer_id=456, db=DatabaseConn())\nresult2 = support_agent.run_sync('What is my balance?', deps=deps2)\n# Gets Alice's information - clean context switch\nWhy This Works:\n\nEach customer gets fresh dependencies with their own context\nDatabase connection is shared but customer_id changes\nInstructions dynamically include the correct customer name\nTools access the right customer‚Äôs data\n\n\nDependencies enable agents to access external resources in a type-safe, testable way. Use them with instructions for dynamic, per-request context (like customer info), or with system_prompt for consistent, conversation-wide context (like API credentials).",
    "crumbs": [
      "Projects",
      "Dependency"
    ]
  },
  {
    "objectID": "W1/1_window_to_linux/boot.html",
    "href": "W1/1_window_to_linux/boot.html",
    "title": "Computer Boot Process: Essential Guide",
    "section": "",
    "text": "Overview\nBoot Process Steps\nPartition Styles vs File Systems\nBoot Process Summary\nFrequently Asked Questions\nResources\n\n\n\n\nThe computer boot process transforms your computer from powered-off state to a fully operational system. This guide covers the essential steps and concepts needed to understand how computers start.\n\n\n\n\n\nWhen you press the power button:\n\nCPU executes the first program BIOS/UEFI\n\nBIOS (Basic Input/Output System) - Legacy firmware (used in old computers)\nUEFI (Unified Extensible Firmware Interface) - Modern firmware\n\n\n\n\n\nPOST (Power-On Self Test):\n\nTests CPU, RAM, and storage devices\nValidates hardware components\n\nBoot Device Selection:\n\nReads Boot Order from firmware settings\nGPT drives: Looks for EFI System Partition\nMBR drives: Checks Master Boot Record in first sector\n\n\n\n\nCommon Bootloaders:\nLinux or Window bootloader, both can scan and start any OS windows or linux.\n- Linux: GRUB2, LILO, systemd-boot - Windows: Windows Boot Manager\nBootloader Tasks:\n\nScans partitions for installed operating systems\nPresents boot menu (if multiple OS found)\nLoads selected OS kernel into memory\n\n\n\n\nLinux OS Boot:\n\nKernel loads and initializes hardware\nsystemd starts (modern init system)\nSystem services launch\nUser login interface appears\n\nWindows OS Boot:\n\nNT Kernel (ntoskrnl.exe) loads\nHardware Abstraction Layer initializes\nRegistry and system drivers load\nSession Manager starts Windows subsystems\nWindows Logon presents login interface\n\n\n\n\n\n\n\nPartition styles define how a drive is divided into sections:\n\n\n\n\n\n\n\n\nFeature\nMBR\nGPT\n\n\n\n\nMax Partitions\n4 primary OR 3 primary + 1 extended\n128 primary\n\n\nMax Storage\n2 TB\n18+ exabytes\n\n\nBoot Support\nBIOS only\nBIOS + UEFI\n\n\n\n\n\n\nFile systems determine how data is stored within partitions:\n\n\n\nFile System\nOS\nUse Case\n\n\n\n\nNTFS\nWindows\nSystem drives, large files\n\n\nFAT32\nCross-platform\nUSB drives, compatibility\n\n\next4\nLinux\nLinux system drives\n\n\nAPFS\nmacOS\nmacOS system drives\n\n\n\n\n\n\n\n\n\n\nPhase\nComponent\nPurpose\n\n\n\n\n1\nFirmware (BIOS/UEFI)\nHardware initialization\n\n\n2\nPOST\nHardware verification\n\n\n3\nBootloader\nOS selection and loading\n\n\n4\nOS Kernel\nSystem initialization\n\n\n\n\n\n\n\n\nflowchart LR\n    A[Power On] --&gt; B[Bios/Uefi]\n    B --&gt; C[POST Check]\n    C --&gt; D[LocateDrive]\n    D --&gt; E[Run Bootloader]\n    E --&gt; F{Select OS}\n    F --&gt;|Linux| G[Kernel + systemd]\n    F --&gt;|Windows| H[NT Kernel]\n    G --&gt; I[User Login]\n    H --&gt; I\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeature\nBIOS\nUEFI\n\n\n\n\nInterface\nText-only\nGraphical possible\n\n\nStorage Support\n2 TB max\nNo practical limit\n\n\nSecurity\nBasic\nSecure Boot\n\n\nSpeed\nSlower\nFaster\n\n\n\n\n\n\nPartition Style: Defines how the drive is divided (MBR vs GPT) File System: Defines how files are stored within each partition (NTFS, ext4, etc.)\n\n\n\nYes, by:\n\nInstalling each OS on separate partitions\nUsing a bootloader that detects all systems\nSelecting which OS to boot at startup\n\n\n\n\n\n\n\n\nBoot Process (English)\nBoot Process (Hindi)\nWindows Partitions (Hindi)\n\n\n\nBoot Process (English) \nBoot Process (Hindi) \nWindows Partitions (Hindi)"
  },
  {
    "objectID": "W1/1_window_to_linux/boot.html#table-of-contents",
    "href": "W1/1_window_to_linux/boot.html#table-of-contents",
    "title": "Computer Boot Process: Essential Guide",
    "section": "",
    "text": "Overview\nBoot Process Steps\nPartition Styles vs File Systems\nBoot Process Summary\nFrequently Asked Questions\nResources"
  },
  {
    "objectID": "W1/1_window_to_linux/boot.html#overview",
    "href": "W1/1_window_to_linux/boot.html#overview",
    "title": "Computer Boot Process: Essential Guide",
    "section": "",
    "text": "The computer boot process transforms your computer from powered-off state to a fully operational system. This guide covers the essential steps and concepts needed to understand how computers start."
  },
  {
    "objectID": "W1/1_window_to_linux/boot.html#boot-process-steps",
    "href": "W1/1_window_to_linux/boot.html#boot-process-steps",
    "title": "Computer Boot Process: Essential Guide",
    "section": "",
    "text": "When you press the power button:\n\nCPU executes the first program BIOS/UEFI\n\nBIOS (Basic Input/Output System) - Legacy firmware (used in old computers)\nUEFI (Unified Extensible Firmware Interface) - Modern firmware\n\n\n\n\n\nPOST (Power-On Self Test):\n\nTests CPU, RAM, and storage devices\nValidates hardware components\n\nBoot Device Selection:\n\nReads Boot Order from firmware settings\nGPT drives: Looks for EFI System Partition\nMBR drives: Checks Master Boot Record in first sector\n\n\n\n\nCommon Bootloaders:\nLinux or Window bootloader, both can scan and start any OS windows or linux.\n- Linux: GRUB2, LILO, systemd-boot - Windows: Windows Boot Manager\nBootloader Tasks:\n\nScans partitions for installed operating systems\nPresents boot menu (if multiple OS found)\nLoads selected OS kernel into memory\n\n\n\n\nLinux OS Boot:\n\nKernel loads and initializes hardware\nsystemd starts (modern init system)\nSystem services launch\nUser login interface appears\n\nWindows OS Boot:\n\nNT Kernel (ntoskrnl.exe) loads\nHardware Abstraction Layer initializes\nRegistry and system drivers load\nSession Manager starts Windows subsystems\nWindows Logon presents login interface"
  },
  {
    "objectID": "W1/1_window_to_linux/boot.html#partition-styles-vs-file-systems",
    "href": "W1/1_window_to_linux/boot.html#partition-styles-vs-file-systems",
    "title": "Computer Boot Process: Essential Guide",
    "section": "",
    "text": "Partition styles define how a drive is divided into sections:\n\n\n\n\n\n\n\n\nFeature\nMBR\nGPT\n\n\n\n\nMax Partitions\n4 primary OR 3 primary + 1 extended\n128 primary\n\n\nMax Storage\n2 TB\n18+ exabytes\n\n\nBoot Support\nBIOS only\nBIOS + UEFI\n\n\n\n\n\n\nFile systems determine how data is stored within partitions:\n\n\n\nFile System\nOS\nUse Case\n\n\n\n\nNTFS\nWindows\nSystem drives, large files\n\n\nFAT32\nCross-platform\nUSB drives, compatibility\n\n\next4\nLinux\nLinux system drives\n\n\nAPFS\nmacOS\nmacOS system drives"
  },
  {
    "objectID": "W1/1_window_to_linux/boot.html#boot-process-summary",
    "href": "W1/1_window_to_linux/boot.html#boot-process-summary",
    "title": "Computer Boot Process: Essential Guide",
    "section": "",
    "text": "Phase\nComponent\nPurpose\n\n\n\n\n1\nFirmware (BIOS/UEFI)\nHardware initialization\n\n\n2\nPOST\nHardware verification\n\n\n3\nBootloader\nOS selection and loading\n\n\n4\nOS Kernel\nSystem initialization\n\n\n\n\n\n\n\n\nflowchart LR\n    A[Power On] --&gt; B[Bios/Uefi]\n    B --&gt; C[POST Check]\n    C --&gt; D[LocateDrive]\n    D --&gt; E[Run Bootloader]\n    E --&gt; F{Select OS}\n    F --&gt;|Linux| G[Kernel + systemd]\n    F --&gt;|Windows| H[NT Kernel]\n    G --&gt; I[User Login]\n    H --&gt; I"
  },
  {
    "objectID": "W1/1_window_to_linux/boot.html#frequently-asked-questions",
    "href": "W1/1_window_to_linux/boot.html#frequently-asked-questions",
    "title": "Computer Boot Process: Essential Guide",
    "section": "",
    "text": "Feature\nBIOS\nUEFI\n\n\n\n\nInterface\nText-only\nGraphical possible\n\n\nStorage Support\n2 TB max\nNo practical limit\n\n\nSecurity\nBasic\nSecure Boot\n\n\nSpeed\nSlower\nFaster\n\n\n\n\n\n\nPartition Style: Defines how the drive is divided (MBR vs GPT) File System: Defines how files are stored within each partition (NTFS, ext4, etc.)\n\n\n\nYes, by:\n\nInstalling each OS on separate partitions\nUsing a bootloader that detects all systems\nSelecting which OS to boot at startup"
  },
  {
    "objectID": "W1/1_window_to_linux/boot.html#resources",
    "href": "W1/1_window_to_linux/boot.html#resources",
    "title": "Computer Boot Process: Essential Guide",
    "section": "",
    "text": "Boot Process (English)\nBoot Process (Hindi)\nWindows Partitions (Hindi)\n\n\n\nBoot Process (English) \nBoot Process (Hindi) \nWindows Partitions (Hindi)"
  },
  {
    "objectID": "W1/1_window_to_linux/vm.html",
    "href": "W1/1_window_to_linux/vm.html",
    "title": "In Windows",
    "section": "",
    "text": "Download and Install Virtual Box\n\nLink\n\nDownload .iso file of any distribution\n\ne.g.¬†Ubuntu\n\nInstall iso file in Virtual Box\n\nClick on New\nSelect Iso and fill required data‚Ä¶\n\n\n\n\n\n\n\n\nVirtual Box Setup (Hindi)"
  },
  {
    "objectID": "W1/1_window_to_linux/vm.html#using-type-2-hypervisor",
    "href": "W1/1_window_to_linux/vm.html#using-type-2-hypervisor",
    "title": "In Windows",
    "section": "",
    "text": "Download and Install Virtual Box\n\nLink\n\nDownload .iso file of any distribution\n\ne.g.¬†Ubuntu\n\nInstall iso file in Virtual Box\n\nClick on New\nSelect Iso and fill required data‚Ä¶"
  },
  {
    "objectID": "W1/1_window_to_linux/vm.html#resources",
    "href": "W1/1_window_to_linux/vm.html#resources",
    "title": "In Windows",
    "section": "",
    "text": "Virtual Box Setup (Hindi)"
  },
  {
    "objectID": "W1/1_window_to_linux/w1_2.html",
    "href": "W1/1_window_to_linux/w1_2.html",
    "title": "WSL - Windows Subsytem for Linux",
    "section": "",
    "text": "We will use windows powershell to install wsl.\n\n\n\nOpen PowerShell\nwsl --version or wsl.exe --version\nIf any above command shows wsl version 2 , then it is already installed\n\n\n\n\n\nSearch Turn Windows features on or off\n\nEnable Virtual Machine Platform, Windows Hypervisor Platform, Windows Subsytem for Linux\n\nRestart Computer (Not power on/off, click on restart)\nInstall WSL WSL-2 Setup\nSwitch to WSL-2 wsl --set-default-version 2\n\n\n\n\nwsl --list --verbose\nFor Each Listed distribution\n- wsl --unregister &lt;DistributionName&gt;\n- Open Settings ‚Üí Apps ‚Üí Installed apps - Find each Linux distribution, click the three-dot menu, and select Uninstall\n\n\n\n\nwsl --update\n\nwsl --list --online\n\nwsl --install Ubuntu-24.04 ‚Äì&gt; Install Ubuntu\n\nsudo apt update && sudo apt upgrade -y ‚Äì&gt; Update Ubuntu\n\n\n\n\n\n\n\nNoteExample\n\n\n\n\n\nPS C:\\Users\\hrith&gt; wsl --install Ubuntu-24.04\nDownloading: Ubuntu 24.04 LTS\nInstalling: Ubuntu 24.04 LTS\nDistribution successfully installed. It can be launched via 'wsl.exe -d Ubuntu-24.04'\nLaunching Ubuntu-24.04...\nProvisioning the new WSL instance Ubuntu-24.04\nThis might take a while...\nCreate a default Unix user account: hrm\nNew password:\nRetype new password:\npasswd: password updated successfully\nTo run a command as administrator (user \"root\"), use \"sudo &lt;command&gt;\".\nSee \"man sudo_root\" for details.\n\nhrm@bitnd:/mnt/c/Users/hrith$ sudo apt update && sudo apt upgrade -y\n[sudo] password for hrm:\nHit:1 http://archive.ubuntu.com/ubuntu noble InRelease\nGet:2 http://security.ubuntu.com/ubuntu noble-security InRelease [126 kB]\n\n\n\n\n\n\n\nPython VENV\n\npython3 -m venv .venv ‚Äì&gt; Copy error code and run sudo apt install python3.12-venv\n\nsudo apt install python3-pip\n\nPIPX\n\nsudo apt install pipx\n\nUV - Rust-based Python package installer\n\npipx install uv It will maintain isolation\n\nLLM\n\npipx install llm ‚Äì&gt; pipx ensurepath\nConfigure it\n\nllm install llm-gemini or llm install llm-ollama\nllm keys set gemini\nllm -m gemini-2.0-flash 'Tell me fun facts about Mountain View'\n\n\nMiniConda\n\nDownload .sh https://www.anaconda.com/download/success\nbash &lt;pathto .sh file&gt;\nconda config --set auto_activate_base false\n\n\n\n\n\n\nnvm install visit and run bash script https://github.com/nvm-sh/nvm\n\nnode install https://nodejs.org/en/download\n\nnvm install 22\n\nnvm list nvm use 22 nvm current\n\ncorepack enable yarn\n\ncorepack enable pnpm\n\n\n\nCheck\n- nvm -v\n- node -v\n- npm -v npx -v\n- pnpm -v\n- yarn -v\n\n\n\n\nsudo apt install build-essential\n\ngcc ‚Äì&gt; The C compiler\n\ng++ ‚Äì&gt; The C++ compiler\n\n\nCheck - gcc ‚Äìverison\n- g++ ‚Äìversion\n\n\n\n\nsudo apt install default-jdk\n\nThis command installs:\n\nJava Development Kit (JDK) - Compiler, debugger, and development tools  \nJava Runtime Environment (JRE) - Required to run Java applications  \nJava Virtual Machine (JVM) - Core execution environment  \n\nConfigure JAVA_HOME Environment Variable\n\necho 'export JAVA_HOME=\"/usr/lib/jvm/default-java\"' &gt;&gt; ~/.bashrc ## confirm the path first using below update-alternatiove‚Ä¶ command\n\nrestart shell\n\nInstall other versions of java\n\nsudo apt install openjdk-17-jdk\n\nSet Default Java/Javac installed version\n\nsudo update-alternatives --config java\nsudo update-alternatives --config javac\n\n\n\nCheck\n- java --version\n- javac --version\n- echo $JAVA_HOME\n\n\n\n\nFor example let install quarto https://quarto.org/docs/get-started/\n\nDownload and move to directory where deb package is downloaded\n\nsudo apt install ./quarto-1.8.24-linux-amd64.deb\n\n\n\n\n\n\n\nWSL-2 Setup",
    "crumbs": [
      "Week1",
      "1. Windows To Linux",
      "WSL"
    ]
  },
  {
    "objectID": "W1/1_window_to_linux/w1_2.html#check-wsl-2-installed-or-not",
    "href": "W1/1_window_to_linux/w1_2.html#check-wsl-2-installed-or-not",
    "title": "WSL - Windows Subsytem for Linux",
    "section": "",
    "text": "Open PowerShell\nwsl --version or wsl.exe --version\nIf any above command shows wsl version 2 , then it is already installed",
    "crumbs": [
      "Week1",
      "1. Windows To Linux",
      "WSL"
    ]
  },
  {
    "objectID": "W1/1_window_to_linux/w1_2.html#if-wsl-2-not-instlled",
    "href": "W1/1_window_to_linux/w1_2.html#if-wsl-2-not-instlled",
    "title": "WSL - Windows Subsytem for Linux",
    "section": "",
    "text": "Search Turn Windows features on or off\n\nEnable Virtual Machine Platform, Windows Hypervisor Platform, Windows Subsytem for Linux\n\nRestart Computer (Not power on/off, click on restart)\nInstall WSL WSL-2 Setup\nSwitch to WSL-2 wsl --set-default-version 2",
    "crumbs": [
      "Week1",
      "1. Windows To Linux",
      "WSL"
    ]
  },
  {
    "objectID": "W1/1_window_to_linux/w1_2.html#if-already-installed-then-remove-previous-installed-distributions",
    "href": "W1/1_window_to_linux/w1_2.html#if-already-installed-then-remove-previous-installed-distributions",
    "title": "WSL - Windows Subsytem for Linux",
    "section": "",
    "text": "wsl --list --verbose\nFor Each Listed distribution\n- wsl --unregister &lt;DistributionName&gt;\n- Open Settings ‚Üí Apps ‚Üí Installed apps - Find each Linux distribution, click the three-dot menu, and select Uninstall",
    "crumbs": [
      "Week1",
      "1. Windows To Linux",
      "WSL"
    ]
  },
  {
    "objectID": "W1/1_window_to_linux/w1_2.html#update-wsl-ubuntu-installation",
    "href": "W1/1_window_to_linux/w1_2.html#update-wsl-ubuntu-installation",
    "title": "WSL - Windows Subsytem for Linux",
    "section": "",
    "text": "wsl --update\n\nwsl --list --online\n\nwsl --install Ubuntu-24.04 ‚Äì&gt; Install Ubuntu\n\nsudo apt update && sudo apt upgrade -y ‚Äì&gt; Update Ubuntu\n\n\n\n\n\n\n\nNoteExample\n\n\n\n\n\nPS C:\\Users\\hrith&gt; wsl --install Ubuntu-24.04\nDownloading: Ubuntu 24.04 LTS\nInstalling: Ubuntu 24.04 LTS\nDistribution successfully installed. It can be launched via 'wsl.exe -d Ubuntu-24.04'\nLaunching Ubuntu-24.04...\nProvisioning the new WSL instance Ubuntu-24.04\nThis might take a while...\nCreate a default Unix user account: hrm\nNew password:\nRetype new password:\npasswd: password updated successfully\nTo run a command as administrator (user \"root\"), use \"sudo &lt;command&gt;\".\nSee \"man sudo_root\" for details.\n\nhrm@bitnd:/mnt/c/Users/hrith$ sudo apt update && sudo apt upgrade -y\n[sudo] password for hrm:\nHit:1 http://archive.ubuntu.com/ubuntu noble InRelease\nGet:2 http://security.ubuntu.com/ubuntu noble-security InRelease [126 kB]",
    "crumbs": [
      "Week1",
      "1. Windows To Linux",
      "WSL"
    ]
  },
  {
    "objectID": "W1/1_window_to_linux/w1_2.html#install-python-tools",
    "href": "W1/1_window_to_linux/w1_2.html#install-python-tools",
    "title": "WSL - Windows Subsytem for Linux",
    "section": "",
    "text": "Python VENV\n\npython3 -m venv .venv ‚Äì&gt; Copy error code and run sudo apt install python3.12-venv\n\nsudo apt install python3-pip\n\nPIPX\n\nsudo apt install pipx\n\nUV - Rust-based Python package installer\n\npipx install uv It will maintain isolation\n\nLLM\n\npipx install llm ‚Äì&gt; pipx ensurepath\nConfigure it\n\nllm install llm-gemini or llm install llm-ollama\nllm keys set gemini\nllm -m gemini-2.0-flash 'Tell me fun facts about Mountain View'\n\n\nMiniConda\n\nDownload .sh https://www.anaconda.com/download/success\nbash &lt;pathto .sh file&gt;\nconda config --set auto_activate_base false",
    "crumbs": [
      "Week1",
      "1. Windows To Linux",
      "WSL"
    ]
  },
  {
    "objectID": "W1/1_window_to_linux/w1_2.html#install-javascript-tools",
    "href": "W1/1_window_to_linux/w1_2.html#install-javascript-tools",
    "title": "WSL - Windows Subsytem for Linux",
    "section": "",
    "text": "nvm install visit and run bash script https://github.com/nvm-sh/nvm\n\nnode install https://nodejs.org/en/download\n\nnvm install 22\n\nnvm list nvm use 22 nvm current\n\ncorepack enable yarn\n\ncorepack enable pnpm\n\n\n\nCheck\n- nvm -v\n- node -v\n- npm -v npx -v\n- pnpm -v\n- yarn -v",
    "crumbs": [
      "Week1",
      "1. Windows To Linux",
      "WSL"
    ]
  },
  {
    "objectID": "W1/1_window_to_linux/w1_2.html#install-cc-tools",
    "href": "W1/1_window_to_linux/w1_2.html#install-cc-tools",
    "title": "WSL - Windows Subsytem for Linux",
    "section": "",
    "text": "sudo apt install build-essential\n\ngcc ‚Äì&gt; The C compiler\n\ng++ ‚Äì&gt; The C++ compiler\n\n\nCheck - gcc ‚Äìverison\n- g++ ‚Äìversion",
    "crumbs": [
      "Week1",
      "1. Windows To Linux",
      "WSL"
    ]
  },
  {
    "objectID": "W1/1_window_to_linux/w1_2.html#install-java-tools",
    "href": "W1/1_window_to_linux/w1_2.html#install-java-tools",
    "title": "WSL - Windows Subsytem for Linux",
    "section": "",
    "text": "sudo apt install default-jdk\n\nThis command installs:\n\nJava Development Kit (JDK) - Compiler, debugger, and development tools  \nJava Runtime Environment (JRE) - Required to run Java applications  \nJava Virtual Machine (JVM) - Core execution environment  \n\nConfigure JAVA_HOME Environment Variable\n\necho 'export JAVA_HOME=\"/usr/lib/jvm/default-java\"' &gt;&gt; ~/.bashrc ## confirm the path first using below update-alternatiove‚Ä¶ command\n\nrestart shell\n\nInstall other versions of java\n\nsudo apt install openjdk-17-jdk\n\nSet Default Java/Javac installed version\n\nsudo update-alternatives --config java\nsudo update-alternatives --config javac\n\n\n\nCheck\n- java --version\n- javac --version\n- echo $JAVA_HOME",
    "crumbs": [
      "Week1",
      "1. Windows To Linux",
      "WSL"
    ]
  },
  {
    "objectID": "W1/1_window_to_linux/w1_2.html#install-any-.deb-packages",
    "href": "W1/1_window_to_linux/w1_2.html#install-any-.deb-packages",
    "title": "WSL - Windows Subsytem for Linux",
    "section": "",
    "text": "For example let install quarto https://quarto.org/docs/get-started/\n\nDownload and move to directory where deb package is downloaded\n\nsudo apt install ./quarto-1.8.24-linux-amd64.deb",
    "crumbs": [
      "Week1",
      "1. Windows To Linux",
      "WSL"
    ]
  },
  {
    "objectID": "W1/1_window_to_linux/w1_2.html#resources",
    "href": "W1/1_window_to_linux/w1_2.html#resources",
    "title": "WSL - Windows Subsytem for Linux",
    "section": "",
    "text": "WSL-2 Setup",
    "crumbs": [
      "Week1",
      "1. Windows To Linux",
      "WSL"
    ]
  },
  {
    "objectID": "W1/1_window_to_linux/w1_4.html",
    "href": "W1/1_window_to_linux/w1_4.html",
    "title": "Dual Boot",
    "section": "",
    "text": "This guide will help you install another operating system (Linux) alongside your existing Windows installation.\n\n\n\n\n\n\nNoteDisclaimer\n\n\n\n\n\nDo this at your own risk. Please watch various YouTube videos and proceed with dual boot only when you are 100% sure about the process.\nIf you want to minimize the risk:\n1. Use External SSD/HDD to install Linux\n- This will prevent almost 99% of any accidental damage to your main system\n2. Install Kubuntu\n- Since dual boot requires creating partitions\n- Kubuntu has a Replace Partition option which avoids most of the headache\n\n\n\n\n\n\nNew SSD/HDD or at least 30GB free storage on your current Windows drive\n\nIf installing on the same SSD/HDD where Windows is installed:\nI recommend shrinking at least a 30GB partition\n\nHow To Shrink Partition\n\n\nPendrive/USB drive (8GB or larger)\nOne ISO file of any Linux distribution. Kubuntu\nSoftware to create a bootable USB drive. Ventoy\n\n\n\n\n\nInsert your USB drive and run Ventoy\nFormat your USB drive with Ventoy to make it bootable\nJust Copy and paste the Linux distribution ISO files into the USB drive\nTo be sure that iso file not got corrupted, you can run sha256sum and check both on local and server matches or not\n\n\n\n\n\nRestart your computer\nGo to BIOS/UEFI Settings\n\nOn HP laptops: restart and keep pressing the F10 key\n\nNavigate to Boot Settings\nDisable Secure Boot (you can re-enable it after installation)\n\n\n\n\nOnce your USB drive and laptop BIOS/UEFI settings are ready, proceed with the installation:\n\nRestart your computer\nGo to Boot Device Options\n\nOn HP laptops: keep pressing the F9 key\n\nChoose your USB Drive\nIf using Ventoy:\n\nIt will ask which OS to boot (based on the ISOs you‚Äôve copied)\n\nIf using BalenaEtcher: it will directly boot into that particular OS\nFor Kubuntu installation:\n\nClick on ‚ÄúInstall‚Äù\nSelect language, keyboard layout, etc.\nWhen choosing where to install or configure partitions:\n\nChoose Replace Partition ‚Äì&gt; then Select the newly created(shrinked) 30GB partition\nThe installer will format two partitions\n\nOne for your Linux installation that you just selected\nOne will be EFI partition of windows(first partition in most cases)\n\n\nChoose your username, password, etc., and proceed\nOnce installation is complete, your computer will automatically reboot or ask you to remove the USB drive and press Enter\nRemove the USB drive and press Enter to reboot\n\n\n\n\n\n\nRestart your computer\nGo to BIOS/UEFI settings\n\nYou can enable Secure Boot if desired\n\nMost importantly:\n\nNavigate to Boot Settings\nChange the boot order\nSet Ubuntu/Linux as the first boot option\n\n\n\n\n\n\nWhen you turn on your computer, you‚Äôll have the option to boot into Ubuntu or Windows\nImportant:\n\nThe first time you boot into Windows after installation:\n\nYou may see a blue screen asking for a BitLocker recovery key\nYou can easily log in with your Microsoft account and retrieve the password from BitLocker Recovery\nThis will only be asked once.\n\n\n\n\n\n\n\nBitLocker Recovery\nKubuntu ISO\nVentoy\n\n\n\n\nShrink Partition",
    "crumbs": [
      "Week1",
      "1. Windows To Linux",
      "Dual Boot"
    ]
  },
  {
    "objectID": "W1/1_window_to_linux/w1_4.html#requirements",
    "href": "W1/1_window_to_linux/w1_4.html#requirements",
    "title": "Dual Boot",
    "section": "",
    "text": "New SSD/HDD or at least 30GB free storage on your current Windows drive\n\nIf installing on the same SSD/HDD where Windows is installed:\nI recommend shrinking at least a 30GB partition\n\nHow To Shrink Partition\n\n\nPendrive/USB drive (8GB or larger)\nOne ISO file of any Linux distribution. Kubuntu\nSoftware to create a bootable USB drive. Ventoy",
    "crumbs": [
      "Week1",
      "1. Windows To Linux",
      "Dual Boot"
    ]
  },
  {
    "objectID": "W1/1_window_to_linux/w1_4.html#prepare-the-usb-drive",
    "href": "W1/1_window_to_linux/w1_4.html#prepare-the-usb-drive",
    "title": "Dual Boot",
    "section": "",
    "text": "Insert your USB drive and run Ventoy\nFormat your USB drive with Ventoy to make it bootable\nJust Copy and paste the Linux distribution ISO files into the USB drive\nTo be sure that iso file not got corrupted, you can run sha256sum and check both on local and server matches or not",
    "crumbs": [
      "Week1",
      "1. Windows To Linux",
      "Dual Boot"
    ]
  },
  {
    "objectID": "W1/1_window_to_linux/w1_4.html#prepare-your-laptopcomputer",
    "href": "W1/1_window_to_linux/w1_4.html#prepare-your-laptopcomputer",
    "title": "Dual Boot",
    "section": "",
    "text": "Restart your computer\nGo to BIOS/UEFI Settings\n\nOn HP laptops: restart and keep pressing the F10 key\n\nNavigate to Boot Settings\nDisable Secure Boot (you can re-enable it after installation)",
    "crumbs": [
      "Week1",
      "1. Windows To Linux",
      "Dual Boot"
    ]
  },
  {
    "objectID": "W1/1_window_to_linux/w1_4.html#installation-process",
    "href": "W1/1_window_to_linux/w1_4.html#installation-process",
    "title": "Dual Boot",
    "section": "",
    "text": "Once your USB drive and laptop BIOS/UEFI settings are ready, proceed with the installation:\n\nRestart your computer\nGo to Boot Device Options\n\nOn HP laptops: keep pressing the F9 key\n\nChoose your USB Drive\nIf using Ventoy:\n\nIt will ask which OS to boot (based on the ISOs you‚Äôve copied)\n\nIf using BalenaEtcher: it will directly boot into that particular OS\nFor Kubuntu installation:\n\nClick on ‚ÄúInstall‚Äù\nSelect language, keyboard layout, etc.\nWhen choosing where to install or configure partitions:\n\nChoose Replace Partition ‚Äì&gt; then Select the newly created(shrinked) 30GB partition\nThe installer will format two partitions\n\nOne for your Linux installation that you just selected\nOne will be EFI partition of windows(first partition in most cases)\n\n\nChoose your username, password, etc., and proceed\nOnce installation is complete, your computer will automatically reboot or ask you to remove the USB drive and press Enter\nRemove the USB drive and press Enter to reboot",
    "crumbs": [
      "Week1",
      "1. Windows To Linux",
      "Dual Boot"
    ]
  },
  {
    "objectID": "W1/1_window_to_linux/w1_4.html#final-computer-setup",
    "href": "W1/1_window_to_linux/w1_4.html#final-computer-setup",
    "title": "Dual Boot",
    "section": "",
    "text": "Restart your computer\nGo to BIOS/UEFI settings\n\nYou can enable Secure Boot if desired\n\nMost importantly:\n\nNavigate to Boot Settings\nChange the boot order\nSet Ubuntu/Linux as the first boot option",
    "crumbs": [
      "Week1",
      "1. Windows To Linux",
      "Dual Boot"
    ]
  },
  {
    "objectID": "W1/1_window_to_linux/w1_4.html#final-notes",
    "href": "W1/1_window_to_linux/w1_4.html#final-notes",
    "title": "Dual Boot",
    "section": "",
    "text": "When you turn on your computer, you‚Äôll have the option to boot into Ubuntu or Windows\nImportant:\n\nThe first time you boot into Windows after installation:\n\nYou may see a blue screen asking for a BitLocker recovery key\nYou can easily log in with your Microsoft account and retrieve the password from BitLocker Recovery\nThis will only be asked once.",
    "crumbs": [
      "Week1",
      "1. Windows To Linux",
      "Dual Boot"
    ]
  },
  {
    "objectID": "W1/1_window_to_linux/w1_4.html#resources",
    "href": "W1/1_window_to_linux/w1_4.html#resources",
    "title": "Dual Boot",
    "section": "",
    "text": "BitLocker Recovery\nKubuntu ISO\nVentoy\n\n\n\n\nShrink Partition",
    "crumbs": [
      "Week1",
      "1. Windows To Linux",
      "Dual Boot"
    ]
  },
  {
    "objectID": "W1/2_git_github/1_git.html",
    "href": "W1/2_git_github/1_git.html",
    "title": "Git",
    "section": "",
    "text": "Git is a distributed VCS(Version Control Software)\nIt store all code history/snapshots in .git folder\nIf you delete even all files, then you can recover all files from .git folder, because all codes are stored there with history\n\n\n\n\nInstall from official website git-scm\n\n\n\n\n\n\n\nNoteFLOW\n\n\n\n\n\nwork --&gt; add/commit --&gt; work --&gt; add/commit --&gt; ...\nwhenever we need we can do undo/redo\n\n\n\nCreate a folder for your project/work\n\ngit config --global init.defaultBranch main or later you may need to do git branch -m master main\ngit init create .git folder\ncreate some files and write something\ngit add . add all files in staging area (except the files/folder name in .gitignore file)\n\n\n\ngit reset abc.txt will unstage, if something done by mistake\n\n\ngit commit -m \"first commit\" record this history in .git folder\nnow repeat 2nd, 3rd and 4th steps\nIf somewhere you want to go back to any specify commit\n\ngit log --one line\ngit checkout &lt;commit_id&gt;\ngit checkout main\ngit reset --soft &lt;commit_id&gt;\n\nsoft -&gt; keeps your changes staged\nmixed -&gt; keeps your changes unstaged\nhard -&gt; ‚ö†Ô∏è discards everything after that commit\n\n\n\n\n\n\n\n\n\nTipJust Previous Commit\n\n\n\n\n\n\ngit reset --soft HEAD~1\n\ngit reset --mixed HEAD~1\n\ngit reset --hard HEAD~1",
    "crumbs": [
      "Week1",
      "2. Git & Github",
      "GIT"
    ]
  },
  {
    "objectID": "W1/2_git_github/1_git.html#lets-start-maintaining-history",
    "href": "W1/2_git_github/1_git.html#lets-start-maintaining-history",
    "title": "Git",
    "section": "",
    "text": "Install from official website git-scm\n\n\n\n\n\n\n\nNoteFLOW\n\n\n\n\n\nwork --&gt; add/commit --&gt; work --&gt; add/commit --&gt; ...\nwhenever we need we can do undo/redo\n\n\n\nCreate a folder for your project/work\n\ngit config --global init.defaultBranch main or later you may need to do git branch -m master main\ngit init create .git folder\ncreate some files and write something\ngit add . add all files in staging area (except the files/folder name in .gitignore file)\n\n\n\ngit reset abc.txt will unstage, if something done by mistake\n\n\ngit commit -m \"first commit\" record this history in .git folder\nnow repeat 2nd, 3rd and 4th steps\nIf somewhere you want to go back to any specify commit\n\ngit log --one line\ngit checkout &lt;commit_id&gt;\ngit checkout main\ngit reset --soft &lt;commit_id&gt;\n\nsoft -&gt; keeps your changes staged\nmixed -&gt; keeps your changes unstaged\nhard -&gt; ‚ö†Ô∏è discards everything after that commit\n\n\n\n\n\n\n\n\n\nTipJust Previous Commit\n\n\n\n\n\n\ngit reset --soft HEAD~1\n\ngit reset --mixed HEAD~1\n\ngit reset --hard HEAD~1",
    "crumbs": [
      "Week1",
      "2. Git & Github",
      "GIT"
    ]
  },
  {
    "objectID": "W1/2_git_github/3_github.html",
    "href": "W1/2_git_github/3_github.html",
    "title": "Github",
    "section": "",
    "text": "GitHub is a cloud-based platform that hosts Git repositories.\n\nIt provides collaboration features like pull requests, issues, and project management.\n\nUnlike Git (local), GitHub makes your code accessible online, enabling teamwork, backups, and sharing.\n\n\nGit = a software (Version Control System)\nGitHub = a cloud service for hosting programming repositories (like Google Drive, but for code)\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nGit itself manages local repositories and pushes/pulls to/from remotes, but it does not create remote repositories on hosting platforms like Github.\n\n\n\nThe repository already exists on GitHub.\n\nThe repository does not yet exist on GitHub but locally exist.\n\n\n\n\n\nLocate the repository on GitHub ‚Üí Click on &lt;&gt; Code ‚Üí Copy the SSH path.\n\nClone the repo into your local machine:\ngit clone git@github.com:myusername/myreponame.git\nA folder myreponame will be created with all repo contents.\nWork as usual ‚Üí edit files, then stage & commit changes:\ngit add .\ngit commit -m \"your message\"\nPush your updates back to GitHub:\ngit push origin main\n\norigin ‚Üí alias for the remote repo URL (e.g., GitHub).\nmain ‚Üí branch name (default branch, often main instead of master).\n\n\n\n\n\n\n\n\nGo to GitHub ‚Üí New Repo.\nEnter repository name ‚Üí e.g., myrepo.\nLeave it empty (don‚Äôt add README, .gitignore, or license).\n\nNote you can again follow scenario 1 where you can clone and then copy paste your new work and then commit and then push, or you can follow below steps.\n\n\n\nFrom your project folder, run:\ngit remote add origin git@github.com:your-username/myrepo.git\n(replace your-username with your actual GitHub username)\n\n\n\ngit branch -M main     # ensure current branch is 'main'\ngit push -u origin main\n\n-u sets the upstream, so later you can simply use:\ngit push\n\nNow your local repo is connected to GitHub. Check it online at: https://github.com/your-username/myrepo",
    "crumbs": [
      "Week1",
      "2. Git & Github",
      "Github"
    ]
  },
  {
    "objectID": "W1/2_git_github/3_github.html#two-scenarios",
    "href": "W1/2_git_github/3_github.html#two-scenarios",
    "title": "Github",
    "section": "",
    "text": "Important\n\n\n\nGit itself manages local repositories and pushes/pulls to/from remotes, but it does not create remote repositories on hosting platforms like Github.\n\n\n\nThe repository already exists on GitHub.\n\nThe repository does not yet exist on GitHub but locally exist.",
    "crumbs": [
      "Week1",
      "2. Git & Github",
      "Github"
    ]
  },
  {
    "objectID": "W1/2_git_github/3_github.html#scenario-1-repo-already-on-github",
    "href": "W1/2_git_github/3_github.html#scenario-1-repo-already-on-github",
    "title": "Github",
    "section": "",
    "text": "Locate the repository on GitHub ‚Üí Click on &lt;&gt; Code ‚Üí Copy the SSH path.\n\nClone the repo into your local machine:\ngit clone git@github.com:myusername/myreponame.git\nA folder myreponame will be created with all repo contents.\nWork as usual ‚Üí edit files, then stage & commit changes:\ngit add .\ngit commit -m \"your message\"\nPush your updates back to GitHub:\ngit push origin main\n\norigin ‚Üí alias for the remote repo URL (e.g., GitHub).\nmain ‚Üí branch name (default branch, often main instead of master).",
    "crumbs": [
      "Week1",
      "2. Git & Github",
      "Github"
    ]
  },
  {
    "objectID": "W1/2_git_github/3_github.html#scenario-2-repo-not-yet-on-github",
    "href": "W1/2_git_github/3_github.html#scenario-2-repo-not-yet-on-github",
    "title": "Github",
    "section": "",
    "text": "Go to GitHub ‚Üí New Repo.\nEnter repository name ‚Üí e.g., myrepo.\nLeave it empty (don‚Äôt add README, .gitignore, or license).\n\nNote you can again follow scenario 1 where you can clone and then copy paste your new work and then commit and then push, or you can follow below steps.\n\n\n\nFrom your project folder, run:\ngit remote add origin git@github.com:your-username/myrepo.git\n(replace your-username with your actual GitHub username)\n\n\n\ngit branch -M main     # ensure current branch is 'main'\ngit push -u origin main\n\n-u sets the upstream, so later you can simply use:\ngit push\n\nNow your local repo is connected to GitHub. Check it online at: https://github.com/your-username/myrepo",
    "crumbs": [
      "Week1",
      "2. Git & Github",
      "Github"
    ]
  },
  {
    "objectID": "W1/3_python_tools/1_uv.html",
    "href": "W1/3_python_tools/1_uv.html",
    "title": "Python Tool - UV",
    "section": "",
    "text": "No need to globally do pip install xyz\nuv add --script a.py numpy\nuv run a.py\nuv remove --script a.py numpy\nhrm@bitnd:~$ batcat a.py\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n       ‚îÇ File: a.py\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n   1   ‚îÇ import numpy as np\n   2   ‚îÇ\n   3   ‚îÇ print(np.array([1,2,3]))\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nhrm@bitnd:~$\nhrm@bitnd:~$ uv add --script a.py numpy\nUpdated `a.py`\n\nhrm@bitnd:~$ batcat a.py\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n       ‚îÇ File: a.py\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n   1   ‚îÇ # /// script\n   2   ‚îÇ # requires-python = \"&gt;=3.12\"\n   3   ‚îÇ # dependencies = [\n   4   ‚îÇ #     \"numpy\",\n   5   ‚îÇ # ]\n   6   ‚îÇ # ///\n   7   ‚îÇ import numpy as np\n   8   ‚îÇ\n   9   ‚îÇ print(np.array([1,2,3]))\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nhrm@bitnd:~$ uv run a.py\n[1 2 3]",
    "crumbs": [
      "Week1",
      "3. Python Tools",
      "UV"
    ]
  },
  {
    "objectID": "W1/3_python_tools/1_uv.html#running-any-standalone-python-script",
    "href": "W1/3_python_tools/1_uv.html#running-any-standalone-python-script",
    "title": "Python Tool - UV",
    "section": "",
    "text": "No need to globally do pip install xyz\nuv add --script a.py numpy\nuv run a.py\nuv remove --script a.py numpy\nhrm@bitnd:~$ batcat a.py\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n       ‚îÇ File: a.py\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n   1   ‚îÇ import numpy as np\n   2   ‚îÇ\n   3   ‚îÇ print(np.array([1,2,3]))\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nhrm@bitnd:~$\nhrm@bitnd:~$ uv add --script a.py numpy\nUpdated `a.py`\n\nhrm@bitnd:~$ batcat a.py\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n       ‚îÇ File: a.py\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n   1   ‚îÇ # /// script\n   2   ‚îÇ # requires-python = \"&gt;=3.12\"\n   3   ‚îÇ # dependencies = [\n   4   ‚îÇ #     \"numpy\",\n   5   ‚îÇ # ]\n   6   ‚îÇ # ///\n   7   ‚îÇ import numpy as np\n   8   ‚îÇ\n   9   ‚îÇ print(np.array([1,2,3]))\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nhrm@bitnd:~$ uv run a.py\n[1 2 3]",
    "crumbs": [
      "Week1",
      "3. Python Tools",
      "UV"
    ]
  },
  {
    "objectID": "W1/3_python_tools/1_uv.html#use-as-normal-virtual-environment",
    "href": "W1/3_python_tools/1_uv.html#use-as-normal-virtual-environment",
    "title": "Python Tool - UV",
    "section": "Use as normal Virtual Environment",
    "text": "Use as normal Virtual Environment\nuv venv myvenv\n# myvenv activate command will be given, execute it\nuv pip install numpy\nuv pip install -r requirements.txt\nuv pip freeze &gt; requirements.txt\nNote Don‚Äôt use uv pip3\nhrm@bitnd:~/test$ ls && batcat a.py\na.py\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n       ‚îÇ File: a.py\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n   1   ‚îÇ import numpy as np\n   2   ‚îÇ\n   3   ‚îÇ print(np.array([1,2,3]))\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\nhrm@bitnd:~/test$ uv venv myvenv\nUsing CPython 3.12.3 interpreter at: /usr/bin/python3\nCreating virtual environment at: myvenv\nActivate with: source myvenv/bin/activate\n\nhrm@bitnd:~/test$ source myvenv/bin/activate\n\n(myvenv) hrm@bitnd:~/test$ which python\n/home/hrm/test/myvenv/bin/python\n\n\n(myvenv) hrm@bitnd:~/test$ uv pip install numpy\nUsing Python 3.12.3 environment at: myvenv\nResolved 1 package in 6ms\nInstalled 1 package in 35ms\n + numpy==2.3.3\n\n(myvenv) hrm@bitnd:~/test$ uv pip freeze\nUsing Python 3.12.3 environment at: myvenv\nnumpy==2.3.3\n\n(myvenv) hrm@bitnd:~/test$ python3 a.py\n[1 2 3]",
    "crumbs": [
      "Week1",
      "3. Python Tools",
      "UV"
    ]
  },
  {
    "objectID": "W1/3_python_tools/3_pipx_uvx.html",
    "href": "W1/3_python_tools/3_pipx_uvx.html",
    "title": "Python Tool - UVX, PIPX, LLM",
    "section": "",
    "text": "# Install a tool, e.g., black formatter\npipx install llm\n# Run a tool once temporarily without installing\npipx run pycowsay hello\n\npipx list # list tools\npipx uninstall black # uninstall a tool \n# Install a tool, e.g., black formatter\nuv tool install llm\n# Run a tool once temporarily without installing\nuvx pycowsay hello\n\n\nuv tool list # list tools\nuv tool uninstall black # uninstall a tool",
    "crumbs": [
      "Week1",
      "3. Python Tools",
      "LLM,UVX,PIPX"
    ]
  },
  {
    "objectID": "W1/3_python_tools/3_pipx_uvx.html#uvx-vs-pipx",
    "href": "W1/3_python_tools/3_pipx_uvx.html#uvx-vs-pipx",
    "title": "Python Tool - UVX, PIPX, LLM",
    "section": "",
    "text": "# Install a tool, e.g., black formatter\npipx install llm\n# Run a tool once temporarily without installing\npipx run pycowsay hello\n\npipx list # list tools\npipx uninstall black # uninstall a tool \n# Install a tool, e.g., black formatter\nuv tool install llm\n# Run a tool once temporarily without installing\nuvx pycowsay hello\n\n\nuv tool list # list tools\nuv tool uninstall black # uninstall a tool",
    "crumbs": [
      "Week1",
      "3. Python Tools",
      "LLM,UVX,PIPX"
    ]
  },
  {
    "objectID": "W1/3_python_tools/3_pipx_uvx.html#llmterminal-setup",
    "href": "W1/3_python_tools/3_pipx_uvx.html#llmterminal-setup",
    "title": "Python Tool - UVX, PIPX, LLM",
    "section": "LLM(terminal) setup",
    "text": "LLM(terminal) setup\n\nCopy Proxy token from here aipipe\nExport relvant environment variable aipipe sannad repo\nexport OPENAI_BASE_URL=https://aipipe.org/openai/v1\nApi key can be set permanently\n\n  hrm@bitnd:~$ llm keys set openai\n  Enter key:\n\n\nhrm@bitnd:~$ uv tool install llm\n`llm` is already installed\n\nhrm@bitnd:~$ export OPENAI_BASE_URL=https://aipipe.org/openai/v1\nhrm@bitnd:~$ llm keys set openai\nEnter key:\n\nhrm@bitnd:~$ llm 'hi'\nHello! How can I assist you today?\n\n\nhrm@bitnd:~$ llm -m 'gpt-4o-mini' 'hi'\nHello! How can I assist you today?",
    "crumbs": [
      "Week1",
      "3. Python Tools",
      "LLM,UVX,PIPX"
    ]
  },
  {
    "objectID": "W1/4_bash/2_bash_script.html",
    "href": "W1/4_bash/2_bash_script.html",
    "title": "Linux - Bash Scripting",
    "section": "",
    "text": "Variables\nVariables store data and don‚Äôt require type declaration. Use $ to access variable values.\n# Variable assignment (no spaces around =)\nname=\"John\"\nage=25\npath=\"/home/user\"\n\n# Access variables\necho $name\necho ${name}  # preferred for clarity\n\n\nData Types\nBash treats everything as strings by default, but supports:\n# String\nmessage=\"Hello World\"\n\n# Integer (for arithmetic)\nnum1=10\nnum2=20\nresult=$((num1 + num2))\n\n# Array\nfruits=(\"apple\" \"banana\" \"cherry\")\necho ${fruits[0]}  # access first element\necho ${fruits[@]}  # all elements\necho ${#fruits[@]} # array length\n\n\nConditionals\nUse if-then-else for decision making:\n# Basic if statement\nif [ $age -gt 18 ]; then\n    echo \"Adult\"\nelif [ $age -eq 18 ]; then\n    echo \"Just turned adult\"\nelse\n    echo \"Minor\"\nfi\n\n# String comparison\nif [ \"$name\" = \"John\" ]; then\n    echo \"Hello John!\"\nfi\n\n# File checks\nif [ -f \"file.txt\" ]; then\n    echo \"File exists\"\nfi\nCommon operators:\n\n-eq equal, -ne not equal\n-gt greater than, -lt less than\n-f file exists, -d directory exists\n\n\n\nLoops\n\nFor Loop\n# Loop through range\nfor i in {1..5}; do\n    echo \"Number: $i\"\ndone\n\n# Loop through array\nfor fruit in \"${fruits[@]}\"; do\n    echo \"Fruit: $fruit\"\ndone\n\n# Loop through files\nfor file in *.txt; do\n    echo \"Processing: $file\"\ndone\n\n\nWhile Loop\ncounter=1\nwhile [ $counter -le 5 ]; do\n    echo \"Count: $counter\"\n    counter=$((counter + 1))\ndone\n\n\n\nText File Handling\n\nReading Files\n# Read line by line\nwhile IFS= read -r line; do\n    echo \"Line: $line\"\ndone &lt; \"input.txt\"\n\n# Read entire file\ncontent=$(cat \"file.txt\")\n\n\nWriting Files\n# Overwrite file\necho \"Hello\" &gt; output.txt\n\n# Append to file\necho \"World\" &gt;&gt; output.txt\n\n# Write multiple lines\ncat &lt;&lt; EOF &gt; config.txt\nSetting1=value1\nSetting2=value2\nEOF\n\n\nText Processing\n# Search in file\ngrep \"pattern\" file.txt\n\n# Replace text\nsed 's/old/new/g' file.txt\n\n# Count lines\nwc -l file.txt\n\n# Extract columns\nawk '{print $1}' file.txt\n\n\n\nFunctions\nfunction greet() {\n    echo \"Hello $1!\"\n}\n\n# Call function\ngreet \"Alice\"\n\n\nCommand Line Arguments\n# Access arguments\necho \"Script name: $0\"\necho \"First argument: $1\"\necho \"All arguments: $@\"\necho \"Number of arguments: $#\"\n\n\n\nExamples\n\nExample 1: Basic System Info Script\n#!/bin/bash\necho \"=== System Information ===\"\necho \"Current user: $(whoami)\"\necho \"Current directory: $(pwd)\"\necho \"Date: $(date)\"\necho \"Disk usage:\"\ndf -h | head -5\n\n\nExample 2: File Processor\n#!/bin/bash\ninput_file=\"data.txt\"\noutput_file=\"processed.txt\"\n\nif [ -f \"$input_file\" ]; then\n    echo \"Processing $input_file...\"\n    \n    # Count lines and words\n    lines=$(wc -l &lt; \"$input_file\")\n    words=$(wc -w &lt; \"$input_file\")\n    \n    echo \"File Stats:\" &gt; \"$output_file\"\n    echo \"Lines: $lines\" &gt;&gt; \"$output_file\"\n    echo \"Words: $words\" &gt;&gt; \"$output_file\"\n    echo \"Content:\" &gt;&gt; \"$output_file\"\n    cat \"$input_file\" &gt;&gt; \"$output_file\"\n    \n    echo \"Processing complete! Check $output_file\"\nelse\n    echo \"Error: $input_file not found!\"\nfi\n\n\nExample 3: Log File Analyzer\n#!/bin/bash\nlog_file=\"/var/log/syslog\"\n\necho \"=== Log Analysis ===\"\necho \"Total lines: $(wc -l &lt; \"$log_file\")\"\necho \"Error count: $(grep -c \"error\" \"$log_file\")\"\necho \"Warning count: $(grep -c \"warning\" \"$log_file\")\"\n\n# Recent errors\necho \"Recent errors:\"\ngrep \"error\" \"$log_file\" | tail -5\n\n\n\nVideo Tutorials\n\nLinux Playlist",
    "crumbs": [
      "Week1",
      "LINUX/BASH - Basic",
      "Bash Script"
    ]
  }
]