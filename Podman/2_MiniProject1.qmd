## MiniProject-1 OllamaTalk (Gemma model running in a Podman container)

* Ollama in podman
* Other work in normal vs code
* Volumne in Podman

```bash
# Pull small ollama imaege
podman pull docker.io/alpine/ollama:0.12.0

podman volume create ollama 
podman run -d -p 11434:11434 -v ollama:/root/.ollama --name ollama docker.io/alpine/ollama:0.12.0

# Check
# http://localhost:11434/

# install model and test it
podman exec -it ollama sh # To exit we can run: exit
ollama pull gemma3:270m
ollama run gemma3:270m
...
...
/exit

exit
```

`main.py` -> A Simple LLM talk
```python
from pydantic_ai import Agent
from pydantic_ai.models.openai import OpenAIChatModel
from pydantic_ai.providers.ollama import OllamaProvider


ollama_model = OpenAIChatModel(
    model_name='gemma3:270m',
    provider=OllamaProvider(base_url='http://localhost:11434/v1'),
)
agent = Agent(ollama_model)

import uvicorn
from fastapi import FastAPI, Query
app = FastAPI()

@app.get('/')
def index(ask: str = Query(..., description="User prompt")):
    result = agent.run_sync(ask)
    return {'ai_answer': result.output}

if __name__ == "__main__":
    # Run the server when script is executed directly
    uvicorn.run(
        app, 
        host="0.0.0.0", 
        port=8000
    )
```