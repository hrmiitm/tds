## MiniProject-4 Formali AI Image to HuggingFace

`Dockerfile`
```bash
FROM docker.io/alpine/ollama:0.12.0

# Set working directory
WORKDIR /app

# Install Python and system dependencies
RUN apk add --no-cache \
    python3 \
    py3-pip \
    curl \
    bash

# Create symbolic link for python command
RUN ln -sf /usr/bin/python3 /usr/bin/python

# Copy requirements and install Python dependencies
COPY requirements.txt .
RUN pip3 install --no-cache-dir -r requirements.txt

# Copy model installation script
COPY install_model.sh .
RUN chmod +x install_model.sh

# Install model during build time
RUN ./install_model.sh

# Copy application files
COPY app.py .
COPY start.sh .

# Make startup script executable
RUN chmod +x start.sh

# Expose only Flask port
EXPOSE 5000

# Override the original entrypoint completely
ENTRYPOINT ["./start.sh"]
```

`app.py`
```python
import requests

def gemma3_response(user_prompt, summary_type):
    url = "http://localhost:11434/api/chat"
    
    payload = {
        "model": "gemma3:270m",
        "messages": [
            {
                "role": "system",
                "content": f"You are a text summarizer/modifier. Rewrite the input in a formal and {summary_type} style."
            },
            {
                "role": "user",
                "content": user_prompt
            }
        ],
        "stream": False
    }
    
    response = requests.post(url, json=payload)
    # return response.text
    return response.json()['message']['content']

# print(gemma3_response('adsv hi howss arse yo?', 'email'))

from flask import Flask
from flask import request
app = Flask(__name__)

@app.route('/')
def index():
    return """
    <form action='/' method='POST'>
        Text: <textarea name='user_prompt'></textarea> <br>
        Style: <input name='style' type='text'> <br>
        <button type='submit'>Ask</button>
    </form>
    """

@app.route('/', methods=['POST'])
def ask():
    user_prompt = request.form.get('user_prompt')
    style = request.form.get('style')
    ai_answer = gemma3_response(user_prompt, style)
    return f"<textarea>{ai_answer}</textarea>"

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
```

`install_model.sh`
```python
#!/bin/bash

echo "üîÑ Installing gemma3:270m model during build..."

# Start Ollama in background using full path
/usr/bin/ollama serve &
OLLAMA_PID=$!

# Wait longer for Alpine-based Ollama to start
sleep 15

# Check if Ollama is ready with timeout
TIMEOUT=60
ELAPSED=0
while ! curl -f http://localhost:11434/api/tags > /dev/null 2>&1; do
    echo "Waiting for Ollama to be ready... ($ELAPSED/$TIMEOUT seconds)"
    sleep 3
    ELAPSED=$((ELAPSED + 3))
    if [ $ELAPSED -ge $TIMEOUT ]; then
        echo "‚ùå Timeout waiting for Ollama to start!"
        kill $OLLAMA_PID 2>/dev/null || true
        exit 1
    fi
done

echo "‚úÖ Ollama is ready!"

# Pull the model using full path
echo "üì• Pulling gemma3:270m model..."
if /usr/bin/ollama pull gemma3:270m; then
    echo "‚úÖ Model gemma3:270m downloaded successfully!"
else
    echo "‚ùå Failed to download model!"
    kill $OLLAMA_PID 2>/dev/null || true
    exit 1
fi

# Verify model was installed
if /usr/bin/ollama list | grep -q "gemma3:270m"; then
    echo "‚úÖ Model gemma3:270m verified in model list!"
else
    echo "‚ùå Model not found in list!"
    kill $OLLAMA_PID 2>/dev/null || true
    exit 1
fi

# Stop Ollama gracefully
echo "üõë Stopping Ollama service..."
kill $OLLAMA_PID
wait $OLLAMA_PID 2>/dev/null || true

echo "‚úÖ Model installation complete!"
```

`start.sh`
```bash
#!/bin/bash

echo "üöÄ Starting FormalAI services..."

# Start Ollama server in background using full path
echo "üì° Starting Ollama server..."
/usr/bin/ollama serve &

# Wait for Ollama to be ready
echo "‚è≥ Waiting for Ollama to start..."
sleep 8

# Health check for Ollama
while ! curl -f http://localhost:11434/api/tags > /dev/null 2>&1; do
    echo "‚è≥ Waiting for Ollama API..."
    sleep 3
done

echo "‚úÖ Ollama is ready!"
echo "‚úÖ Model gemma3:270m is pre-installed!"

# List available models for verification
echo "üìã Available models:"
/usr/bin/ollama list

# Start Flask application
echo "üåê Starting Flask application on port 5000..."
python app.py
```

`requirements.txt`
```text
Flask
requests
```
